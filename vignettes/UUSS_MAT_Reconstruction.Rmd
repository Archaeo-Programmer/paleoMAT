---
title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)

# devtools::install()
# devtools::load_all()
library(paleomat)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. 

However, improvements in these techniques, and the increasing breadth of paleoclimatic proxies available have furthered our understanding of the effects of climate-driven variability on past societies. 

This program allows you to reconstruct the climate for multiple locations across North America (when data are sufficient to do). In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008) so that there will be columns of taxa with counts, as well as metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too.

# Get fossil, modern, and coretop pollen data.
```{r function_model, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs.
gpids <-
  neotoma::get_table(table.name = 'GeoPoliticalUnits')

NAID <-
  gpids %>%
  dplyr::filter(
    GeoPoliticalName %in% c('United States', 'Canada', 'Mexico'),
    GeoPoliticalUnit == 'country'
  ) %$%
  GeoPoliticalID

# BEGIN TESTING for MP_metadata_counts
# AZID <-
#   gpids %>%
#   dplyr::filter(GeoPoliticalName %in% c('Arizona'),
#                 GeoPoliticalUnit == 'state') %$%
#   GeoPoliticalID
#
# if(!file.exists(here::here("vignettes/data/AZ_metadata_counts.rds"))){
#   AZ_metadata_counts <-
#   get_modern_pollen(gpid = AZID) %T>%
#   readr::write_rds(here::here("vignettes/data/AZ_metadata_counts.rds"))
# }
# AZ_metadata_counts <-
#   readr::read_rds(here::here("vignettes/data/AZ_metadata_counts.rds"))

# END TESTING.

if (!file.exists(here::here("vignettes/data/MP_metadata_counts.rds"))) {
  MP_metadata_counts <-
    get_modern_pollen(gpid = NAID) %T>%
    readr::write_rds(here::here("vignettes/data/MP_metadata_counts.rds"))
}
MP_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/MP_metadata_counts.rds"))

if (!file.exists(here::here("vignettes/data/NAfossil_metadata_counts.rds"))) {
  NAfossil_metadata_counts <-
    get_fossil_pollen(gpid = NAID) %T>%
    readr::write_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))
}
NAfossil_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))

coreTops <-
  paleomat::get_coreTops() %>%
  dplyr::rename(dataset.id = DatasetID,
                site.id = SiteID,
                sample.id = SampleID)

```

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

if(!file.exists(here::here("vignettes/data/NAfossil_metadata_counts.rds"))) {
  CT_metadata_counts <-
    dplyr::left_join(coreTops,
                     NAfossil_metadata_counts,
                     by  = c("dataset.id", "site.id", "sample.id")) %>%
    dplyr::mutate(type = "core top") %>%
    dplyr::arrange(dataset.id) %T>%
    readr::write_rds(here::here("vignettes/data/CT_metadata_counts.rds"))
}
CT_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/CT_metadata_counts.rds"))


```

The Neotoma Modern Pollen Database contains `r nrow(MP_counts)` samples, representing `r ncol(MP_counts)` different pollen taxa.

## Combine Core Top and Modern Pollen Data

```{r Combine_CT_MP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Now the two dataframes can be combined using the function, then sort the rows by dataset.id. The distinct function is used to make sure there is no duplicate data from combining the two dataframes. Then, we convert all NA values to zero.
MPCT_metadata_counts <-
  dplyr::bind_rows(MP_metadata_counts, CT_metadata_counts) %>%
  dplyr::arrange(dataset.id) %>%
  dplyr::mutate_at(.vars = dplyr::vars(-dataset.id:-pub_year),
                   .funs = tidyr::replace_na,
                   0) %>%
  dplyr::select(dataset.id:pub_year,
                sort(tidyselect::peek_vars())) %>%
  dplyr::filter(!is.na(long)) %>%
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326)

# Keep only modern sites in the United States.
# Read in the United States shapefile, which will be used to filter out modern pollen sites that are outside of continental United States.
states <-
  rgdal::readOGR("vignettes/data/raw_data/statesp010g/statesp010g.shp",
                 layer = 'statesp010g')
# Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
states <-
  sp::spTransform(states,
                  sp::CRS(
                    "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
                  ))
# Filter out non-continental states and territories.
continental.states <-
  subset(
    states,
    NAME != "Alaska" &
      NAME != "Hawaii" &
      NAME != "Puerto Rico" & NAME != "U.S. Virgin Islands"
  )
# Convert sp to sf.
continental.states <- sf::st_as_sf(continental.states, crs = 4326)
# Update projection to CRS 4326, which is WGS 84.
continental.states <- sf::st_transform(continental.states, 4326)
# Need to make the geometry valid for the states shapefile.
continental.states <- sf::st_make_valid(continental.states)

# Do an intersection and keep only the sites that are within the continental United States.
sites_in_states <-
  sf::st_intersection(MPCT_metadata_counts, continental.states)
# Drop the columns from the states shapefile.
MPCT_metadata_counts_final <-
  dplyr::select (sites_in_states, -c(NAME:PRIM_MILES))



# # Keep only modern samples in specific ecoregions. This is if the reconstructions are limited by their level 1 ecoregions.
# # Read in the Level 1 ecoregions shapefile (https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/cec_na/na_cec_eco_l1.zip), which will be used to filter out modern pollen sites that are outside of the given ecoregions.
# ecoregions <- readOGR("/Volumes/PUCK_HD/Users/andrew/Downloads/na_cec_eco_l1/NA_CEC_Eco_Level1.shp")
# # Keep only ecoregions that there are fossil pollen sites in for the UUSS.
# western.ecoregions <- subset(ecoregions, NA_L1CODE == 6 | NA_L1CODE == 10 | NA_L1CODE == 12 | NA_L1CODE == 13)
# # Convert sp to sf.
# western.ecoregions2 <- sf::st_as_sf(western.ecoregions, crs = 4326)
# # Update projection to CRS 4326, which is WGS 84.
# western.ecoregions2 <- sf::st_transform(western.ecoregions2, 4326)
# # Need to make the geometry valid for the states shapefile.
# western.ecoregions2 <- sf::st_make_valid(western.ecoregions2)
#
# # Keep only ecoregion area that falls within the continental united states.
# ecoregions_in_states <- sf::st_intersection(western.ecoregions2, continental.states)
#
# # Do an intersection and keep only the sites that are within the selected western ecoregions.
# sites_in_ecoregions <- sf::st_intersection(MPCT_metadata_counts, ecoregions_in_states)
# # Drop the columns from the states shapefile.
# sites_in_ecoregions_final <- dplyr::select (sites_in_ecoregions,-c(NAME:PRIM_MILES))

```

## Remove Coretops data from the fossil pollen dataset
```{r removeCT, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

fossil_metadata_counts <-
  dplyr::anti_join(NAfossil_metadata_counts,
                   coreTops,
                   by = c("dataset.id", "site.id", "sample.id")) %>%
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326)

# # Do an intersection and keep only the sites that are within the continental United States.
# fossil_sites_in_states <- sf::st_intersection(fossil_metadata_counts, continental.states)
# # Drop the columns from the states shapefile.
# fossil_metadata_counts_final <- dplyr::select (fossil_sites_in_states,-c(NAME:PRIM_MILES))

# Limit fossil sites to just the Southwest for reconstruction.
# First, create a bounding box for the upper united states southwest
bb <-
  c(
    "xmin" = -113,
    "xmax" = -105,
    "ymin" = 31.334,
    "ymax" = 38.09
  ) %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

# Do an intersection and keep only the sites that are within the continental United States.
fossil_metadata_counts_final <-
  sf::st_intersection(fossil_metadata_counts, bb)


fossil_final <-
  fossil_metadata_counts_final %>%
  dplyr::select(!c(dataset.id, site.id, site.name:pub_year)) %>%
  tibble::as_tibble() %>%
  dplyr::mutate_all(tidyr::replace_na, 0) %>%
  dplyr::mutate(dplyr::across(!c(sample.id, geometry), ~ ifelse(.x < 0, 0, .x))) %>%
  tidyr::nest(pollen.counts = c(-sample.id,-geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(sample.id,
                pollen = pollen.counts)

```

# Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).  

```{r extract_prism_normals, echo = FALSE, warning=FALSE}

#Load prism data. Note: This is only if having trouble installing/loading paleomat package.
# filenames <- list.files("data", pattern="*.rda", full.names=TRUE)
# lapply(filenames,load,.GlobalEnv)
#load(file = "data/prism_1.rda")

# Extract Prism Climate data for modern pollen.
MPCT_climate <-
  MPCT_metadata_counts_final %>%
  dplyr::select(sample.id) %>%
  extract_prism_normals() %>%
  sf::st_drop_geometry() %>%
  dplyr::left_join(MPCT_metadata_counts_final, by = "sample.id") %>%
  dplyr::select(sample.id,
                prism.normals,
                geometry,
                ABIES:XANTHIUM) %>%
  tidyr::nest(pollen.counts = c(-sample.id,-prism.normals,-geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(sample.id,
                climate = prism.normals,
                pollen = pollen.counts)

# Get the data into the final format used by the palaeoSig and rioja packages.
MPCT_final <-
  MPCT_climate %>%
  dplyr::mutate(# Proportional data is only needed for the significance testing and the proportion are calculated within the palaeoSig::randomTF function.
    # pollen =
    #   purrr::map(pollen,
    #     function(x){
    #       x/rowSums(x)
    #     }),
    climate =
      purrr::map(climate,
                 function(x) {
                   x %>%
                     dplyr::select(month,
                                   ppt,
                                   tavg,
                                   gdd) %>%
                     tidyr::pivot_longer(ppt:gdd,
                                         names_to = "element") %>%
                     tidyr::unite("element_month", element, month) %>%
                     tidyr::pivot_wider(names_from = element_month,
                                        values_from = value)
                 }))


```

## Checking the calibration data set

Need to change this into a function. **

To test the model, I used the `palaeoSig` package's `randomTF()` function, which tests the models against randomly sorted data. If we get a significance for a climate variable, then we know that the model reconstruction is better than random. The model takes the proportion of variance accounted for by the actual data, and then compares it to the proportion of variance accounted for by the randomized data.

The MAT method is testing whether the modern calibration is able to detect signals in each of the climate parameters. The example here uses the entire North American Modern Pollen Database, rather than a targeted data subset.


### MAT - ten closest

```{r MAT_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# For some reason this fails consistently when we try to use the weighted MAT (col=2).
palaeoSig::randomTF(spp = 
                      MPCT_final %>%
                      dplyr::select(sample.id, pollen) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(pollen)) %>%
                      tibble::column_to_rownames("sample.id"),
                    env = 
                      MPCT_final %>%
                      dplyr::select(sample.id, climate) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(climate)) %>%
                      tibble::column_to_rownames("sample.id"),
                    fos = 
                      MPCT_final %>%
                      dplyr::select(sample.id, pollen) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(pollen)) %>%
                      tibble::column_to_rownames("sample.id"),
                    n = 99,
                    fun = rioja::MAT,
                    col = 1,
                    k = 10) %$%
  tibble::tibble("variable" = colnames(MPCT_final$climate[[1]]),
                 "% Explained" = EX*100,
                 "p-value" = sig)

```

The variance explained is very low for some variables, although the p value is very low. This highlights that in some cases you may not just want to rely on the p value. This also shows the issue with temporal autocorrelation on temperature and, to some extent also, on precipitation variables.

# Reconstruction Statistics

## Reconstruction Significance

Now we can do a similar test on fossil assemblage reconstructions. This again uses the same `randomTF()` function, but the variance explained and significance will probably change given that we are using a slightly more constrained dataset.

Now, we apply a reconstruction to a real dataset.

#### MAT - ten closest

```{r MAT_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

palaeoSig::randomTF(spp = 
                      MPCT_final %>%
                      dplyr::select(sample.id, pollen) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(pollen)) %>%
                      tibble::column_to_rownames("sample.id"),
                    env = 
                      MPCT_final %>%
                      dplyr::select(sample.id, climate) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(climate)) %>%
                      tibble::column_to_rownames("sample.id"),
                    fos = 
                      fossil_final %>%
                      dplyr::select(sample.id, pollen) %>%
                      sf::st_drop_geometry() %>%
                      tidyr::unnest(cols = c(pollen)) %>%
                      tibble::column_to_rownames("sample.id"),
                    n = 99,
                    fun = rioja::MAT,
                    col = 1,
                    k = 10) %$%
  tibble::tibble("variable" = colnames(MPCT_final[[3]][[1]]),
                 "% Explained" = EX*100,
                 "p-value" = sig)


# Old
# This is an altered wrapper for the randomTF function.
# source('../R/sig_test2.R')
# 
# MP_counts_noNAs <- MPCT_metadata_counts_final %>% 
#   dplyr::select(-site.id:-pub_year) %>% 
#   mutate_all(funs(ifelse(is.na(.), 0, .)))
# 
# mat_reconst <- run_tf2(MP_counts_noNAs, fossil = fossil_data, 
#                        climate = Clim_noNAs[,13:24],
#                        func = MAT, col = 2, k = 10)
# 
# mat_reconst[[1]]

# Create a list of all the fossil pollen sites to be reconstructed for the continental United States.
# fossil_sites <- unique(fossil_metadata_counts_final$dataset.id)

# for (i in 1:length(fossil_sites)) {
#   nam <- paste("mat_reconst_", fossil_sites[[i]], sep = "")
#   dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
#                  climate = Temperature,
#                  func = mat, col = 2, k = 10)
#   assign(nam, dat)
# }

```

Again, no significance for the model. This indicates that we are just not able to see a signal within the data, but this is probably due to the calibration dataset being too broad.

## Reconstruction

Once we have validated the methods, we re-run the analyses using MAT.

### Model Summary

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Now, we carry out the reconstruction for all fossil pollen sites in the southwest US study area.

# Average Temperature
# The reconstructions are in one output file, which has the reconstructed mean temperature (tavg) and associated error for each months.

# First, to save time, check to see if the sites have already been reconstructed, if so then this chunk will not run. If you would like to re-run the reconstructions, then simply delete the reconstruction .csv and it will run.

if (!file.exists("./vignettes/data/reconst_tavg.rds")) {
  mat_models_temp <- purrr::map(
    1:12,
    .f = function(j) {
      rioja::MAT(
        y = dplyr::bind_rows(MPCT_final$pollen),
        x = dplyr::bind_rows(MPCT_final$climate)[[paste0("tavg_", j)]],
        k = 5,
        dist.method = "sq.chord",
        lean = FALSE
      )
    }
  )
  
  reconst_tavg <- mat_models_temp %>%
    purrr::map(
      .f = function(the.model) {
        # Create the MAT reconstruction.
        predict(
          object = the.model,
          newdata = dplyr::bind_rows(fossil_final$pollen) %>%
            as.data.frame() %>%
            set_rownames(fossil_final$sample.id),
          sse = TRUE
        )
      }
    )
  
  readr::write_rds(reconst_tavg, "./vignettes/data/reconst_tavg.rds")
  
} else if (!exists("reconst_tavg")) {
  reconst_tavg <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg.rds"))
  
} else {
  stop(
    "The SWUS reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the directory."
  )
  
}

# GDD
# The reconstructions are in one output file, which has the reconstructed gdd and associated error.

# First, to save time, check to see if the sites have already been reconstructed, if so then this chunk will not run. If you would like to re-run the reconstructions, then simply delete the reconstruction .csv and it will run.

if (!file.exists("./vignettes/data/reconst_gdd.rds")) {
  mat_models_temp <- purrr::map(
    1:12,
    .f = function(j) {
      rioja::MAT(
        y = dplyr::bind_rows(MPCT_final$pollen),
        x = dplyr::bind_rows(MPCT_final$climate)[[paste0("gdd_", j)]],
        k = 5,
        dist.method = "sq.chord",
        lean = FALSE
      )
    }
  )
  
  reconst_gdd <- mat_models_temp %>%
    purrr::map(
      .f = function(the.model) {
        # Create the MAT reconstruction.
        predict(
          object = the.model,
          newdata = dplyr::bind_rows(fossil_final$pollen) %>%
            as.data.frame() %>%
            set_rownames(fossil_final$sample.id),
          sse = TRUE
        )
      }
    )
  
  readr::write_rds(reconst_gdd, "./vignettes/data/reconst_gdd.rds")
  
} else if (!exists("reconst_gdd")) {
  reconst_gdd <-
    readr::read_rds(here::here("./vignettes/data/reconst_gdd.rds"))
  
} else {
  stop(
    "The SWUS reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the directory."
  )
  
}


```

### Apply Threshold to Reconstruction

```{r clim_threshold, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Apply a threshold to the reconstruction, so that if an analogue is too dissimilar that it will not be used in the reconstruction. Then, based on the remaining analogues, the final reconstruction and associated error will be re-calculated.

#tavg
if (!file.exists("./vignettes/data/reconst_tavg_threshold.rds")) {
  reconst_tavg_threshold <-
    reconst_tavg %>%
    purrr::map(
      .f = function(the.reconst) {
        apply_threshold(x = the.reconst,
                        qtile = .995)
      }
    )
  
  readr::write_rds(reconst_tavg_threshold,
                   "./vignettes/data/reconst_tavg_threshold.rds")
  
} else if (!exists("reconst_tavg_threshold")) {
  reconst_tavg_threshold <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg_threshold.rds"))
  
} else {
  stop(
    "The SWUS reconstruction threshold has already been applied. If you would like to re-run the threshold, then delete the file from the directory."
  )
  
}


#gdd
if (!file.exists("./vignettes/data/reconst_gdd_threshold.rds")) {
  reconst_gdd_threshold <-
    reconst_gdd %>%
    purrr::map(
      .f = function(the.reconst) {
        apply_threshold(x = the.reconst,
                        qtile = .995)
      }
    )
  
  readr::write_rds(reconst_gdd_threshold,
                   "./vignettes/data/reconst_gdd_threshold.rds")
  
} else if (!exists("reconst_gdd_threshold")) {
  reconst_gdd_threshold <-
    readr::read_rds(here::here("./vignettes/data/reconst_gdd_threshold.rds"))
  
} else {
  stop(
    "The SWUS reconstruction threshold has already been applied. If you would like to re-run the threshold, then delete the file from the directory."
  )
}
  
```

### Simplify Reconstruction Output

```{r clim_cleanup, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Simplify tavg

# First, need to update 12 fossil pollen sites age models that have been done using Bacon. Here, we read in the updated age models from individual .csv files.
setwd("/Volumes/VILLAGE/paleomat/vignettes/data/updated_age_models")
file_names <- dir()
updated_age_models <- do.call(rbind,lapply(file_names,read.csv))
setwd("/Volumes/VILLAGE/paleomat")
  

# Put the reconstructed climate data into a tibble, using the simplify_reconstruction function. The function takes to parameters, the full reconstruction dataset (reconstruction.data) and the fossil dataset (with metadata and counts) that was used initially loaded that has all of the metadata. The function will select only the metadata and ignore the pollen counts.
if (!file.exists("./vignettes/data/reconst_tavg_final.rds")) {
  reconst_tavg_final <-
    simplify_reconstruction(reconstruction.data = reconst_tavg_threshold,
                            fossil.metadata = fossil_metadata_counts_final) %>%
    # Remove the rows that have NAs for the reconstructed value (i.e., this indicates that there were too few analogues to do a reconstruction). Also, remove rows that do not have an age chronology.
    tidyr::drop_na(age, value) %>% 
    # Update age models for multiple fossil pollen sites.
    dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>% 
    dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>% 
    dplyr::select(-age.y, -age.x) %>% 
    dplyr::mutate(age = round(age))
  
  readr::write_rds(reconst_tavg_final,
                   "./vignettes/data/reconst_tavg_final.rds")
  
} else if (!exists("reconst_tavg_final")) {
  reconst_tavg_final <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg_final.rds"))
  
} else {
  stop(
    "The SWUS reconstruction has already been simplified. If you would like to re-run the code, then delete the file from the directory."
  )
  
}


#Simplify GDD

# Put the reconstructed climate data into a tibble, using the simplify_reconstruction function. The function takes to parameters, the full reconstruction dataset (reconstruction.data) and the fossil dataset (with metadata and counts) that was used initially loaded that has all of the metadata. The function will select only the metadata and ignore the pollen counts.
if (!file.exists("./vignettes/data/reconst_gdd_final.rds")) {
  reconst_gdd_final <-
    simplify_reconstruction(reconstruction.data = reconst_gdd_threshold,
                            fossil.metadata = fossil_metadata_counts_final) %>%
    # Remove the rows that have NAs for the reconstructed value (i.e., this indicates that there were too few analogues to do a reconstruction). Also, remove rows that do not have an age chronology.
    tidyr::drop_na(age, value)
  
  readr::write_rds(reconst_gdd_final,
                   "./vignettes/data/reconst_gdd_final.rds")
  
} else if (!exists("reconst_gdd_final")) {
  reconst_gdd_final <-
    readr::read_rds(here::here("./vignettes/data/reconst_gdd_final.rds"))
  
} else {
  stop(
    "The SWUS reconstruction has already been simplified. If you would like to re-run the code, then delete the file from the directory."
  )
  
}

```

### Temporal Interpolation

```{r temporal_interpolation, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Determine what sites have multiple datasets.
unique_datasets <- unique(tavg[c("dataset.id", "site.name")])
duplicated_sites <- unique_datasets$site.name[duplicated(unique_datasets$site.name, fromLast = TRUE)] %>% 
  unique()
#list1[!list1 %in% list2]

#Just temporary as this number is actually calculated in the Data_visualization.rmd
warmest.month = 15.6182926829268

# Put lat and long into individual columns.
tavg <- reconst_tavg_final %>%
  tidyr::unnest(geometry) %>%
  dplyr::group_by(sample.id, month) %>%
  dplyr::mutate(col = seq_along(sample.id)) %>% #add a column indicator
  tidyr::spread(key = col, value = geometry) %>%
  dplyr::rename(long = "1", lat = "2") %>% 
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age < 5950 & age >= 0 & month == 7) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-4000, 2000, 100))) %>%
  dplyr::mutate(anom = value - warmest.month)
  

# After more closely examining the Beef Pasture data, the spline was heavily impacted by Petersen's Beef Pasture data. The data more or less produced an extremely smooth spline that led to underfitting. This could be the result of the age model on the Petersen data, which has way fewer dates than the Wright Beef Pasture data. Therefore, I use the Wright data for 1-2000 AD and the Petersen data for the time before 1AD (as the Wright data does not go any farther back in time).
tavg_predicted <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(site.data = purrr::map(site.data,
                                       function(x) {
                                         x %>%
                                           dplyr::group_by(age) %>%
                                           dplyr::mutate(
                                             value = mean(value),
                                             error = mean(error),
                                             anom = mean(anom)
                                           ) %>%
                                           dplyr::distinct(age, .keep_all = TRUE) %>%
                                           dplyr::arrange(age)
                                       })) %>%
  # sf::st_as_sf(coords = c("long", "lat"),
  #                crs = 4326) %>%
  dplyr::mutate(mean_100yrs = purrr::map(site.data, interpolate_time)) %>%
  dplyr::select(-site.data) %>%
  dplyr::arrange(site.name) %>%
  unnest_wider(mean_100yrs) %>%
  dplyr::rename(mean_100yrs = 6, gam_fit = 7)


# Get number of samples for each site.
# ok2 <- ok %>% 
#   dplyr::mutate(no.samples = purrr::map(site.data, ))
# ok2 <- ok %>%
#     dplyr::mutate(mean.anom = purrr::map(
#     site.data,
#     .f = function(x) {
#       nrow(x)
#     }
#   )) 
# Plot all sites GAM
longlake <- tavg_predicted %>% 
  dplyr::filter(site.name == "Long Lake")


# Currently just using base plot to make these quickly. All of the figures can't fit into one screen, so have to divide them up for now. 
tavg_predicted.nolm <- tavg_predicted %>% 
  dplyr::filter(!site.name == "Long Lake")
tavg_predicted.nolm <- tavg_predicted.nolm[1:20, ]
par(mfrow = c(4,5), oma = c(1.1,1,1,1))

purrr::map(
    1:length(tavg_predicted.nolm$site.id),
    .f = function(j) {
      plot(
        tavg_predicted.nolm$gam_fit[[j]],
        xlab = "Date (Years BC/AD)", 
        ylab = "Temperature Anamoly", 
        main = tavg_predicted.nolm$site.name[j]
      )
    }
  )

tavg_predicted.nolm <- tavg_predicted %>% 
  dplyr::filter(!site.name == "Long Lake")
tavg_predicted.nolm <- tavg_predicted.nolm[21:34, ]
par(mfrow = c(3,5), oma = c(1.1,1,1,1))

purrr::map(
    1:length(tavg_predicted.nolm$site.id),
    .f = function(j) {
      plot(
        tavg_predicted.nolm$gam_fit[[j]],
        xlab = "Date (Years BC/AD)", 
        ylab = "Temperature Anamoly", 
        main = tavg_predicted.nolm$site.name[j]
      )
    }
  )


# For TESTING
 
 temp_data <- tavg %>%
   dplyr::filter(date >= -1400 & date <= 2000) %>%
   dplyr::filter(site.id == 486) %>%
   # dplyr::filter(sample.id == 264381 |
   #                 sample.id == 264428 | sample.id == 264447) %>%
   #dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
   dplyr::group_by(site.id, site.name, elev, long, lat) %>%
   tidyr::nest() %>%
   dplyr::rename(site.data = data) %>%
   dplyr::mutate(site.data = purrr::map(site.data,
                                        function(x) {
                                          x %>%
                                            dplyr::group_by(age) %>%
                                            dplyr::mutate(
                                              value = mean(value),
                                              error = mean(error),
                                              anom = mean(anom)
                                            ) %>%
                                            dplyr::distinct(age, .keep_all = TRUE) %>%
                                            dplyr::arrange(age)
                                        }))
    temp_data <- as.data.frame(temp_data$site.data)
    
    
    fita <- smooth.spline(temp_data$date, temp_data$anom, df = 15)
    plot(temp_data$date, temp_data$anom)
    lines(fita)
    
    names(fit) <- c("x", "y", "se.fit")
    plot(fit$x,
         fit$y,
         xlim = c(min(fit$x), max(fit$x)),
         ylim = c(min(fit$y), max(fit$y)))
    lines(fit$x, fit$y)

# (tavg %>% dplyr::ungroup() %>% dplyr::select(dataset.id, site.name) %>% dplyr::distinct(dataset.id, site.name))

# write.c((tavg %>% dplyr::ungroup() %>% dplyr::select(dataset.id, site.name) %>% dplyr::distinct(dataset.id, site.name)), "/Users/andrew/Desktop/okay.csv", row.names = FALSE)
# =SUBSTITUTE(MID(SUBSTITUTE("/" & E2&REPT(" ",6),"/",REPT(",",255)),2*255,255),",","")



beef.pasture <- tavg %>%
  dplyr::filter(month == 7 & date >= -1400 & date <= 2000) %>%
  #dplyr::filter(site.id == 246) %>% 
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.id, age) %>%
  dplyr::mutate(value = mean(value)) %>%
  dplyr::distinct(age, .keep_all = TRUE) %>%
  dplyr::arrange(age)


boxplot(beef.pasture$anom)$out
Q <- quantile(beef.pasture$anom, probs=c(.10, .90), na.rm = FALSE)
iqr <- IQR(beef.pasture$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated <- subset(beef.pasture, beef.pasture$anom > (Q[1] - 1.5*iqr) & beef.pasture$anom < (Q[2]+1.5*iqr))
x <- eliminated$date
y <- eliminated$anom

fit <- spline(x,y, method = "natural")
plot(x,y, xlim = c(min(fit$x), max(fit$x)), ylim = c(min(fit$y), max(fit$y)))
lines(fit)
fit <- as.data.frame(fit)

boxplot(fit$y)$out
Q <- quantile(fit$y, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(fit$y)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated<- subset(fit, fit$y > (Q[1] - 1.5*iqr) & fit$y < (Q[2]+1.5*iqr))

fit <- smooth.spline(eliminated, all.knots = TRUE)
plot(eliminated$x, eliminated$y, xlim = c(min(fit$x), max(fit$x)), ylim = c(min(fit$y), max(fit$y)))
lines(fit)

agemax <- max(eliminated$x)
agemax_rounded <- agemax %>% 
  DescTools::RoundTo(multiple = 100, FUN = ceiling)
agemin <- min(eliminated$x)
agemin_rounded <- agemin %>% 
  DescTools::RoundTo(multiple = 100, FUN = floor)

midpoints <- seq((agemin_rounded + 50), (agemax_rounded - 50), by = 100)
fit <- as.data.frame(predict(fit, agemin:agemax)) %>% 
  dplyr::filter(x >= -1000 & x <= 1799) %>% 
  group_by(mean = (row_number() -1) %/% 100) %>%
  mutate(mean = mean(y)) %>% 
  dplyr::filter(x %in% midpoints)

names(fit) <- c("x", "mean", "se.fit")
plot(fit$x, fit$mean, xlim = c(min(fit$x), max(fit$x)), ylim = c(min(fit$y), max(fit$y)))
lines(fit$x, fit$mean)




bpmax <- max(eliminated$date)
bpmin <- min(eliminated$date)
totalyrs <- bpmax - bpmin
spacing <- totalyrs/75
#interval <- seq(-1050, 1800, length.out=spacing)
interval <- seq(0, 1, length.out=spacing)
#interval <- interval[2:(length(interval)-1)]
fit <- smooth.spline(x,y, df = (length(y)/2))
#fit <- npreg::ss(x, y, method = "GCV", m =3, knots = interval)
plot(x,y, xlim = c(min(fit$x), max(fit$x)))
lines(fit)
abline(v=interval)

plot(predict(fit, -1050:1800), ylim = c(-4, 4))


#abline(v = c(600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400), lty = 2)



beef.pasture <- tavg %>% 
  #dplyr::filter(site.id == 246) %>% 
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>% 
  dplyr::filter(date >= -1400 & date <= 2000) %>% 
  dplyr::group_by(age) %>% 
  dplyr::mutate(value = mean(value)) %>% 
  dplyr::distinct(age, .keep_all = TRUE) %>% 
  dplyr::arrange(age) %>% 
  # Keep just top row of each site (just for testing).
  dplyr::group_by(site.name) %>% 
  dplyr::do(head(.,1))

beef.pasture <- beef.pasture[-c(8, 9, 18), ]
c1 <- beef.pasture$date
c2 <- beef.pasture$anom

beef.pasture.limited <- beef.pasture %>% 
  dplyr::filter(!(dataset.id == 250 & date >= 0))
d1 <- beef.pasture.limited$date
d2 <- beef.pasture.limited$anom
# beef.pasture %>% 
#   #dplyr::filter((dataset.id == 25551)) %>% 
#   dplyr::pull(anom) %>% 
#   mean()

beef.pasture.old <- beef.pasture %>% 
  dplyr::filter(dataset.id == 250)
a1 <- beef.pasture.old$date
a2 <- beef.pasture.old$anom

beef.pasture.new <- beef.pasture %>% 
  dplyr::filter(dataset.id == 25551)
b1 <- beef.pasture.new$date
b2 <- beef.pasture.new$anom

#Beef Pasture all
par(mfrow = c(1,1))
boxplot(beef.pasture$anom)$out
Q <- quantile(beef.pasture$anom, probs=c(.01, .99), na.rm = FALSE)
iqr <- IQR(beef.pasture$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated <- subset(beef.pasture, beef.pasture$anom > (Q[1] - 1.5*iqr) & beef.pasture$anom < (Q[2]+1.5*iqr))
c1 <- eliminated$date
c2 <- eliminated$anom
latc <- eliminated$lat
longc <- eliminated$long

fit <- spline(c1,c2)
fita <- smooth.spline(c1,c2, df = 41)
plot(c1,c2, ylim=c(min(fita$y), max(fita$y)))
lines(fita)

# GAM and prediction
#attach(eliminated)
gam_fit <- mgcv::gam(anom ~ s(date, bs = 'cs', k = (length(eliminated$anom)/2)), data = eliminated, family=gaussian())
#detach("eliminated")
gam_knots <- gam_fit$smooth[[1]]$xp  ## extract knots locations

plot(eliminated$date,fitted(gam_fit))
lines(eliminated$date,fitted(gam_fit))
plot(eliminated$date, eliminated$anom, col= "grey", main = "cubic spline");
lines(eliminated$date, gam_fit$linear.predictors, col = 2, lwd = 2)
abline(v = gam_knots, lty = 2)

agemax <- max(eliminated$date)
agemax_rounded <- agemax %>% 
  DescTools::RoundTo(multiple = 100, FUN = ceiling)
agemin <- min(eliminated$date)
agemin_rounded <- agemin %>% 
  DescTools::RoundTo(multiple = 100, FUN = floor)

predict.points <- as.data.frame(seq(agemin_rounded, agemax_rounded, by = 100))
#predict.points <- as.data.frame(seq((agemin_rounded + 50), (agemax_rounded - 50), by = 100))
names(predict.points)[1] <- "date"

predict.anom <- mgcv::predict.gam(gam_fit,predict.points,type='response')
FinalData <- data.frame(predict.points,predict.anom)
plot(predict.points$date,FinalData$predict.anom)
lines(predict.points$date,FinalData$predict.anom)


# gam mapping
# site_means2 <- beef.pasture %>%
#   dplyr::mutate(z = rnorm(1, sd = 1),
#          yhat = value + z)
# 
# beef.pasture$sitef <- factor(beef.pasture$site.id)
# beef.pasture <- beef.pasture %>% dplyr::arrange(sitef)

m1 <- gam(anom ~ s(long, lat),
          data = beef.pasture, method = "REML")
m <- getViz(m1)
o <- plot( sm(m, 1) )
o + l_fitLine(colour = "red") + l_rug(mapping = aes(x=x, y=y), alpha = 0.8) +
    l_ciLine(mul = 5, colour = "blue", linetype = 2) + 
    l_points(shape = 19, size = 1, alpha = 0.1) + theme_classic()
plot(sm(m, 1)) + l_fitRaster() + l_fitContour() + l_points()

#USE THIS CODE!!
m_int <- gam(anom ~s(long, lat), data = beef.pasture)
plot(m_int)
rsst2 <- aggregate(UUSS_prism, 2)
icell <- 1:ncell(UUSS_prism)
pred <- data.frame(value = rsst2[icell],
                       cells = icell,
                       x = xFromCell(rsst2, icell),
                      y = yFromCell(rsst2, icell))
pred <- na.omit(pred)
pred <- pred %>% 
  dplyr::mutate(value = value / 10.00)
names(pred)[3:4] <- c("long", "lat")
pred$anom_pred <- predict(m_int, newdata = pred, type = "response")
head(pred)

rpred <- raster(rsst2)
rpred[pred$cells] <- pred$richness_pred
rpred_cropped <- crop(x = rpred, y = bb2)

colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(rpred_cropped, col=colors)


rzeros <- UUSS_prism
rzeros[] <- 0
names(rzeros) <- c("date")
rpred <- predict(rzeros, m1)


#Natural cubic spline versus b-spline.
cr_fit <- mgcv::gam(c2 ~ s(c1, bs = 'cs', k = 41))
cr_knots <- cr_fit$smooth[[1]]$xp  ## extract knots locations

bs_fit <- mgcv::gam(c2 ~ s(c1, bs = 'bs'))
bs_knots <- bs_fit$smooth[[1]]$knots  ## extract knots locations

#Beef Pasture limited
Q <- quantile(beef.pasture.limited$anom, probs=c(.10, .90), na.rm = FALSE)
iqr <- IQR(beef.pasture.limited$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated<- subset(beef.pasture.limited, beef.pasture.limited$anom > (Q[1] - 1.5*iqr) & beef.pasture.limited$anom < (Q[2]+1.5*iqr))
d1 <- eliminated$date
d2 <- eliminated$anom

fit3 <- spline(d1,d2)
fit3a <- smooth.spline(d1,d2)
# plot(d1,d2, ylim=c(-5, 5))
# lines(fit3)
#Natural cubic spline versus b-spline.
cr_fit3 <- mgcv::gam(d2 ~ s(d1, bs = 'cs'))
cr_knots3 <- cr_fit3$smooth[[1]]$xp  ## extract knots locations

bs_fit3 <- mgcv::gam(d2 ~ s(d1, bs = 'bs'))
bs_knots3 <- bs_fit3$smooth[[1]]$knots  ## extract knots locations

#Beef Pasture old
Q <- quantile(beef.pasture.old$anom, probs=c(.10, .90), na.rm = FALSE)
iqr <- IQR(beef.pasture.old$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated <- subset(beef.pasture.old, beef.pasture.old$anom > (Q[1] - 1.5*iqr) & beef.pasture.old$anom < (Q[2]+1.5*iqr))
a1 <- eliminated$date
a2 <- eliminated$anom

fit1 <- spline(a1,a2)
fit1a <- smooth.spline(a1,a2)
# plot(a1,a2, ylim=c(-5, 5))
# lines(fit1)
# xgrid1 <- seq(min(fit1$x), max(fit1$x),,2000)
# plot(predict(fit1a, xgrid1))

#Natural cubic spline versus b-spline.
cr_fit1 <- mgcv::gam(a2 ~ s(a1, bs = 'cs'))
cr_knots1 <- cr_fit1$smooth[[1]]$xp  ## extract knots locations

bs_fit1 <- mgcv::gam(a2 ~ s(a1, bs = 'bs'))
bs_knots1 <- bs_fit1$smooth[[1]]$knots  ## extract knots locations


# Beef Pasture new
Q <- quantile(beef.pasture.new$anom, probs=c(.10, .90), na.rm = FALSE)
iqr <- IQR(beef.pasture.new$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated <- subset(beef.pasture.new, beef.pasture.new$anom > (Q[1] - 1.5*iqr) & beef.pasture.new$anom < (Q[2]+1.5*iqr))
b1 <- eliminated$date
b2 <- eliminated$anom

fit2 <- spline(b1, b2)
fit2a <- smooth.spline(b1, b2)
plot(b1, b2, ylim=c(-5, 5))
lines(fit2a)
# xgrid2 <- seq(min(fit2a$x), max(fit2a$x),,2000)
# plot(predict(fit2a, xgrid2))

#Natural cubic spline versus b-spline.
cr_fit2 <- mgcv::gam(b2 ~ s(b1, bs = 'cs'))
cr_knots2 <- cr_fit2$smooth[[1]]$xp  ## extract knots locations

bs_fit2 <- mgcv::gam(b2 ~ s(b1, bs = 'bs'))
bs_knots2 <- bs_fit2$smooth[[1]]$knots  ## extract knots locations


par(mfrow = c(4,4))
#Beef Pasture all
par(mfrow = c(1,2))
plot(c1, c2, col= "grey", main = "natural cubic spline");
lines(c1, cr_fit$linear.predictors, col = 2, lwd = 2)
abline(v = cr_knots, lty = 2)
plot(c1, c2, col= "grey", main = "B-spline");
lines(c1, bs_fit$linear.predictors, col = 2, lwd = 2)
abline(v = bs_knots, lty = 2)
plot(c1, c2, col= "grey", main = "Forsythe, Malcolm and Moler cubic spline");
lines(fit)
plot(c1, c2, col= "grey", main = "Smoothed Spline");
lines(fita)
#Beef pasture limited
plot(d1, d2, col= "grey", main = "natural cubic spline");
lines(d1, cr_fit3$linear.predictors, col = 2, lwd = 2)
abline(v = cr_knots3, lty = 2)
plot(d1, d2, col= "grey", main = "B-spline");
lines(d1, bs_fit3$linear.predictors, col = 2, lwd = 2)
abline(v = bs_knots3, lty = 2)
plot(d1, d2, col= "grey", main = "Forsythe, Malcolm and Moler cubic spline");
lines(fit3)
plot(d1, d2, col= "grey", main = "Smoothed Spline");
lines(fit3a)
#Beef pasture old
plot(a1, a2, col= "grey", main = "natural cubic spline");
lines(a1, cr_fit1$linear.predictors, col = 2, lwd = 2)
abline(v = cr_knots1, lty = 2)
plot(a1, a2, col= "grey", main = "B-spline");
lines(a1, bs_fit1$linear.predictors, col = 2, lwd = 2)
abline(v = bs_knots1, lty = 2)
plot(a1, a2, col= "grey", main = "Forsythe, Malcolm and Moler cubic spline");
lines(fit1)
plot(a1, a2, col= "grey", main = "Smoothed Spline");
lines(fit1a)
#Beef pasture new
plot(b1, b2, col= "grey", main = "natural cubic spline");
lines(b1, cr_fit2$linear.predictors, col = 2, lwd = 2)
abline(v = cr_knots2, lty = 2)
plot(b1, b2, col= "grey", main = "B-spline");
lines(b1, bs_fit2$linear.predictors, col = 2, lwd = 2)
abline(v = bs_knots2, lty = 2)
plot(b1, b2, col= "grey", main = "Forsythe, Malcolm and Moler cubic spline");
lines(fit2)
plot(b1, b2, col= "grey", main = "Smoothed Spline");
lines(fit2a)
mtext("Beef Pasture All", outer = TRUE)


par(mfrow = c(1,1))
b <- gam(anom~s(date),data=eliminated)
plot(c1, c2, col= "grey", main = "spline")
lines(c1, b$linear.predictors, col = 2, lwd = 2)
summary(b)




out <- fields::Tps(x,y)
xgrid<- seq(  min(out$x), max(out$x),,2000)
fhat<- predict(out,xgrid)
plot(x,y)
title('Tps')
lines(xgrid, fhat,)
SE<- predictSE(out, xgrid)
lines(xgrid,fhat + 1.96* SE, col="red", lty=2)
lines(xgrid, fhat - 1.96*SE, col="red", lty=2)

# Another way to run this is to use a generalized additive models with integrated smoothness estimation.
library(mgcv)
fit <- mgcv::gam(y ~ s(x, bs = "cr", k = 20))
summary(fit)
gam.check(fit)
plot(fit)

plot(y ~ x, col = "dark red", pch = 16)
curve(predict(fit, newdata = data.frame(x = x)), add = TRUE, 
      from = min(x), to = max(x), n = 1e3, lwd = 2)




# Test smooth.spline with different degrees of freedom. 
#Beef Pasture all
fita <- smooth.spline(x,y, df = length(x)*0.10)
fitb <- smooth.spline(x,y, df = length(x)*0.20)
fitc <- smooth.spline(x,y, df = length(x)*0.30)
fitd <- smooth.spline(x,y, df = length(x)*0.40)
fite <- smooth.spline(x,y, df = length(x)*0.50)
fitf <- smooth.spline(x,y, df = length(x)*0.60)
fitg <- smooth.spline(x,y, df = length(x)*0.70)
fith <- smooth.spline(x,y, df = length(x)*0.80)
fiti <- smooth.spline(x,y, df = length(x)*0.90)
fitj <- smooth.spline(x,y, df = length(x)*1.00)
#Beef Pasture old
fit1a <- smooth.spline(x1,y1, df = length(x1)*0.10)
fit1b <- smooth.spline(x1,y1, df = length(x1)*0.20)
fit1c <- smooth.spline(x1,y1, df = length(x1)*0.30)
fit1d <- smooth.spline(x1,y1, df = length(x1)*0.40)
fit1e <- smooth.spline(x1,y1, df = length(x1)*0.50)
fit1f <- smooth.spline(x1,y1, df = length(x1)*0.60)
fit1g <- smooth.spline(x1,y1, df = length(x1)*0.70)
fit1h <- smooth.spline(x1,y1, df = length(x1)*0.80)
fit1i <- smooth.spline(x1,y1, df = length(x1)*0.90)
fit1j <- smooth.spline(x1,y1, df = length(x1)*1.00)
#Beef Pasture new
fit2a <- smooth.spline(x2,y2, df = length(x2)*0.10)
fit2b <- smooth.spline(x2,y2, df = length(x2)*0.20)
fit2c <- smooth.spline(x2,y2, df = length(x2)*0.30)
fit2d <- smooth.spline(x2,y2, df = length(x2)*0.40)
fit2e <- smooth.spline(x2,y2, df = length(x2)*0.50)
fit2f <- smooth.spline(x2,y2, df = length(x2)*0.60)
fit2g <- smooth.spline(x2,y2, df = length(x2)*0.70)
fit2h <- smooth.spline(x2,y2, df = length(x2)*0.80)
fit2i <- smooth.spline(x2,y2, df = length(x2)*0.90)
fit2j <- smooth.spline(x2,y2, df = length(x2)*1.00)


par(mfrow = c(3,5))
#Beef Pasture all
plot(x, y, col= "grey", main = "BP All df: 10%");
lines(fita)
plot(x, y, col= "grey", main = "BP All df: 20%");
lines(fitb)
plot(x, y, col= "grey", main = "BP All df: 30%");
lines(fitc)
plot(x, y, col= "grey", main = "BP All df: 40%");
lines(fitd)
plot(x, y, col= "grey", main = "BP All df: 50%");
lines(fite)
#Beef pasture old
plot(x1,y1, col= "grey", main = "BP Old df: 10%");
lines(fit1a)
plot(x1,y1, col= "grey", main = "BP Old df: 20%");
lines(fit1b)
plot(x1,y1, col= "grey", main = "BP Old df: 30%");
lines(fit1c)
plot(x1,y1, col= "grey", main = "BP Old df: 40%");
lines(fit1d)
plot(x1,y1, col= "grey", main = "BP Old df: 50%");
lines(fit1e)
#Beef pasture new
plot(x2,y2, col= "grey", main = "BP New df: 10%");
lines(fit2a)
plot(x2,y2, col= "grey", main = "BP New df: 20%");
lines(fit2b)
plot(x2,y2, col= "grey", main = "BP New df: 30%");
lines(fit2c)
plot(x2,y2, col= "grey", main = "BP New df: 40%");
lines(fit2d)
plot(x2,y2, col= "grey", main = "BP New df: 50%");
lines(fit2e)




par(mfrow = c(3,5))
plot(x, y, col= "grey", main = "BP All df: 60%");
lines(fitf)
plot(x, y, col= "grey", main = "BP All df: 70%");
lines(fitg)
plot(x, y, col= "grey", main = "BP All df: 80%");
lines(fith)
plot(x, y, col= "grey", main = "BP All df: 90%");
lines(fiti)
plot(x, y, col= "grey", main = "BP All df: 100%");
lines(fitj)
plot(x1,y1, col= "grey", main = "BP Old df: 60%");
lines(fit1f)
plot(x1,y1, col= "grey", main = "BP Old df: 70%");
lines(fit1g)
plot(x1,y1, col= "grey", main = "BP Old df: 80%");
lines(fit1h)
plot(x1,y1, col= "grey", main = "BP Old df: 90%");
lines(fit1i)
plot(x1,y1, col= "grey", main = "BP Old df: 100%");
lines(fit1j)
plot(x2,y2, col= "grey", main = "BP New df: 60%");
lines(fit2f)
plot(x2,y2, col= "grey", main = "BP New df: 70%");
lines(fit2g)
plot(x2,y2, col= "grey", main = "BP New df: 80%");
lines(fit2h)
plot(x2,y2, col= "grey", main = "BP New df: 90%");
lines(fit2i)
plot(x2,y2, col= "grey", main = "BP New df: 100%");
lines(fit2j)


# Find out what spar value has the lowest residual sum of squares.
splineres <- function(x, y, spar){
  res <- rep(0, length(x))
  for (i in 1:length(x)){
    mod <- smooth.spline(x[-i], y[-i], spar = spar)
    res[i] <- predict(mod, x[i])$y - y[i]
  }
  return(sum(res^2))
}

spars <- seq(0, 1.5, by = 0.001)
#BP all
ss <- rep(0, length(spars))
for (i in 1:length(spars)){
  ss[i] <- splineres(x, y, spars[i])
}
#plot(spars, ss, 'l', xlab = 'spar', ylab = 'Cross Validation Residual Sum of Squares' , main = 'CV RSS vs Spar')
spars[which.min(ss)]
#BP Old
ss1 <- rep(0, length(spars))
for (i in 1:length(spars)){
  ss1[i] <- splineres(x1, y1, spars[i])
}
plot(spars, ss1, 'l', xlab = 'spar', ylab = 'Cross Validation Residual Sum of Squares' , main = 'CV RSS vs Spar')
spars[which.min(ss1)]
#BP New
ss2 <- rep(0, length(spars))
for (i in 1:length(spars)){
  ss2[i] <- splineres(x2, y2, spars[i])
}
plot(spars, ss2, 'l', xlab = 'spar', ylab = 'Cross Validation Residual Sum of Squares' , main = 'CV RSS vs Spar')
spars[which.min(ss2)]

fita <- smooth.spline(x,y, df = length(x)*0.10)
fit1a <- smooth.spline(x1,y1, df = length(x1)*0.10)
fit2a <- smooth.spline(x2,y2)



plot(predict(fit2a, 0:2000))


# Visualize outliers.
boxplot(beef.pasture$anom)$out
ggstatsplot::ggbetweenstats(data = beef.pasture,
                x = site.id, y = anom, outlier.tagging = TRUE)

#Remove outliers via quantile.
Q <- quantile(beef.pasture$anom, probs=c(.25, .75), na.rm = FALSE)
iqr <- IQR(beef.pasture$anom)
up <-  Q[2]+1.5*iqr # Upper Range  
low<- Q[1]-1.5*iqr # Lower Range
eliminated<- subset(beef.pasture, beef.pasture$anom > (Q[1] - 1.5*iqr) & beef.pasture$anom < (Q[2]+1.5*iqr))
ggstatsplot::ggbetweenstats(eliminated, site.id, anom, outlier.tagging = TRUE)

fit <- spline(eliminated$age, eliminated$anom)
plot(eliminated$age, eliminated$anom, ylim=c(-5, 5))
lines(fit)


```













### Thin Plate Spline Regression (TPS) and Mapping

```{r mapping_tps, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
tavg_100yrs <- tavg_predicted %>% 
  dplyr::select(-gam_fit) %>% 
  tidyr::unnest(mean_100yrs) %>% 
  dplyr::filter(date >= -1000 & date <= 1800) %>% 
  dplyr::group_by(date) %>% 
  tidyr::nest() %>% 
  dplyr::rename(site.predictions = data)

tavg_preds <- tavg_100yrs %>%
  dplyr::mutate(site.predictions = purrr::map(
    site.predictions,
    .f = function(x) {
      map_predictions(
        site.preds = x,
           degree.res = 0.25,
           site.locs = fossil.locs,
           rast.extent = TRUE
      )
    }
  ))

colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(tavg_preds$site.predictions[[1]], col=colors)





#First, prepare the raster grid that will be used to do predictions over. In this case, it is the PRISM grid that covers the study area (via the extents done by bounding box).
# Convert reconstruction data to class sf, then get the spatial extent of the fossil pollen sites.
xys <- sf::st_as_sf(reconst_tavg_final) %>%
  raster::extent()

# Get extent of fossil pollen sites and make a polygon.
fossilpnts <- reconst_tavg_final %>%
  sf::st_as_sf(crs = 4326)
fossilpolygon <- concaveman::concaveman(fossilpnts) %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326) %>%
  sf:::as_Spatial()

# Get site locations. 
fossil.locs <- tavg %>%
  dplyr::group_by(site.name) %>% 
  dplyr::slice(1) %>% 
  dplyr::select(site.id, site.name, long, lat)

# Next, apply a 25 mile (or 40233.6 meters) buffer from the extent of the sites (to be the edge of extrapolation from the sites). Here, the projection is changed as gBuffer (or GEOS) expects planar coordinates. Then, these are converted back in order to use mask to clip the raster below.
# fossilpolygon <- sp::spTransform(fossilpolygon, CRS("+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs"))
# fossilpolygon <- rgeos::gBuffer(fossilpolygon, width = 40233.6)
# fossilpolygon <- sp::spTransform(fossilpolygon, CRS("+init=epsg:4326"))

# Create a bounding box for the extent of the sites.
# bb2 <- c("xmin" = xys[1], "xmax" = xys[2], "ymin" = xys[3], "ymax" = xys[4]) %>%
#   sf::st_bbox() %>%
#   sf::st_as_sfc() %>%
#   sf::st_as_sf(crs = 4326) %>%
#   sf::st_transform(crs = 4326)

# Get prism grid. First, just need one raster layer, which here is just the tmax values for month 7.
prism <- prism_7[2,]$normal[[1]]
# Do intersection to get just the spatial extent of the fossil pollen sites (via a bounding box or polygon).
UUSS_prism <- raster::mask(prism, fossilpolygon)


# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
tavg_100yrs <- tavg_predicted %>% 
  dplyr::select(-gam_fit) %>% 
  tidyr::unnest(mean_100yrs) %>% 
  dplyr::filter(date >= -1000 & date <= 1800) %>% 
  dplyr::group_by(date) %>% 
  tidyr::nest() %>% 
  dplyr::rename(site.predictions = data)
  

# FOR TESTING
test <- tavg_100yrs[19:21,]
site.preds = test$site.predictions
site.preds <- as.data.frame(site.preds)
prism.raster = UUSS_prism
preds.extent = fossilpolygon


tavg_preds <- tavg_100yrs %>%
  dplyr::mutate(raster.preds = purrr::map(
    site.predictions,
    .f = function(x) {
      map_predictions(
        site.preds = x,
        prism.raster = UUSS_prism,
        preds.extent = fossilpolygon
      )
    }
  ))

colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(rpred_cropped, col=colors)


ok <- tavg_100yrs[19,]$site.predictions %>% 
  as.data.frame() %>% 
  dplyr::rename(x = long, y = lat)

# New options using Sören Wilke's post: https://swilke-geoscience.net/post/spatial_interpolation/
# First make a grid.
bbox <- c(
  "xmin" = min(ok$x),
  "ymin" = min(ok$y),
  "xmax" = max(ok$x),
  "ymax" = max(ok$y)
)

grd_template <- expand.grid(
  x = seq(from = bbox["xmin"], to = bbox["xmax"], by = 0.25),
  y = seq(from = bbox["ymin"], to = bbox["ymax"], by = 0.25) # in degrees resolution
)
# grd_template <- expand.grid(
#   x = seq(from = min(as.data.frame(bb2[[1]][[1]][[1]])$V1), to = max(as.data.frame(bb2[[1]][[1]][[1]])$V1), by = 0.25),
#   y = seq(from = min(as.data.frame(bb2[[1]][[1]][[1]])$V2), to = max(as.data.frame(bb2[[1]][[1]][[1]])$V2), by = 0.25) # in degrees resolution
# )
#Or use sf to make the grid instead.
sf_NH4 <- st_as_sf(ok, coords = c("x", "y"), crs = 4326)

alt_grd_template_sf <- sf_NH4 %>% 
  st_bbox() %>% 
  st_as_sfc() %>% 
  st_make_grid(
  cellsize = c(0.5, 0.5),
  what = "centers"
  ) %>%
  st_as_sf() %>%
  cbind(., st_coordinates(.)) %>% 
  st_drop_geometry() %>% 
  mutate(Z = 0)

#Rasterize the grid. 
crs_raster_format <- "+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs"

grd_template_raster <- grd_template %>% 
  dplyr::mutate(Z = 0) %>% 
  raster::rasterFromXYZ( 
    crs = crs_raster_format)

alt_grd_template_raster <- alt_grd_template_sf %>% 
  raster::rasterFromXYZ(
     crs = crs_raster_format
  )

# Fit the model as the first step in the process.
# Thin Plate Spline Regression
fit_TPS <- fields::Tps( # using {fields}
  x = as.matrix(ok[, c("x", "y")]), # accepts points but expects them as matrix
  Y = ok$anom,  # the dependent variable
  #Z = ok$elev,
  miles = TRUE, 
  df = 20
)

interp_TPS <- interpolate(grd_template_raster, fit_TPS)
crs(interp_TPS) <- CRS('+init=EPSG:4326')
suppressWarnings(mapview::mapview(interp_TPS))

out.p<-fields::predictSurface( fit_TPS)
image( out.p)

# Generalized Additive Model
fit_GAM <- mgcv::gam( # using {mgcv}
  anom ~ s(x, y, k = 23),      # here come our X/Y/Z data - straightforward enough
  data = ok      # specify in which object the data is stored
)

interp_GAM <- grd_template %>% 
  mutate(Z = predict(fit_GAM, .)) %>% 
  rasterFromXYZ(crs = crs_raster_format)
crs(interp_GAM) <- CRS('+init=EPSG:4326')
mapview::mapview(interp_GAM)

# Triangular Irregular Surface
fit_TIN <- interp::interp( # using {interp}
  x = ok$x,           # the function actually accepts coordinate vectors
  y = ok$y,
  z = ok$anom,
  xo = grd_template$x,     # here we already define the target grid
  yo = grd_template$y,
  output = "points"
) %>% bind_cols()

interp_TIN <- raster::rasterFromXYZ(fit_TIN, crs = crs_raster_format)
crs(interp_TIN) <- CRS('+init=EPSG:4326')
mapview::mapview(interp_TIN)

hello <- tavg_100yrs %>%
  dplyr::mutate(mean.anom = purrr::map(
    site.predictions,
    .f = function(x) {
      nrow(x)
    }
  ))


tavg_preds <- tavg_100yrs %>%
  dplyr::mutate(site.predictions = purrr::map(
    site.predictions,
    .f = function(x) {
      map_predictions(
        site.preds = x,
           degree.res = 0.25,
           site.locs = fossil.locs,
           rast.extent = TRUE
      )
    }
  ))

colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(tavg_preds$site.predictions[[1]], col=colors)


# purrr::map(
#       .f = function(the.reconst) {
#         apply_threshold(x = the.reconst,
#                         qtile = .995)
#       }
#     )

ok5 <- beef.pasture[1:32, ]

m_int <- mgcv::gam(anom ~s(long, lat, k = 29), data = ok5)
plot(m_int)
rsst2 <- aggregate(UUSS_prism, 2)
icell <- 1:ncell(UUSS_prism)
pred <- data.frame(value = rsst2[icell],
                       cells = icell,
                       x = xFromCell(rsst2, icell),
                      y = yFromCell(rsst2, icell))
pred <- na.omit(pred)
pred <- pred %>% 
  dplyr::mutate(value = value / 10.00)
names(pred)[3:4] <- c("long", "lat")
pred$anom_pred <- predict(m_int, newdata = pred, type = "response")
head(pred)

rpred <- raster(rsst2)
rpred[pred$cells] <- pred$richness_pred
rpred_cropped <- crop(x = rpred, y = bb2)

colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(rpred_cropped, col=colors)




tavg$coords <- matrix(c(tavg$long, tavg$lat), byrow = F, ncol = 2)

# First, for testing and building the map. I will just use the warmest month (i.e., July), and will only pick 2 centuries to do (i.e., 601-700 and 1201-1300).
# tavg_BC4000 <- tavg %>%
#      dplyr::filter(period == "(-4e+03,-3.9e+03]")

#600-700 AD
tavg_AD600 <- tavg %>%
  dplyr::filter(period == "(600,700]")

tavg_AD600 <- fields::Tps(tavg_AD600$coords, tavg_AD600$value)
fields::surface(tavg_AD600, nx = 100, ny = 100)

#1200-1300 AD
tavg_AD1200 <- tavg %>%
  dplyr::filter(period == "(1.2e+03,1.3e+03]")

tavg_AD1200tps <- fields::Tps(tavg_AD1200$coords, tavg_AD1200$anom)
fields::surface(tavg_AD1200tps)

# Convert reconstruction data to class sf, then get the spatial extent of the fossil pollen sites.
xys <- sf::st_as_sf(reconst_tavg_final) %>%
  raster::extent()

# Get extent of fossil pollen sites and make a polygon.
fossilpnts <- reconst_tavg_final %>%
  sf::st_as_sf(crs = 4326)
fossilpolygon <- concaveman::concaveman(fossilpnts) %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326) %>%
  sf:::as_Spatial()

# Create a bounding box for the extent of the sites.
bb2 <- c("xmin" = xys[1], "xmax" = xys[2], "ymin" = xys[3], "ymax" = xys[4]) %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

# Get prism grid. First, just need one raster layer, which here is just the tmax values for month 7.
prism <- prism_7[2,]$normal[[1]]
# Do intersection to get just the spatial extent of the fossil pollen sites (via a bounding box or polygon).
UUSS_prism <- raster::mask(prism, bb2)
# Convert raster layer to a dataframe to put into correct format for the fields package.
UUSS_prism_df <- raster::as.data.frame(UUSS_prism, xy = TRUE) %>%
  dplyr::select(x, y)

# The prediction locations must be formatted as a matrix, then the predict.Krig function can be called to do the prediction.
UUSS_prism_df$coords <-
  matrix(c(UUSS_prism_df$x, UUSS_prism_df$y),
         byrow = F,
         ncol = 2)

# Make predictions onto prism grid (which is limited to the extent of the fossil pollen sites via polygon).
tavg_AD1200.pred <- fields::predict.Krig(tavg_AD1200, UUSS_prism_df$coords)
# grid <-  fields.convert.grid(UUSS_prism_df$coords)
# grid <- make.surface.grid(grid)
# out.p<- as.surface(grid, tavg_AD1200.pred)

# Add the predictions (i.e., tavg_AD1200.pred) to the prism grid (i.e., UUSS_prism_df).
UUSS_prism_df <- UUSS_prism_df %>%
  dplyr::mutate(pred = tavg_AD1200.pred) %>% 
  dplyr::select(-coords)


# plot(UUSS_prism_df$coords, pch=20, asp=1,
#  cex=0.8*tavg_AD1200.pred/max(tavg_AD1200.pred),
#  xlab="E", ylab="N",
#  main="Temperature °C")
#
# ggplot(data = UUSS_prism_df, aes(x, y)) + geom_raster(aes(fill=pred))
# raster::rasterFromXYZ(UUSS_prism_df, crs = 4326)


#Other things
# You can quickly plot the values by using quilt.plot. This will also average the samples that fall within the same grid box.
x <- cbind(tavg_AD1200$long, tavg_AD1200$lat)
y <- tavg_AD1200$anom
quilt.plot(x,y)
US(add=TRUE)

#Temperature Anomalies through Time
matplot( x = tavg$age, y = tavg$anom, 
         ylab="Temperature Anomaly", xlab="Years BP", pch=16)
title("Temperature Anomalies through Time")


```
