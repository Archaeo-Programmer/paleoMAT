---
  title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
  code_folding: show
keep_md: yes
number_sections: yes
theme: sandstone
toc: yes
toc_depth: 3
toc_float: yes
pdf_document:
  toc: yes
toc_depth: '3'
always_allow_html: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)

# devtools::install()
# devtools::load_all()
library(paleomat)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. Improvements in these techniques and the increasing breadth of paleoclimatic proxies available, however, have furthered our understanding of the effects of climate-driven variability on past societies.

This program allows you to reconstruct the climate for multiple locations across the southwestern United States. In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008), so that there will be columns of taxa with counts and associated metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too. Here, we use the Neotoma package, version 1.7.4.

# Get fossil, modern, and coretop pollen data.
```{r function_model, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then, get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs.
gpids <-
  neotoma::get_table(table.name = 'GeoPoliticalUnits')

NAID <-
  gpids %>%
  dplyr::filter(
    GeoPoliticalName %in% c('United States', 'Canada', 'Mexico'),
    GeoPoliticalUnit == 'country'
  ) %$%
  GeoPoliticalID


if (!file.exists(here::here("vignettes/data/MP_metadata_counts.rds"))) {
  MP_metadata_counts <-
    get_modern_pollen(gpid = NAID) %T>%
    readr::write_rds(here::here("vignettes/data/MP_metadata_counts.rds"))
}
MP_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/MP_metadata_counts.rds"))

if (!file.exists(here::here("vignettes/data/NAfossil_metadata_counts.rds"))) {
  NAfossil_metadata_counts <-
    get_fossil_pollen(gpid = NAID) %T>%
    readr::write_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))
}
NAfossil_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))

```

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# This was originally used to pull the core top data; however, the SSL certificate has now expired. The data was saved and can be loaded below.
# coreTops <-
#   paleomat::get_coreTops() %>%
#   dplyr::rename(dataset.id = DatasetID,
#                 site.id = SiteID,
#                 sample.id = SampleID)
coreTops <- 
  readr::read_rds(here::here("vignettes/data/coreTops.rds"))


if(!file.exists(here::here("vignettes/data/NAfossil_metadata_counts.rds"))) {
  CT_metadata_counts <-
    dplyr::left_join(coreTops,
                     NAfossil_metadata_counts,
                     by  = c("dataset.id", "site.id", "sample.id")) %>%
    dplyr::mutate(type = "core top") %>%
    dplyr::arrange(dataset.id) %T>%
    readr::write_rds(here::here("vignettes/data/CT_metadata_counts.rds"))
}
CT_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/CT_metadata_counts.rds"))


```

The Neotoma Modern Pollen Database contains `r nrow(MP_metadata_counts)` samples, representing `r ncol(MP_metadata_counts %>% dplyr::select(ABIES:tail(names(.), 1)))` different pollen taxa.

## Combine Core Top and Modern Pollen Data for Continental United States

```{r Combine_CT_MP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

#Combine the modern and core top data, then keep only sites that fall within the continental United states
if (!file.exists(here::here("vignettes/data/MPCT_metadata_counts_final.rds"))) {
  # Now, the two dataframes (i.e., the modern data and core top data) can be combined using this function, then sort the rows by dataset.id.
  MPCT_metadata_counts <-
    dplyr::bind_rows(MP_metadata_counts, CT_metadata_counts) %>%
    dplyr::arrange(dataset.id) %>%
    dplyr::mutate(across(-c(dataset.id:pub_year), ~ tidyr::replace_na(.x, 0))) %>%
    dplyr::select(dataset.id:pub_year,
                  sort(tidyselect::peek_vars())) %>%
    dplyr::filter(!is.na(long)) %>%
    sf::st_as_sf(coords = c("long", "lat"), crs = 4326)
  
  # Keep only modern sites in the continental United States.
  # Read in the United States shapefile, which will be used to filter out modern pollen sites that are outside of continental United States.
  states <-
    rgdal::readOGR("vignettes/data/raw_data/statesp010g/statesp010g.shp",
                   layer = 'statesp010g')
  
  # Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
  states <-
    sp::spTransform(
      states,
      sp::CRS(
        "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
      )
    )
  
  # Filter out non-continental states and territories.
  continental.states <-
    subset(
      states,
      NAME != "Alaska" &
        NAME != "Hawaii" &
        NAME != "Puerto Rico" & NAME != "U.S. Virgin Islands"
    )
  
  # Convert sp to sf.
  continental.states <- sf::st_as_sf(continental.states, crs = 4326)
  
  # Update projection to CRS 4326, which is WGS 84.
  continental.states <- sf::st_transform(continental.states, 4326)
  
  # Need to make the geometry valid for the states shapefile.
  continental.states <- sf::st_make_valid(continental.states)
  continental.states_union <- sf::st_union(continental.states)
  
  # Do an intersection and keep only the sites that are within the continental United States.
  buffer <- sf::st_buffer(continental.states_union, 0)
  sites_in_states3 <-
    sf::st_intersection(MPCT_metadata_counts, buffer)
  MPCT_metadata_counts_final <- sites_in_states3
  
  # Drop the columns from the states shapefile.
  # MPCT_metadata_counts_final <-
  #   dplyr::select (sites_in_states3, -c(NAME:PRIM_MILES))
  
  readr::write_rds(
    MPCT_metadata_counts_final,
    here::here("vignettes/data/MPCT_metadata_counts_final.rds")
  )
  
}
MPCT_metadata_counts_final <-
  readr::read_rds(here::here("vignettes/data/MPCT_metadata_counts_final.rds"))


# Create shapefile for only the western  United States, as we will only use modern samples from the western United States. Here, we use the Williams and Shuman 2008 (https://doi.org/10.1016/j.quascirev.2008.01.004) eastern and western splits and use the maximum of all of the splits to designate the extent of the western United States.
if (!file.exists(here::here("vignettes/data/raw_data/west_in_states.shp"))) {
  # Read in all the western shapefiles.
  temp <-
    list.files(path = "vignettes/data/raw_data/Williams and Shuman 2008 Supplementary",
               pattern = "*WEST.shp$",
               full.names = TRUE)
  
  # Then, apply the st_read function to the list of shapefiles.
  myfiles <- lapply(temp, st_read, crs = 4326)
  
  # Then, set the name of each list element (each shapefile) to its respective file name.
  names(myfiles) <- gsub(
    "*WEST.shp$",
    "",
    list.files(
      "vignettes/data/raw_data/Williams and Shuman 2008 Supplementary",
      pattern = "*WEST.shp$",
      full.names = FALSE
    ),
    fixed = TRUE
  )
  
  # Combine all the shapefiles into 1 shapefile with 1 layer.
  all_west <- sf::st_as_sf(data.table::rbindlist(myfiles))
  all_west <- st_union(all_west$geometry)
  west_in_states <- all_west
  
  write_sf(west_in_states,
           dsn = "vignettes/data/raw_data/",
           layer = 'west_in_states',
           driver = "ESRI Shapefile")
  
}
west_in_states <-
  sf::read_sf("vignettes/data/raw_data/west_in_states.shp")

```

## Remove Coretops data from the fossil pollen dataset
```{r removeCT, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Remove the core tops from the fossil pollen dataset, and then limit fossil pollen sites to just the southwestern United States.
if (!file.exists(here::here("vignettes/data/fossil_metadata_counts_final2.rds"))) {
  # Remove the coretop samples from the fossil pollen dataset.
  fossil_metadata_counts <-
    dplyr::anti_join(NAfossil_metadata_counts,
                     coreTops,
                     by = c("dataset.id", "site.id", "sample.id")) %>%
    sf::st_as_sf(coords = c("long", "lat"), crs = 4326)
  
  # Limit fossil pollen sites to just the southwestern United States.
  # First, create a bounding box for the Upper United States Southwest (after Bocinsky and Kohler 2014 - https://doi.org/10.1038/ncomms6618).
  bb <-
    c(
      "xmin" = -113,
      "xmax" = -105,
      "ymin" = 31.334,
      "ymax" = 38.09
    ) %>%
    sf::st_bbox() %>%
    sf::st_as_sfc() %>%
    sf::st_as_sf(crs = 4326) %>%
    sf::st_transform(crs = 4326)
  
  # Do an intersection and keep only the sites that are within the southwestern United States, and remove any samples that do not have an age.
  fossil_metadata_counts_final2 <-
    sf::st_intersection(fossil_metadata_counts, bb) %>%
    filter(!is.na(age))
  
  readr::write_rds(
    fossil_metadata_counts_final2,
    here::here("vignettes/data/fossil_metadata_counts_final2.rds")
  )
  
}
fossil_metadata_counts_final2 <-
  readr::read_rds(here::here("vignettes/data/fossil_metadata_counts_final2.rds"))


# Limit samples to post 5500 BP.
if (!file.exists(here::here("vignettes/data/fossil_final_west_metadata_post5500BP.rds"))) {
  fossil_final_west_metadata_post5500BP <-
    fossil_metadata_counts_final2 %>%
    filter(age <= 5500)
  
  readr::write_rds(
    fossil_final_west_metadata_post5500BP,
    here::here("vignettes/data/fossil_final_west_metadata_post5500BP.rds")
  )
}
fossil_final_west_metadata_post5500BP <-
  readr::read_rds(here::here("vignettes/data/fossil_final_west_metadata_post5500BP.rds"))

# Cleaning up fossil dataset.
fossil_final <-
  fossil_metadata_counts_final2 %>%
  filter(!is.na(age)) %>% 
  dplyr::select(!c(site.id, site.name:pub_year)) %>%
  tibble::as_tibble() %>%
  # For the pollen columns, replace any NA with a 0.
  dplyr::mutate(across(-c(dataset.id, geometry), ~ tidyr::replace_na(.x, 0))) %>%
  # For the pollen columns, replace any negative number with a 0 (in case of any errors).
  dplyr::mutate(dplyr::across(!c(dataset.id, geometry), ~ ifelse(.x < 0, 0, .x))) %>%
  #tibble::column_to_rownames("sample.id") %>% 
  tidyr::nest(pollen.counts = -c(dataset.id, geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(dataset.id,
                pollen = pollen.counts) %>% 
  # Add column that shows the total number of samples for the core.
  mutate(n_samples = map_dbl(pollen, nrow))

```

## Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).

```{r extract_prism_normals, echo = FALSE, warning=FALSE}

#Load prism data. Note: This is only if having trouble installing/loading paleomat package.
# filenames <- list.files("data", pattern="*.rda", full.names=TRUE)
# lapply(filenames,load,.GlobalEnv)
#load(file = "data/prism_1.rda")

# Extract Prism climate data for modern pollen sites and limit to the western United States.
if (!file.exists(here::here("vignettes/data/MPCT_climate_final.rds"))) {
MPCT_climate <- 
  MPCT_metadata_counts_final %>%
  dplyr::select(sample.id) %>%
  extract_prism_normals() %>%
  sf::st_drop_geometry() %>%
  dplyr::left_join(MPCT_metadata_counts_final, by = "sample.id") %>%
  dplyr::select(sample.id,
                prism.normals,
                geometry,
                ABIES:XANTHIUM) %>%
  tidyr::nest(pollen.counts = c(-sample.id,-prism.normals,-geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(sample.id,
                climate = prism.normals,
                pollen = pollen.counts)

# Do an intersection and keep only the sites that are within the western continental United States.
MPCT_climate_final <-
  sf::st_intersection(MPCT_climate, west_in_states)

  readr::write_rds(
    MPCT_climate_final,
    here::here("vignettes/data/MPCT_climate_final.rds")
  )
}
MPCT_climate_final <-
  readr::read_rds(here::here("vignettes/data/MPCT_climate_final.rds"))

```


## Finalize Modern and Fossil Datasets
```{r finalize-datasets, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Modern Dataset

# Clean up modern pollen and climate dataframe to prepare for transfer function and reconstruction diagnostics. Here, we transform the pollen data into proportions.
MPCT_proportions <-
  MPCT_climate_final %>%
  dplyr::mutate(
    pollen =
      purrr::map(pollen,
                 function(x) {
                   x / rowSums(x)
                 }),
    climate =
      purrr::map(climate,
                 function(x) {
                   x %>%
                     dplyr::select(month, tavg)
                 })
  )

# Put the modern pollen data into a dataframe and make the sample.id as the row names.
spp <- bind_rows(MPCT_proportions$pollen) %>% 
  bind_cols(sample.id = MPCT_proportions$sample.id, .) %>% 
  tibble::column_to_rownames("sample.id")

# Put the modern climate data into a dataframe and make the sample.id as the row names.
climate_full <- MPCT_proportions %>% 
  dplyr::select(sample.id, climate) %>% 
  unnest(climate)

# Pull just the climate (tavg) values as a vector.
env <- climate_full %>%
  pull(tavg)

# Turn climate value (tavg) vector into a named vector with the sample ids.
names(env) <- c(MPCT_proportions$sample.id)


# Fossil Dataset

if (!file.exists(here::here("vignettes/data/fossil_pollen.rds"))) {
  # Prepare fossil pollen dataset for transfer function and reconstruction diagnostics.
  fossil_final$pollen <-
    purrr::map(fossil_final$pollen, function(x)
      tibble::column_to_rownames(x, "sample.id"))
  
  # Transform fossil pollen data into proportions, as we did with the modern pollen data above.
  fossil_pollen <- fossil_final %>%
    dplyr::mutate(pollen =
                    purrr::map(pollen,
                               function(x) {
                                 x / rowSums(x)
                               }))
  
  readr::write_rds(fossil_pollen,
                   here::here("vignettes/data/fossil_pollen.rds"))
}
fossil_pollen <-
  readr::read_rds(here::here("vignettes/data/fossil_pollen.rds"))


```


# Reconstruction

## Transfer Function and Reconstruction Diagnostics

Here, we use several transfer function and reconstruction diagnostics, such as squared residual length, the proportion of variance in the fossil data explained by an environmental reconstruction, and the no-analog threshold.

### Squared-residual Length

```{r squared-residual-length, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Step 1: We use squared residual length. Here, we return the residual length for each sample, and the 0.95 quantile.
rlen_results <-
  purrr::map(
    fossil_pollen$pollen,
    .f = function(x) {
      rlen <- residLen(spp, env, x, method = "cca")
      
      list(res_lengths = stack(rlen$passive),
           quantile_95 = as.numeric(quantile(rlen$train, probs = 0.95)))
    }
  )

# Extract the 0.95 quantile baseline to be used for sample removal.
rlen_ninety5quant <- rlen_results[[1]]$quantile_95

# Put the squared residual length and sample.id into one dataframe.
rlen_results <- purrr::map_dfr(rlen_results, `[[`, 1) %>%
  dplyr::rename("Squared_res_lengths" = 1, "sample.id" = 2)

# Extract the sample.ids for samples that will need to be removed since they did not meet the 0.95 threshold.
rlen_removal <-
  as.numeric(levels(rlen_results$sample.id))[rlen_results$Squared_res_lengths > rlen_ninety5quant]

# Results
rlen_full_results1 <- rlen_results %>%
  mutate(sample.id = as.integer(levels(sample.id))) %>%
  filter(sample.id %in% fossil_final_west_metadata_post5500BP$sample.id)

if (!file.exists(here::here("vignettes/data/rlen_full_results2.rds"))) {
  rlen_full_results2 <-
    structure(list(res_lengths = rlen_full_results1,
                   quantile_95 = rlen_ninety5quant),
              class = "list")
  
  readr::write_rds(rlen_full_results2,
                   here::here("vignettes/data/rlen_full_results2.rds"))
}
rlen_full_results2 <-
  readr::read_rds(here::here("vignettes/data/rlen_full_results2.rds"))

fos <- fossil_pollen
fos$pollen <-
  purrr::map(
    fos$pollen,
    ~ .x %>% tibble::rownames_to_column("sample.id") %>% filter(!sample.id %in% rlen_removal) %>% tibble::column_to_rownames("sample.id")
  )


# Remove samples that did not meet the 0.95 threshold from the fossil pollen dataset.
if (!file.exists(here::here("vignettes/data/fos2.rds"))) {
  # Remove any sites that have 0 or 1 samples from the fossil pollen dataset.
  fos2 <- fos %>%
    dplyr::select(-n_samples) %>%
    mutate(rows = map_dbl(pollen, ~ nrow(.x))) %>%
    filter(!rows %in% c(0, 1))
  
  readr::write_rds(fos2,
                   here::here("vignettes/data/fos2.rds"))
}
fos2 <-
  readr::read_rds(here::here("vignettes/data/fos2.rds"))

```

### randomTF Testing

```{r randomTF, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Step 2: We use Random TF to remove sites where tJuly cannot explain at least 90% of the variance. 
if (!file.exists(here::here("vignettes/data/sig_results2_final.rds"))) {
  sig_results2_full <-
    map2(fos2$pollen, fos2$dataset.id, function(x, y) {
      output <- randomTF(
        spp = spp,
        env = data.frame(tjul = env),
        fos = x,
        n = 10000,
        fun = MAT,
        col = 1
      )
      prop <- output$EX
      names(prop) <- y
      sig_out <- output$sig
      names(sig_out) <- y
      return(list(prop, sig_out))
    })
  
  # Extract the proportion explained variance and the dataset ID.
  sig_results2_proportions <- sapply(sig_results2_full, "[[", 1) %>%
    stack(.) %>%
    rename(dataset.id = ind, proportion = values)
  
  # Extract the p-values and the dataset ID.
  sig_results2_pvalues <- sapply(sig_results2_full, "[[", 2) %>%
    stack(.) %>%
    rename(dataset.id = ind, pvalue = values)
  
  # Put the proportion of variance and p-value into the same dataframe.
  sig_results2_final <-
    left_join(sig_results2_proportions, sig_results2_pvalues, by = "dataset.id") %>%
    dplyr::select(dataset.id, proportion, pvalue) %>%
    mutate(dataset.id = as.numeric(levels(dataset.id)))
  
  readr::write_rds(sig_results2_final,
                   here::here("vignettes/data/sig_results2_final.rds"))
}
sig_results2_final <-
  readr::read_rds(here::here("vignettes/data/sig_results2_final.rds")
  )

# Pull out the dataset IDs that have p-values less than the 0.10.
only_90b <- sig_results2_final %>%
  filter(pvalue <= 0.10) %>% 
  pull(dataset.id)

# Finally, subset the fossil pollen dataset by keeping only the dataset IDs that passed randomTF.
fos3_90 <- fos %>% 
  filter(dataset.id %in% only_90b)

```


### MAT Transfer Function and Prediction

```{r MAT_transfer_prediction, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Step 3: Run MAT and do prediction/reconstruction.

# Remove any columns in the modern pollen dataset that have 0 for the pollen taxa. 
pollen_input <- spp %>% 
  dplyr::select(which(colSums(.) > 0))

# Remove any columns in the fossil pollen dataset that have 0 for the pollen taxa. 
fossil_input <- do.call(rbind, unname(fos3_90$pollen)) %>% 
  dplyr::select(which(colSums(.) > 0))

# Keep only columns that are the same between the modern and fossil pollen dataset, as they need to be the same to run MAT.
keep_cols <- colnames(pollen_input[colSums(pollen_input) > 0])
keep_cols <- keep_cols[keep_cols %in% colnames(fossil_input)]

pollen_input <- pollen_input[, keep_cols]
fossil_input <- fossil_input[, keep_cols]

# Save output of MAT if not alaready created.
if (!file.exists(here::here("vignettes/data/mat_calibration.rds"))) {
  # Create the transfer function using mat from the analogue package.
  swap.mat2 <- mat(pollen_input, env, method = "SQchord")
  
  readr::write_rds(swap.mat2,
                   here::here("vignettes/data/mat_calibration.rds"))
}
swap.mat2 <-
  readr::read_rds(here::here("vignettes/data/mat_calibration.rds"))

# Here, we run the prediction twice, so that we can extract the minimum dissimilarity from the predict.mat object.

if (!file.exists(here::here("vignettes/data/mat_prediction.rds"))) {
  # Create the transfer function using mat from the analogue package.
  rlgh.mat2 <- predict(object = swap.mat2, newdata = fossil_input, k = 4)
  
  readr::write_rds(rlgh.mat2,
                   here::here("vignettes/data/mat_prediction.rds"))
}
rlgh.mat2 <-
  readr::read_rds(here::here("vignettes/data/mat_prediction.rds"))

# Here, we run the prediction again in order to get the bootstrap estimates.
if (!file.exists(here::here("vignettes/data/mat_prediction_bootstrap.rds"))) {
  # Create the transfer function using mat from the analogue package.
  rlgh.mat2b4 <- predict(object = swap.mat2, newdata = fossil_input, k = 4, bootstrap = T)
  
  readr::write_rds(rlgh.mat2b4,
                   here::here("vignettes/data/mat_prediction_bootstrap.rds"))
}
rlgh.mat2b4 <-
  readr::read_rds(here::here("vignettes/data/mat_prediction_bootstrap.rds"))

# Here, we pull out the data from the predict.mat object. First, we pull out the predicted values for the four best analogues. In the analogue package, the analogues have cumulative means, so we only need to extract the fourth record here.
mat_prediction_4 <- as.data.frame(rlgh.mat2b4$predictions$model$predicted[4,]) %>%
  tibble::rownames_to_column("sample.id") %>%
  rename(value = 2) %>% 
  # Pull out the bootstrap predicted values.
  left_join(., as.data.frame(rlgh.mat2b4[["predictions"]][["bootstrap"]][["predicted"]][,4]) %>% tibble::rownames_to_column("sample.id") %>%
  rename(bootstrap = 2), by = "sample.id") %>% 
  # Pull out the bootstrap rmsep.
  left_join(., as.data.frame(rlgh.mat2b4[["predictions"]][["sample.errors"]][["rmsep"]][,4]) %>% tibble::rownames_to_column("sample.id") %>%
  rename(rmsep = 2), by = "sample.id") %>% 
  # Pull out the bootstrap s1 errors.
    left_join(., as.data.frame(rlgh.mat2b4[["predictions"]][["sample.errors"]][["s1"]][,4]) %>% tibble::rownames_to_column("sample.id") %>%
  rename(s1 = 2), by = "sample.id")

```


### No Analog Threshold (or Minimum Dissimilarity)

```{r MAT_transfer_prediction, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Step 4: Remove any fossil pollen samples that have a minimum dissimilarity that is greater than the 5% quantile.
if (!file.exists(here::here("vignettes/data/minDC_full_results.rds"))) {
  # Get a list of the samples that remain in the fossil pollen dataset after rlen and the randomTF removal steps.
  pre_minDC_samples <- rlen_full_results1 %>%
    filter(!sample.id %in% rlen_removal) %>%
    left_join(
      .,
      fossil_final_west_metadata_post5500BP %>%
        dplyr::select(dataset.id, site.id, sample.id),
      by = "sample.id"
    ) %>%
    filter(dataset.id %in% only_90b) %>%
    pull(sample.id)
  
  # Get the minimum dissimilarity for each sample.
  rlgh.mdc <- minDC(rlgh.mat2)
  
  # Put the results into a list.
  minDC_results <- stack(rlgh.mdc$minDC) %>%
    mutate(ind = as.integer(levels(ind))) %>%
    filter(ind %in% pre_minDC_samples) %>%
    left_join(
      .,
      fossil_final_west_metadata_post5500BP %>%
        dplyr::select(sample.id, dataset.id, site.id, age) %>%
        mutate(age = BPtoBCAD(age)) %>%
        sf::st_drop_geometry(),
      by = c("ind"  = "sample.id")
    )
  
  minDC_full_results <- structure(list(minDC = minDC_results,
                                       quantiles = rlgh.mdc$quantiles),
                                  class = "list")
  
  readr::write_rds(minDC_full_results,
                   here::here("vignettes/data/minDC_full_results.rds"))
}
minDC_full_results <-
  readr::read_rds(here::here("vignettes/data/minDC_full_results.rds"))


# Put data into a list. Create list of samples that did and did not meet the threshold, and subset the fossil pollen data to the samples that did pass the minDC test.
if (!file.exists(here::here("vignettes/data/minDC_summary.rds"))) {
  remove_samples <- as.data.frame(rlgh.mdc$minDC) %>%
    tibble::rownames_to_column("sample.id") %>%
    rename(dissimilarity = 2) %>%
    filter(dissimilarity > as.numeric(rlgh.mat2$quantiles["5%"]))
  
  
  mat_prediction_cleaned <- mat_prediction_4 %>%
    filter(!sample.id %in% remove_samples$sample.id)
  
  # Final Sample set.
  final_approved_samples <- minDC_full_results$minDC %>%
    filter(!ind %in% remove_samples$sample.id) %>%
    pull(ind)
  
  minDC_summary <- list(
    remove_samples = remove_samples,
    mat_prediction_cleaned = mat_prediction_cleaned,
    final_approved_samples = final_approved_samples
  )
  
  readr::write_rds(minDC_summary,
                   here::here("vignettes/data/minDC_summary.rds"))
}
minDC_summary <-
  readr::read_rds(here::here("vignettes/data/minDC_summary.rds"))

# Need to update age models for some fossil pollen sites that have not been updated with the Bacon age models. Here, we read in the updated age models from individual .csv files.
updated_age_models <-
  list.files("./vignettes/data/updated_age_models", full.names = TRUE) %>%
  map_dfr(read.csv)

# Clean up fossil pollen data to create the final dataset of samples.
if (!file.exists(here::here("vignettes/data/fos_90.rds"))) {
  fos_90 <- mat_prediction_cleaned %>%
    mutate(sample.id = as.integer(sample.id)) %>%
    filter(sample.id %in% final_approved_samples) %>%
    dplyr::left_join(.,
                     fossil_metadata_counts_final %>%
                       dplyr::select(dataset.id:pub_year),
                     by = "sample.id") %>%
    filter(age <= 5500) %>%
    dplyr::left_join(updated_age_models,
                     by = c("site.id", "dataset.id", "sample.id")) %>%
    dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
    dplyr::select(-age.y,-age.x) %>%
    dplyr::mutate(age = round(age)) %>%
    sf::st_as_sf()
  
  readr::write_rds(fos_90,
                   here::here("vignettes/data/fos_90.rds"))
}
fos_90 <-
  readr::read_rds(here::here("vignettes/data/fos_90.rds"))








# COMBO #3 - ONLY NO ANALOGUE REMOVAL.

fos <- fossil_pollen
fos2 <- fos %>% 
  dplyr::select(-n_samples) %>% 
  mutate(rows = map_dbl(pollen, ~ nrow(.x))) %>% 
  filter(!rows %in% c(0, 1))

pollen_input <- spp %>% 
  dplyr::select(which(colSums(.) > 0))

fossil_input <- do.call(rbind, unname(fos2$pollen)) %>% 
  dplyr::select(which(colSums(.) > 0))

keep_cols <- colnames(pollen_input[colSums(pollen_input) > 0])
keep_cols <- keep_cols[keep_cols %in% colnames(fossil_input)]

pollen_input <- pollen_input[, keep_cols]
fossil_input <- fossil_input[, keep_cols]


swap.mat2 <- mat(pollen_input, env, method = "SQchord")

rlgh.mat2 <- predict(object = swap.mat2, newdata = fossil_input)

mat_prediction <- as.data.frame(rlgh.mat2$predictions$model$predicted[4,]) %>%
  tibble::rownames_to_column("sample.id") %>%
  rename(value = 2)

rlgh.mdc <- minDC(rlgh.mat2)

remove_samples <- as.data.frame(rlgh.mdc$minDC) %>%
  tibble::rownames_to_column("sample.id") %>%
  rename(dissimilarity = 2) %>%
  filter(dissimilarity > as.numeric(rlgh.mat2$quantiles["5%"]))

mat_prediction_cleaned <- mat_prediction %>%
  filter(!sample.id %in% remove_samples$sample.id)

fos_90 <- mat_prediction_cleaned %>% 
  mutate(sample.id = as.integer(sample.id)) %>% 
  dplyr::left_join(., fossil_metadata_counts_final %>% 
  dplyr::select(dataset.id:pub_year), by = "sample.id") %>% 
  #dplyr::mutate(error = new_output_recont_fos3_90$SEP.boot[, "MAT"]) %>% 
  filter(age <= 5500) %>% 
  dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
  dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
  dplyr::select(-age.y, -age.x) %>%
  dplyr::mutate(age = round(age)) %>% 
  # filter(site.id == 246) %>% 
  sf::st_as_sf()

write.csv(fos_90, "Tim_NoAnalogue_Only_1best_analogs.csv", row.names = FALSE)

```


# Reconstruction

## Model Summary

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Now, we carry out the reconstruction for all fossil pollen sites in the Upper United States Southwest study area.

# Average Temperature
# The reconstructions are in one output file, which has the reconstructed mean temperature (tavg) and associated error for each month.

# First, to save time, check to see if the sites have already been reconstructed, if so, then this chunk will not run. If you would like to re-run the reconstructions, then simply delete the reconstruction .csv and re-run it.

if (!file.exists("./vignettes/data/reconst_tavg_west_3.rds")) {
  mat_models_temp_west_3 <- purrr::map(
    1:12,
    .f = function(j) {
      rioja::MAT(
        y = dplyr::bind_rows(MPCT_final$pollen),
        x = dplyr::bind_rows(MPCT_final$climate)[[paste0("tavg_", 7)]],
        k = 20,
        dist.method = "sq.chord",
        lean = FALSE
      )
    }
  )

  reconst_tavg_west_3 <- mat_models_temp_west_3 %>%
    purrr::map(
      .f = function(the.model) {
        # Create the MAT reconstruction.
        predict(
          object = the.model,
          newdata = dplyr::bind_rows(fossil_final$pollen) %>%
            as.data.frame() %>%
            set_rownames(fossil_final$sample.id),
          k = 3,
          sse = TRUE
        )
      }
    )

  readr::write_rds(reconst_tavg_west, "./vignettes/data/reconst_tavg_west_3.rds")

} else if (!exists("reconst_tavg_west_3")) {
  reconst_tavg <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg_west_3.rds"))

} else {
  stop(
    "The SWUS reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the directory."
  )

}





# Previous one
# Here, I use the analogue package rather than rioja. Also able to remove the dissimilarities when greater than 5%.


ok <- tavg_new %>% 
  filter(dataset.id == 25551) %>% 
  pull(sample.id)

depth <- tavg_new %>% 
    filter(age <= 5500 | !is.na(age)) %>% 
    filter(dataset.id == 25551) %>% 
    pull(depth)

# keep <- tavg_new %>% 
#     filter(age <= 5500 | !is.na(age)) %>% 
#     #filter(dataset.id == 25551) %>% 
#     pull(sample.id)
  
remove_older <- fossil_metadata_counts_final %>% 
  filter(age >= 5500 | is.na(age)) %>% 
  pull(sample.id)

modern_p <- bind_rows(MPCT_climate_final$pollen)
modern_p <- cbind(MPCT_climate_final$sample.id, modern_p)
modern_p <- tibble::column_to_rownames(modern_p, "MPCT_climate_final$sample.id")
modern_p2 <- dplyr::select(modern_p, which(colSums(modern_p) > 0))
modern_p_prop <- modern_p2/rowSums(modern_p2)

fossil_final_reduced <- fossil_final %>% 
  #filter(sample.id %in% keep) #%>% 
  filter(!sample.id %in% remove_older) %>% 
  filter(sample.id %in% ok)
fossil_p <- data.frame(bind_rows(fossil_final_reduced$pollen))
row.names(fossil_p) <- depth
fossil_p2 <- dplyr::select(fossil_p, which(colSums(fossil_p) > 0))
fossil_p_prop <- fossil_p2/rowSums(fossil_p2)


keep_cols <- colnames(modern_p_prop[colSums(modern_p_prop) > 0])
keep_cols <- keep_cols[keep_cols %in% colnames(fossil_p_prop)]

pollen_input <- modern_p_prop[, keep_cols]
# pollen_input <- cbind(MPCT_climate_final$sample.id, pollen_input)
# pollen_input <- tibble::column_to_rownames(pollen_input, "MPCT_climate_final$sample.id")

fossil_input <- fossil_p_prop[, keep_cols]

month_name <- data.frame(month = c(1:12), labels = c("tjan", "tfeb", "tmar", "tapr", "tmay", "tjun", "tjul", "taug", "tsep", "toct", "tnov", "tdec"))

climate_input <- bind_rows(MPCT_climate_final$climate) %>% 
  select(month, tavg) %>% 
  left_join(., month_name, by = "month") %>% 
  select(month = labels, tavg) %>% 
  group_by(grp = as.integer(gl(n(), 12, n()))) %>% 
  pivot_wider(names_from = "month", values_from = "tavg") %>% 
  ungroup %>% 
  select(-grp)
climate_input <- filter(climate_input, month == 7)
climate_input <- select(climate_input, tavg)
climate_input <- pull(climate_input, tavg)
names(climate_input) <- c(MPCT_climate_final$sample.id)

swap.mat2 <- mat(pollen_input, climate_input, method = "SQchord")

rlgh.mat2 <- predict(object = swap.mat2, newdata = fossil_input, k = 4)


mat_prediction <- as.data.frame(rlgh.mat2$predictions$model$predicted[4,]) %>%
  tibble::rownames_to_column("sample.id") %>%
  rename(value = 2)

rlgh.mdc <- minDC(rlgh.mat2)


# This is to change the names from sample.id to age in order to get the min. dissimilarity plot.
rename_mindc <- fossil_metadata_counts_final %>% 
  filter(sample.id %in% names(rlgh.mdc$minDC)) %>% 
  select(sample.id, age) %>% 
  sf::st_drop_geometry()

rlgh.mdc2 <- rlgh.mdc
names(rlgh.mdc2$minDC) <- rename_mindc$age[match(names(rlgh.mdc2$minDC),rename_mindc$sample.id)]
rlgh.mdc2$minDC <- rlgh.mdc2$minDC[order(as.numeric(names(rlgh.mdc2$minDC)))]

plot(rlgh.mdc2, use.labels = TRUE, xlab = "Age (Years BP)")
# End diss. plot with percentiles.


remove_samples <- as.data.frame(rlgh.mdc$minDC) %>%
  tibble::rownames_to_column("sample.id") %>%
  rename(dissimilarity = 2) %>%
  filter(dissimilarity > as.numeric(rlgh.mat2$quantiles["5%"]))


mat_prediction_cleaned <- mat_prediction %>%
  filter(!sample.id %in% remove_samples$sample.id)


```

## Apply Threshold to Reconstruction

```{r clim_threshold, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Apply a threshold to the reconstruction, so that if an analogue is too dissimilar that it will not be used in the reconstruction. Then, based on the remaining analogues, the final reconstruction and associated error will be re-calculated.

#tavg
if (!file.exists("./vignettes/data/reconst_tavg_threshold_west_3.rds")) {
  reconst_tavg_threshold_west_3 <-
    reconst_tavg_west_3 %>%
    purrr::map(
      .f = function(the.reconst) {
        apply_threshold(x = the.reconst,
                        qtile = .995)
      }
    )

  readr::write_rds(reconst_tavg_threshold_west_3,
                   "./vignettes/data/reconst_tavg_threshold_west_3.rds")

} else if (!exists("reconst_tavg_threshold_west_3")) {
  reconst_tavg_threshold_west_3 <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg_threshold_west_3.rds"))

} else {
  stop(
    "The SWUS reconstruction threshold has already been applied. If you would like to re-run the threshold, then delete the file from the directory."
  )

}

```

## Simplify Reconstruction Output

```{r clim_cleanup, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Simplify tavg

# First, need to update 12 fossil pollen sites age models that have been done using Bacon. Here, we read in the updated age models from individual .csv files.
setwd("./vignettes/data/updated_age_models")
file_names <- dir()
updated_age_models <- do.call(rbind,lapply(file_names,read.csv))
# Return back to parent folder for the paleomat directory.
setwd("./../../..")


# Put the reconstructed climate data into a tibble, using the simplify_reconstruction function. The function takes two parameters, the full reconstruction dataset (reconstruction.data) and the fossil dataset (with metadata and counts) that was used initially loaded that has all of the metadata. The function will select only the metadata and ignore the pollen counts.
if (!file.exists("./vignettes/data/reconst_tavg_final_west_3.rds")) {
  reconst_tavg_final_west_3 <-
    simplify_reconstruction(reconstruction.data = reconst_tavg_threshold_west_3,
                            fossil.metadata = fossil_metadata_counts_final) %>%
    # Remove the rows that have NAs for the reconstructed value (i.e., this indicates that there were too few analogues to do a reconstruction). Also, remove rows that do not have an age chronology.
    tidyr::drop_na(age, value) %>%
    # Update age models for multiple fossil pollen sites.
    dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
    dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
    dplyr::select(-age.y, -age.x) %>%
    dplyr::mutate(age = round(age))

  readr::write_rds(reconst_tavg_final_west_3,
                   "./vignettes/data/reconst_tavg_final_west_3.rds")

} else if (!exists("reconst_tavg_final_west_3")) {
  reconst_tavg_final_west_3 <-
    readr::read_rds(here::here("./vignettes/data/reconst_tavg_final_west_3.rds"))

} else {
  stop(
    "The SWUS reconstruction has already been simplified. If you would like to re-run the code, then delete the file from the directory."
  )

}


newoutput <- left_join(mat_prediction_cleaned %>% mutate(sample.id = as.integer(sample.id)), fossil_metadata_counts_final %>% dplyr::select(dataset.id:pub_year), by = "sample.id") %>% 
    dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
    dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
    dplyr::select(-age.y, -age.x) %>%
    dplyr::mutate(age = round(age))




# Working on the 90% and 95% reconstructions.
new_output_recont_fos3_90
new_output_recont_fos2_95

fossil_metadata_counts_final

fos_90 <- new_output_recont_fos3_90$fit[, "MAT"] %>% 
  stack() %>% 
  tibble::as_tibble() %>% 
  mutate(ind = as.numeric(levels(ind))[ind]) %>% 
  rename(sample.id = ind) %>% 
  dplyr::left_join(., fossil_metadata_counts_final %>% 
  dplyr::select(dataset.id:pub_year), by = "sample.id") %>% 
  dplyr::mutate(error = new_output_recont_fos3_90$SEP.boot[, "MAT"]) %>% 
  filter(age <= 5500) %>% 
  dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
  dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
  dplyr::select(-age.y, -age.x) %>%
  dplyr::mutate(age = round(age)) %>% 
  # filter(site.id == 246) %>% 
  sf::st_as_sf() #%>% 
  # mutate(date = BPtoBCAD(age)) %>% 
  # mutate(anom = values - 14.569209613)

# fos_95 <- new_output_recont_fos2_95$fit[, "MAT"] %>% 
#   stack() %>% 
#   tibble::as_tibble() %>% 
#   mutate(ind = as.numeric(levels(ind))[ind]) %>% 
#   rename(sample.id = ind) %>% 
#   dplyr::left_join(., fossil_metadata_counts_final %>% 
#   dplyr::select(dataset.id:pub_year), by = "sample.id") %>% 
#   dplyr::mutate(error = new_output_recont_fos2_95$SEP.boot[, "MAT"])  %>% 
#   filter(age <= 5500) %>% 
#   dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
#   dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
#   dplyr::select(-age.y, -age.x) %>%
#   dplyr::mutate(age = round(age)) %>% 
#   # filter(site.id == 246) %>% 
#   sf::st_as_sf() #%>% 
#   # mutate(date = BPtoBCAD(age)) %>% 
#   # mutate(anom = values - 14.569209613)

```


# Spatial and Temporal Interpolation

```{r spatiotemporal_interpolation, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

#Just temporary as the anomaly is actually calculated at the bottom of this code chunk.
warmest.month = 15.6182926829268

# Put lat and long into individual columns.
tavg <- reconst_tavg_final_west_3 %>%
  tidyr::unnest(geometry) %>%
  dplyr::group_by(sample.id, month) %>%
  dplyr::mutate(col = seq_along(sample.id)) %>% #add a column indicator
  tidyr::spread(key = col, value = geometry) %>%
  dplyr::rename(long = "1", lat = "2") %>%
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age < 5950 & age >= 0 & month == 7) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-4000, 2000, 100))) %>%
  dplyr::mutate(anom = value - warmest.month)


# After more closely examining the Beef Pasture data, the spline was heavily impacted by Petersen's Beef Pasture data. The data more or less produced an extremely smooth spline that led to underfitting. This could be the result of the age model on the Petersen data, which has way fewer dates than the Wright Beef Pasture data. Therefore, I use the Wright data for 1-2000 AD and the Petersen data for the time before 1AD (as the Wright data does not any farther back in time).
# Also, filter out the earliest date for Sagehen Marsh as there is too large of a time gap between the first point and the next point (about 1102 years [i.e., 600 to 1702]).
tavg_predicted_west <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::filter(!(site.id == 2241 & date <= 1600)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(site.data = purrr::map(site.data,
                                       function(x) {
                                         x %>%
                                           dplyr::group_by(age) %>%
                                           dplyr::mutate(
                                             value = mean(value),
                                             error = mean(error),
                                             anom = mean(anom)
                                           ) %>%
                                           dplyr::distinct(age, .keep_all = TRUE) %>%
                                           dplyr::arrange(age)
                                       })) %>%
  # sf::st_as_sf(coords = c("long", "lat"),
  #                crs = 4326) %>%
  dplyr::mutate(mean_100yrs = purrr::map(
    site.data,
    .f = function(x) {
      interpolate_time(temp_data = x,
                       model = "tps")
    }
  )) %>%
  dplyr::select(-site.data) %>%
  dplyr::arrange(site.name) %>%
  unnest_wider(mean_100yrs) %>%
  dplyr::rename(mean_100yrs = 6,
                model_fit = 7,
                plot_fit = 8)

saveRDS(tavg_predicted_west, "./output/temporal_interpolations.rds")

# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
tavg_100yrs <- tavg_predicted_west %>%
  dplyr::select(-model_fit,-plot_fit) %>%
  tidyr::unnest(mean_100yrs) %>%
  dplyr::filter(date >= -1000 & date <= 1800) %>%
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)

# Extract the modern temperature from the PRISM data at each of the fossil pollen locations. Then, calculate the anomaly for each fossil pollen sample.
site.preds.unlisted <- unnest(tavg_100yrs, cols = site.predictions)
modern_temperature <-
  as.data.frame(
    raster::extract(
      x = temp.raster,
      y = site.preds.unlisted %>% ungroup() %>%
        dplyr::select(long, lat)
    )
  ) %>%
  dplyr::rename(modern = 1)

site.preds.unlisted <-
  cbind(site.preds.unlisted, modern_temperature$modern) %>%
  dplyr::rename(modern = 11)

site.preds.unlisted.anom <- site.preds.unlisted %>%
  rowwise() %>%
  dplyr::mutate(anom = value - modern)

# here, we nest the data into each time bin.
site.preds.unlisted.anom.nested <- site.preds.unlisted.anom %>%
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)


x_range <-
  as.numeric(c(-112.81446,-105.52416))  # min/max longitude of the interpolation area
y_range <-
  as.numeric(c(32.65757, 38.089))  # min/max latitude of the interpolation area
# create an empty grid of values ranging from the xmin-xmax, ymin-ymax
grd <- expand.grid(
  x = seq(from = x_range[1],
          to = x_range[2],
          by = 0.1),
  y = seq(from = y_range[1], to = y_range[2],
          by = 0.1)
)

# expand points to grid
coordinates(grd) <- ~ x + y
# turn into a spatial pixels object
gridded(grd) <- TRUE


# Do the interpolation using the inverse distance weighted method from gstat.
tavg_idw <-
  purrr::map(
    site.preds.unlisted.anom.nested$site.predictions,
    .f = function(x) {
      coordinates(x) <- ~ long + lat
      idw_pow2 <- gstat::idw(
        formula = anom ~ 1,
        locations = x,
        newdata = grd,
        idp = 1
      )
      idw_pow3 <- raster(idw_pow2)
      crs(idw_pow3) <-
        "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
      return(idw_pow3)
    }
  )

tavg_idw_stack <- raster::stack(tavg_idw)
names(tavg_idw_stack) <- c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")

tavg_idw_brick <- raster::stack(tavg_idw)
names(tavg_idw_brick) <- c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")

writeRaster(
  tavg_idw_brick,
  filename = "./output/paleomat-rasters.tif",
  options = "INTERLEAVE=BAND",
  overwrite = TRUE
)

# plot(tavg_idw_stack[[1:16]], col = colors)
# plot(tavg_idw_stack[[17:28]], col = colors)


# Calculate the anomaly across time for entire area (to produce line graph).
anom_new <- purrr::map(tavg_idw, cellStats, mean) %>%
  as.data.frame() %>%
  tidyr::pivot_longer(cols = 1:28) %>%
  dplyr::mutate(name = seq(-950, 1750, 100)) %>%
  dplyr::rename(year = name, anom = value)

# Write results to a csv.
write.csv(anom_new,
          "./output/paleomat-temp-anomaly-final.csv",
          row.names = FALSE)


# For figures for journal article, please see "Paleomat Figures.Rmd".

```




# Spatial and Temporal Interpolation for analogue package

```{r spatiotemporal_interpolation2, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# get_midpoints <- function(x, dp=2){
#     lower <- as.numeric(gsub(",.*","",gsub("\\(|\\[|\\)|\\]","", x)))
#     upper <- as.numeric(gsub(".*,","",gsub("\\(|\\[|\\)|\\]","", x)))
#     return(round(lower+(upper-lower)/2, dp))
# }

#Just temporary as the anomaly is actually calculated at the bottom of this code chunk.
#warmest.month = 15.6182926829268

# Put lat and long into individual columns.
tavg_new_90 <- fos_90 %>%
  mutate(long = unlist(purrr::map(.$geometry,1)),
           lat = unlist(purrr::map(.$geometry,2))) %>% 
  dplyr::select(-geometry) %>% 
  sf::st_drop_geometry() %>% 
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age <= 5500) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-5500, 2100, 100))) %>%
  #mutate(observ = n()) %>% 
  mutate(mid_period = get_midpoints(period)) #%>% 
  #dplyr::mutate(anom = value - warmest.month)

# tavg_new_90 %>%
#   filter(date >= -1000) %>%
# ggplot(., aes(x=date, y=value, group=site.name)) +
#   geom_line(aes(color=site.name)) +
#   #geom_smooth(method="auto", se=TRUE, fullrange=TRUE, level=0.95) +
#   facet_wrap(~ site.name, ncol=4)
  

# These samples are not close to other samples; therefore, I pull them out for the temporal interpolation, then they will be readded later.
# sample_filter <- c(41285, 41284, 34305, 31510, 31511, 21844, 21845, 21846, 21847, 21848,
#                    11534, 11535, 11536, 11537, 11538, 11539, 62039, 62041, 
#                    57919, 57920, 57921, 57922, 57923, 57924, 57925, 57926, 57927,
#                    58052, 58053, 58054, 58055, 58056, 58057, 58058, 58215, 58216, 58217,
#                    61609, 61610, 61636, 61637, 62179, 62180, 62181, 62182, 62183, 62184,
#                    136828, 136827, 154427, 154428, 192219)
# 
# individual_samples_filtered <- tavg_new %>% 
#   filter(sample.id %in% sample_filter)
# 
# # The results for this one are extremely odd, so just removing it completely
# remove_site <- c(9830)

# tavg_new %>% 
#   group_by(site.id) %>% 
#   arrange(site.id, age) %>% 
#   filter(site.id == 9830) %>% 
#   mutate(cumbbb = date - lag(date)) %>% 
#   ggplot(aes(x = date, y = value)) +
#   geom_point() +
#   geom_line()


  # mutate(cumbbb = date - lag(date))
  #          cumsum((row_number() > 1) * bbb))


# 
# tavg_predicted_west_new_90 <- tavg_new_90 %>%
#   # group_by(site.name, site.id) %>%
#   # summarise(observ = n())
#   # filter(!sample.id %in% sample_filter) %>% 
#   # filter(site.id != 1661) %>% 
#   filter(!site.id %in% c(1661, 2260, 2241)) %>% 
#   dplyr::group_by(site.id, site.name, elev, long, lat) %>%
#   tidyr::nest() %>%
#   dplyr::rename(site.data = data) %>%
#   dplyr::mutate(site.data = purrr::map(site.data,
#                                        function(x) {
#                                          x %>%
#                                            dplyr::group_by(age) %>%
#                                            dplyr::mutate(
#                                              value = mean(value),
#                                              anom = mean(anom)
#                                            ) %>%
#                                            dplyr::distinct(age, .keep_all = TRUE) %>%
#                                            dplyr::arrange(age)
#                                        })) %>%
#   # sf::st_as_sf(coords = c("long", "lat"),
#   #                crs = 4326) %>%
#   dplyr::mutate(mean_100yrs = purrr::map(
#     site.data,
#     .f = function(x) {
#       interpolate_time(temp_data = x,
#                        model = "tps")
#     }
#   )) %>% 
#   dplyr::select(-site.data) %>%
#   dplyr::arrange(site.name) %>% 
#   dplyr::mutate(mean_100yrs = purrr::map(mean_100yrs, setNames, c("mean_100yrs","model_fit", "plot_fit"))) %>%
#   #mutate(mean_100yrs = map(mean_100yrs, as.list)) %>% 
#   unnest(mean_100yrs) %>%
#   group_by(site.id) %>% 
#   mutate(col = row_number()) %>% 
#   pivot_wider(names_from = col, values_from = mean_100yrs) %>% 
#   dplyr::rename(mean_100yrs = 6,
#                 model_fit = 7,
#                 plot_fit = 8)

#saveRDS(tavg_predicted_west_new, "./output/temporal_interpolations_new.rds")


# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
# tavg_100yrs_new <- tavg_predicted_west_new_90 %>%
#   dplyr::select(-model_fit,-plot_fit) %>%
#   tidyr::unnest(mean_100yrs) %>%
#   bind_rows(., tavg_new_90 %>% filter(site.id %in% c(1661, 2260, 2241)) %>% dplyr::select("site.id", "site.name", "elev", "long", "lat","date"= "mid_period" , "value")) %>% #sf::st_drop_geometry() ) %>% 
#   # bind_rows(., individual_samples_filtered %>% ungroup %>% dplyr::select("site.id", "site.name", "elev", "long", "lat", "mid_period" ,"value") %>% rename(date = "mid_period")) %>% 
#   #dplyr::filter(date >= -3550 & date <= 1800) %>%
#   arrange(date) %>% 
#   dplyr::group_by(date) %>%
#   tidyr::nest() %>%
#   dplyr::rename(site.predictions = data)

# Extract the modern temperature from the PRISM data at each of the fossil pollen locations. Then, calculate the anomaly for each fossil pollen sample.
#site.preds.unlisted <- unnest(tavg_100yrs_new, cols = site.predictions)

modern_temperature <-
  as.data.frame(
    raster::extract(
      x = temp.raster,
      y = tavg_new_90 %>% ungroup() %>%
        dplyr::select(long, lat)
    )
  ) %>%
  dplyr::rename(modern = 1)

site.preds.unlisted <-
  cbind(tavg_new_90, modern = modern_temperature$modern)


site.preds.unlisted.anom <- site.preds.unlisted %>%
  rowwise() %>%
  dplyr::mutate(anom = value - modern) %>% 
  dplyr::mutate(anom_bootstrap = bootstrap - modern)

if (!file.exists(here::here("vignettes/data/final_paleomat_dataset3.rds"))) {
  readr::write_rds(
    site.preds.unlisted.anom,
    here::here("vignettes/data/final_paleomat_dataset3.rds")
  )
}
site.preds.unlisted.anom <-
  readr::read_rds(here::here("vignettes/data/final_paleomat_dataset3.rds")
  )


# #Modern Weighted Mean
# site.preds.unlisted %>% 
#   ungroup %>% 
#   dplyr::select(site.id, modern) %>% 
#   distinct() %>% 
#   left_join(tavg_new_90, ., by = "site.id") %>% 
#   group_by(site.id) %>% 
#   mutate(n = n()) %>% 
#   slice(1) %>% 
#   filter(!site.id %in% c(1661, 1609)) %>% 
#   ungroup %>% 
#   summarise(modern = (n * modern)) %>% 
#   summarise(sum(modern)/818)

# ok <- site.preds.unlisted.anom %>% 
# sf::st_as_sf(coords = c("long", "lat"),
#                   crs = 4326) 

# ok <- site.preds.unlisted.anom %>%   group_by(site.name, site.id) %>%
#     summarise(observ = n(), minDate = min(date), maxDate = max(date))

#write.csv(site.preds.unlisted.anom, "./vignettes/data/output/Anom_All3Steps_withBootstrapErrors.csv", row.names = FALSE)

# Look at the distribution:
# ggplot(site.preds.unlisted.anom, aes(x = anom)) + geom_density(aes(y = ..count..), fill = "lightgray") +
#     geom_vline(aes(xintercept = mean(anom)), 
#                linetype = "dashed", size = 0.6,
#                color = "#FC4E07")

# here, we nest the data into each time bin.
site.preds.unlisted.anom.nested <- site.preds.unlisted.anom %>%
  arrange(date) %>% 
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)


x_range <-
  as.numeric(c(-112.81446,-105.52416))  # min/max longitude of the interpolation area
y_range <-
  as.numeric(c(32.65757, 38.089))  # min/max latitude of the interpolation area
# create an empty grid of values ranging from the xmin-xmax, ymin-ymax
grd <- expand.grid(
  x = seq(from = x_range[1],
          to = x_range[2],
          by = 2.0),
  y = seq(from = y_range[1], to = y_range[2],
          by = 2.0)
)

# expand points to grid
coordinates(grd) <- ~ x + y
# turn into a spatial pixels object
gridded(grd) <- TRUE


# Do the interpolation using the inverse distance weighted method from gstat.
tavg_idw <-
  purrr::map(
    site.preds.unlisted.anom.nested$site.predictions,
    .f = function(x) {
      coordinates(x) <- ~ long + lat
      idw_pow2 <- gstat::idw(
        formula = anom ~ 1,
        locations = x,
        newdata = grd,
        idp = 1
      )
      idw_pow3 <- raster(idw_pow2)
      crs(idw_pow3) <-
        "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
      return(idw_pow3)
    }
  )

date_name_list_BC <- paste0("BC", gsub("-", "", site.preds.unlisted.anom.nested$date[site.preds.unlisted.anom.nested$date < 0]))
date_name_list_AD <- paste0("AD", gsub("-", "", site.preds.unlisted.anom.nested$date[site.preds.unlisted.anom.nested$date > 0]))
date_name_list <- c(date_name_list_BC, date_name_list_AD)

tavg_idw_stack <- raster::stack(tavg_idw)
names(tavg_idw_stack) <- date_name_list
  
  # c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")

tavg_idw_brick <- raster::stack(tavg_idw)
names(tavg_idw_brick) <- date_name_list

# writeRaster(
#   tavg_idw_brick,
#   filename = "./output/paleomat-rasters.tif",
#   options = "INTERLEAVE=BAND",
#   overwrite = TRUE
# )

plot(tavg_idw_stack[[1:16]], col = colors, zlim=c(-4.0, 3.0))
plot(tavg_idw_stack[[17:32]], col = colors, zlim=c(-4.0, 3.0))
plot(tavg_idw_stack[[33:48]], col = colors, zlim=c(-4.0, 3.0))
plot(tavg_idw_stack[[49:55]], col = colors, zlim=c(-4.0, 3.0))


# Calculate the anomaly across time for entire area (to produce line graph).
anom_new <- purrr::map(tavg_idw, cellStats, mean) %>%
  as.data.frame() %>%
  t() %>% 
  data.frame(row.names = NULL) %>%
  dplyr::mutate(date = site.preds.unlisted.anom.nested$date) %>%
  dplyr::rename(anom = 1, year = 2)

# Write results to a csv.
write.csv(anom_new,
          "./output/paleomat-temp-anomaly-final.csv",
          row.names = FALSE)


# For figures for journal article, please see "Paleomat Figures.Rmd".



# plot list of tavg_preds_temp3 and raster stack tavg_idw_stack

```

# END OF SCRIPT














# Old Stuff
### Temporal Interpolation

```{r temporal_interpolation, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Determine what sites have multiple datasets.
# unique_datasets <- unique(tavg[c("dataset.id", "site.name")])
# duplicated_sites <- unique_datasets$site.name[duplicated(unique_datasets$site.name, fromLast = TRUE)] %>%
#   unique()
#list1[!list1 %in% list2]

#Just temporary as this number is actually calculated in the Data_visualization.rmd
warmest.month = 15.6182926829268

# Put lat and long into individual columns.
tavg <- reconst_tavg_final_west_3 %>%
  tidyr::unnest(geometry) %>%
  dplyr::group_by(sample.id, month) %>%
  dplyr::mutate(col = seq_along(sample.id)) %>% #add a column indicator
  tidyr::spread(key = col, value = geometry) %>%
  dplyr::rename(long = "1", lat = "2") %>%
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age < 5950 & age >= 0 & month == 7) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-4000, 2000, 100))) %>%
  dplyr::mutate(anom = value - warmest.month)


# After more closely examining the Beef Pasture data, the spline was heavily impacted by Petersen's Beef Pasture data. The data more or less produced an extremely smooth spline that led to underfitting. This could be the result of the age model on the Petersen data, which has way fewer dates than the Wright Beef Pasture data. Therefore, I use the Wright data for 1-2000 AD and the Petersen data for the time before 1AD (as the Wright data does not go any farther back in time).
# Also, filter out the earliest date for Sagehen Marsh as there is too large of a time gap between the first point and the next point (about 1102 years [i.e., 600 to 1702]).
tavg_predicted_west <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::filter(!(site.id == 2241 & date <= 1600)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(site.data = purrr::map(site.data,
                                       function(x) {
                                         x %>%
                                           dplyr::group_by(age) %>%
                                           dplyr::mutate(
                                             value = mean(value),
                                             error = mean(error),
                                             anom = mean(anom)
                                           ) %>%
                                           dplyr::distinct(age, .keep_all = TRUE) %>%
                                           dplyr::arrange(age)
                                       })) %>%
  # sf::st_as_sf(coords = c("long", "lat"),
  #                crs = 4326) %>%
  dplyr::mutate(mean_100yrs = purrr::map(
    site.data,
    .f = function(x) {
      interpolate_time(temp_data = x,
                       model = "tps")
    }
  )) %>%
  dplyr::select(-site.data) %>%
  dplyr::arrange(site.name) %>%
  unnest_wider(mean_100yrs) %>%
  dplyr::rename(mean_100yrs = 6, model_fit = 7, plot_fit = 8)

# Plot results in one PDF.
plot_objects <- tavg_predicted_west$plot_fit
names(plot_objects) <- c(tavg_predicted_west$site.name)

plot_objects_arranged <- gridExtra::marrangeGrob(plot_objects, nrow=1, ncol=1, top=quote(names(plot_objects)[g]))

# Save as one pdf. Use scale here in order for the multi-plots to fit on each page.
ggsave("/Users/andrew/Dropbox/WSU/SKOPEII/Figures/Misc/site_temporal_interpolation_west.pdf", plot_objects_arranged, scale = 1.5)


# Get number of samples for each site.
no.samples <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(samples = purrr::map(
    site.data,
    .f = function(x) {
      nrow(x)
    }
  ))


```


### Thin Plate Spline Regression (TPS) and Mapping

```{r mapping_tps, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
# tavg_100yrs <- tavg_predicted_west_new_90 %>%
#   dplyr::select(-model_fit, -plot_fit) %>%
#   tidyr::unnest(mean_100yrs) %>%
#   dplyr::filter(date >= -1000 & date <= 1800) %>%
#   dplyr::group_by(date) %>%
#   tidyr::nest() %>%
#   dplyr::rename(site.predictions = data)

tavg_100yrs_new

# Get site locations.
fossil.locs <- tavg_new_90 %>%
  dplyr::group_by(site.name) %>%
  dplyr::slice(1) %>%
  dplyr::select(site.id, site.name, long, lat)

#Get the elevation dataset.
NED <-
  raster::raster("/Volumes/DATA/NED/EXTRACTIONS/UUSS_NED_1.tif")
# Temporarily re-projecting at a lower resolution that the code runs faster.
NED <- raster::aggregate(NED, fact = 10)
NED <-
  raster::projectRaster(NED, crs = CRS("+init=epsg:4326"))

# Get the PRISM modern July temperature dataset.
modern_temp <- brick("/Users/andrewgillreath-brown/Dropbox/WSU/SKOPEII/Figures/PRISM_tavg_monthly_normals.tif") %>%
  raster::subset(7)

temp.raster <-
  raster::projectRaster(modern_temp, crs = CRS("+init=epsg:4326"))

# Resample NED so that it is the same resolution and scale as temp.raster.
bbox_buffer <- c(
  "xmin" = min(fossil.locs$long) - 1,
  "ymin" = min(fossil.locs$lat) - 1,
  "xmax" = max(fossil.locs$long) + 1,
  "ymax" = max(fossil.locs$lat) + 1
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

bbox_buffer2 <- c(
  "xmin" = min(fossil.locs$long),
  "ymin" = min(fossil.locs$lat),
  "xmax" = max(fossil.locs$long),
  "ymax" = max(fossil.locs$lat)
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

temp.raster.cropped <- raster::crop(temp.raster, bbox_buffer)
NED.raster.cropped <- raster::crop(NED, bbox_buffer)

#NED.raster.cropped <- projectRaster(NED.raster.cropped2, temp.raster.cropped, method = 'ngb')

# Do the thin plate spline regression and return interpolated temperature raster and standard error raster for each time period.
# tavg_preds_refined2 <-
#   purrr::map(
#     site.preds.unlisted.anom.nested[1:3,]$site.predictions,
#     .f = function(x) {
#       map_predictions(
#         site.preds = x,
#         site.locs = fossil.locs,
#         raster.data = temp.raster.cropped,
#         nfraction.df = NA,
#         rast.extrap = FALSE
#       )
#     }
#   )

tavg_preds_refined3 <-
  purrr::map(
    site.preds.unlisted.anom.nested[-c(1, 54, 55),]$site.predictions,
    .f = function(x) {
      map_predictions2(
        site.preds = x,
        grid.res = 1.25,
        nfraction.df = NA
      )
    }
  )

tavg_preds_temp3 <- purrr::map(tavg_preds_refined3, 1)
tavg_preds_temp3_SE <- purrr::map(tavg_preds_refined3, 2)

names(tavg_preds_temp3) <- site.preds.unlisted.anom.nested[-c(1, 54, 55),]$date


# Save the data as RDS files.
saveRDS(tavg_preds, file = "/Users/andrew/Dropbox/WSU/SKOPEII/Figures/temp_rasters_convex_west.RDS")

# Calculate anomaly for each time period using the modern PRISM data for July.
tavg_preds_temp2 <- purrr::map(tavg_preds_refined2, 1)
tavg_preds_anom2 <- purrr::map(tavg_preds_temp2, function(x){x - temp.raster.cropped})


# Standardize values using x - mean / std.
tavg_preds_anom2_stdz <- purrr::map2(.x = tavg_preds_anom2, .y = names(tavg_preds_anom2), .f = function(x, y){(x - raster::cellStats(tavg_preds_anom2[[y]], stat='mean', na.rm=TRUE)) / raster::cellStats(tavg_preds_anom2[[y]], stat='sd', na.rm=TRUE)})



# Save the anomaly data as RDS files.
saveRDS(tavg_preds_anom, file = "/Users/andrewgillreath-brown/Dropbox/WSU/SKOPEII/Figures/temp_rasters_convex_west_anomaly.RDS")

# Create a raster stack.
tavg_rstack2 <- raster::stack(tavg_preds_temp3)
# Create a raster brick.
tavg_rbrick2 <- raster::brick(tavg_preds_temp3)

# Rename the layers.
names(tavg_rstack2) <- c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")
# names(tavg_preds) <- c("1000BC","900BC","800BC", "700BC", "600BC","500BC", "400BC", "300BC", "200BC", "100BC", "1AD", "100AD", "200AD", "300AD", "400AD", "500AD", "600AD", "700AD", "800AD","900AD", "100AD0", "1100AD", "1200AD", "1300AD", "1400AD", "1500AD", "1600AD", "1700AD", "1800AD")

# Define custom color palette.
colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(255)
plot(tavg_preds_anom2_stdz[[1]], col=colors)

# Create maps from ggplot
library(maps)
data(us.cities)

# Create a hillshade file.
NED.raster.cropped2 <- raster::crop(NED, bbox_buffer2)

NED.raster.cropped2 <-  projectRaster(NED.raster.cropped2,tavg_preds_anom2_stdz[[1]],method = 'ngb')

vep3_ned2 <- raster("EXTRACTIONS/vep3/NED/vep3_NED_1.tif")
vep3_ned2 <- raster::aggregate(vep3_ned2, fact = 3)
vep3_ned2 <-  projectRaster(vep3_ned2,tavg_preds_anom2_stdz[[1]],method = 'ngb')

slope <- terrain(vep3_ned2, opt='slope')
aspect <- terrain(vep3_ned2, opt='aspect')
dsm.hill <- hillShade(slope, aspect,
                      angle=40,
                      direction=270)
hill_spdf <- as(dsm.hill, "SpatialPixelsDataFrame")
hill_spdf <- as.data.frame(hill_spdf)
colnames(hill_spdf) <- c("value", "x", "y")


names(tavg_preds_anom2_stdz) <- c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")

anomaly_plots2 <-
  purrr::map2(
    .x = tavg_preds_anom2_stdz, .y = names(tavg_preds_anom2_stdz),
    .f =
      ~plot_map_anomaly(x = .x, x.name = .y, us.cities = us.cities, hillshade = hill_spdf, cities = FALSE, animation = TRUE)
  )

anomaly_plots3 <-
  purrr::map2(
    .x = tavg_preds_temp3, .y = names(tavg_preds_temp3),
    .f =
      ~plot_map_anomaly2(x = .x, x.name = .y, hillshade = hill_spdf)
  )

# Arrange all of the plots, then can output below as PDF.
anomaly_plots_arranged <- gridExtra::marrangeGrob(anomaly_plots3, nrow=2, ncol=1, top=NULL)

# Save as one pdf. Use scale here in order for the multi-plots to fit on each page.
ggsave("/Users/andrew/Dropbox/WSU/SKOPEII/Figures/anomaly_plots_arranged_west_NEWTPS.pdf", anomaly_plots_arranged, scale = 1.5,
       width = 210,
       height = 297,
       units = "mm")

transition_manual(date)

# Do quick animation with raster::animate.
tavg_animated <- raster::animate(tavg_rbrick, n =2, col = colors)

# Quick visualization plotting.
# Levelplot visualization (will put all in the plots viewer side by side, etc.).
rasterVis::levelplot(tavg_rstack,
                     margin=FALSE,
                     colorkey=list(
                       space='bottom',
                       labels=list(at=-5:5, font=4),
                       axis.line=list(col='black'),
                       width=0.75
                     ),
                     par.settings=list(
                       strip.border=list(col='transparent'),
                       strip.background=list(col='transparent'),
                       axis.line=list(col='transparent')
                     ),
                     scales=list(draw=FALSE),
                     col.regions=colors,
                     at=seq(-5, 5, len=101))

rasterVis::gplot(tavg_rbrick) +
  geom_tile(aes(fill = value)) +
  facet_wrap(~ variable) +
  scale_fill_gradientn(colours = colors) +
  coord_equal() + theme_classic()

# Visualizing maize over the reconstructions.
tavg_rbrick3 <- tavg_rbrick[[1:15]]
usa <- map("state", fill = TRUE, plot = FALSE)
par(mfrow=c(5,3), mai = c(.2, .35, .3, .3))
for (ii in 1:nlayers(tavg_rbrick3)){
  plot(subset(tavg_rbrick3,ii), main=names(tavg_rbrick3)[ii])
  plot(MDB[[ii]], main=names(tavg_rbrick3)[ii], pch = 19, cex = 1, add=T)
  polygon(usa$x, usa$y)
}
dev.off()

#Gif animation
## list file names and read in
imgs <- list.files("/Users/andrew/Dropbox/WSU/SKOPEII/Figures/animation_anomaly/animation_3/", full.names = TRUE)

# Put them in the correct order.
bfile.names <- sub("\\..*$", "", basename(imgs))
year.names <-
 c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")
year.number <- match(bfile.names, year.names)
imgs <- imgs[ order(year.number) ]


# index = c(28:24, 22:19, 23, 13, 6, 10:12, 14:18, 1:5, 7:9)
# index = c(4, 3, 2, 1, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)
# imgs = imgs[order(index)]
img_list <- lapply(imgs, magick::image_read)

## join the images together
img_joined <- magick::image_join(img_list)

## animate at 2 frames per second
img_animated2 <- magick::image_animate(img_joined, fps = 20)

## view animated image
img_animated

## save to disk
magick::image_write(image = img_animated2,
                    path = "/Users/andrew/Dropbox/WSU/SKOPEII/Figures/anomaly_plots_arranged_USGS2.gif")




# Calculate the anomaly across time for entire area (to produce line graph).
anom2 <- purrr::map(tavg_preds_anom2_stdz, cellStats, mean, na.rm = TRUE) %>%
  as.data.frame() %>%
  tidyr::pivot_longer(cols = BC950:AD1750) %>%
  dplyr::mutate(name = seq(-950, 1750, 100)) %>%
  dplyr::mutate(value = (value - mean(value)) / sd(value)) %>% 
  ggplot() +
  geom_line(aes(x = name, y = value))

anom2 %>%
  ggplot() +
  geom_line(aes(x = name, y = value), colour = "red", size = 1.0) +
  xlab("Year BC/AD") +
  ylab("July Temperature Anomaly (°C)") +
  scale_x_continuous(breaks = seq(-1000, 1800, 200),
                     minor_breaks = seq((-1000 + 100), (1800 - 100), 200)) +
  scale_y_continuous(breaks = seq(-2.0, 2.5, 0.5),
                     limits = c(-2.0, 2.5)) +
  theme_bw() +
  # geom_vline(xintercept = c(883, 1150, 1290, 1590)) +
  annotate(
    "rect",
    xmin = 800,
    xmax = 1200,
    ymin = -2.0,
    ymax = 2.5,
    alpha = .2
  ) +
  annotate(
    "text",
    x = 1000,
    y = 2.25,
    label = "Medieval Warm \n Period",
    size = 4.5
  ) +
    annotate(
    "rect",
    xmin = 1400,
    xmax = 1800,
    ymin = -2.0,
    ymax = 2.5,
    alpha = .2
  ) +
  annotate(
    "text",
    x = 1600,
    y = 2.25,
    label = "Little Ice \n Age",
    size = 4.5
  ) +
  theme(
    panel.border = element_blank(),
    panel.grid.major = element_blank(),
    axis.text = element_text(
      size = 12,
      colour = "black",
      family = "Helvetica"
    ),
    axis.title.y = element_text(
      size = 14,
      family = "Helvetica",
      margin = margin(
        t = 10,
        r = 6,
        b = 10,
        l = 10
      )
    ),
    axis.title.x = element_text(
      size = 14,
      family = "Helvetica",
      margin = margin(
        t = 6,
        r = 10,
        b = 10,
        l = 10
      )
    ),
    panel.grid.minor = element_blank(),
    axis.line = element_line(colour = "black")
  )


# Get number of samples per time period for each site.
no.samples <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.name, period) %>%
  dplyr::mutate(no.samples = length(anom)) %>%
  dplyr::slice(1) %>%
  dplyr::select(site.id, site.name, date, period, no.samples) %>%
  dplyr::ungroup() %>%
  dplyr::select(-period, -site.name) %>%
  dplyr::mutate(date = DescTools::RoundTo(x = date, multiple = 100, FUN = floor)) %>%
  dplyr::group_by(site.id, date) %>%
  dplyr::mutate(no.samples = sum(no.samples)) %>%
  dplyr::slice(1)

ok3 <- dplyr::left_join(tavg_100yrs, no.samples, by = c("site.id", "date"), keep = FALSE) %>%
  dplyr::mutate(no.samples = dplyr::coalesce(no.samples, 0))



temp_extract <- tavg_100yrs %>%
  ungroup() %>%
  dplyr::mutate(preds_anom = tavg_preds_anom) %>%
  rowwise() %>%
  dplyr::mutate(new_anom = raster::extract(preds_anom, site.predictions %>% dplyr::select(long, lat)) %>% list())


dplyr::mutate(new = purrr::map2(.x = site.predictions, .y = tavg_preds_anom, function(x, y){
  raster::extract(x = y, y = x %>% dplyr::select(long, lat))
}))

raster.anom <- temp_extract %>%
  ungroup() %>%
  unnest(cols = new_anom) %>%
  dplyr::select(new_anom)

temp_extract2 <- tavg_100yrs %>%
  ungroup() %>%
  unnest(cols = site.predictions)

raster.anom <- cbind(temp_extract2, raster.anom)


# BC950 <- tavg_preds_anom[[1]]
# BC950_points <- tavg_100yrs[1,]$site.predictions[[1]]
#
# new <- raster::extract(BC950, BC950_points %>% dplyr::select(long, lat))



# Alternative for getting anomaly. Here, I extract the modern temperature from the PRISM data at each of the fossil pollen locations. Then, calculate the anomaly for each fossil pollen sample.
site.preds.unlisted <- unnest(tavg_100yrs, cols = site.predictions)
modern_temperature <-
  as.data.frame(
    raster::extract(
      x = temp.raster,
      y = site.preds.unlisted %>% ungroup() %>%
        dplyr::select(long, lat)
    )
  ) %>%
  dplyr::rename(modern = 1)

site.preds.unlisted <-
  cbind(site.preds.unlisted, modern_temperature$modern) %>%
  dplyr::rename(modern = 11)

site.preds.unlisted.anom <- site.preds.unlisted %>%
  rowwise() %>%
  dplyr::mutate(anom = value - modern)

# here, we nest the data into each time bin.
site.preds.unlisted.anom.nested <- site.preds.unlisted.anom %>%
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)


site.preds3 <- site.preds.unlisted.anom.nested$site.predictions[[1]]


temp_extract <- tavg_100yrs %>%
  ungroup() %>%
  dplyr::mutate(preds_anom = tavg_preds_anom) %>%
  rowwise() %>%
  dplyr::mutate(new_anom = raster::extract(preds_anom, site.predictions %>% dplyr::select(long, lat)) %>% list())


dplyr::mutate(new = purrr::map2(.x = site.predictions, .y = tavg_preds_anom, function(x, y){
  raster::extract(x = y, y = x %>% dplyr::select(long, lat))
}))

raster.anom <- tavg_100yrs %>%
  unnest(cols = site.predictions)



# Alternative for getting anomaly.
site.preds.unlisted <- unnest(tavg_100yrs, cols = site.predictions) %>%
  ungroup() %>%
  dplyr::mutate(modern = as.data.frame(
    raster::extract(
      x = temp.raster,
      y = site.preds.unlisted %>% ungroup() %>%
        dplyr::select(long, lat)
    )
  )) %>%
  dplyr::rename(modern = 11)

site.preds.unlisted.anom <- site.preds.unlisted %>%
  rowwise() %>%
  dplyr::mutate(anom = value - modern)


# Both.
anoms_evaluated <- cbind(raster.anom, dplyr::select(site.preds.unlisted.anom, anom)) %>%
  rowwise() %>%
  dplyr::mutate(difference = new_anom - anom) %>%
  ungroup() %>%
  as.data.frame()


dplyr::select(anoms_evaluated, difference) %>% as.data.frame() %>%
  unlist() %>%
  as.data.frame() %>%
  dplyr::rename(anom = 1) %>%
  ggplot(aes(anom)) +
  geom_histogram()







point_plot <- ggplot(
  data = unnest(tavg_100yrs_new, site.predictions),
  mapping = aes(x = long, y = lat, color = value)) +
  geom_point(size = 3) +
  scale_color_gradientn(colors = c("blue", "yellow", "red"))

point_plot

temp_data <- unnest(tavg_100yrs_new, site.predictions)

bbox <- c(
  "xmin" = min(temp_data$long),
  "ymin" = min(temp_data$lat),
  "xmax" = max(temp_data$long),
  "ymax" = max(temp_data$lat)
)

grd_template <- expand.grid(
  X = seq(from = bbox["xmin"], to = bbox["xmax"], by = 2.0),
  Y = seq(from = bbox["ymin"], to = bbox["ymax"], by = 2.0) # 20 m resolution
)

grid_plot <- ggplot() +
  geom_point(data = grd_template, aes(x = X, y = Y), size = 0.01) +
  geom_point(data = temp_data,
  mapping = aes(x = long, y = lat, color = value), size = 3) +
  scale_color_gradientn(colors = c("blue", "yellow", "red")) +
  theme_bw()

grid_plot

sf_NH4 <- st_as_sf(temp_data, coords = c("long", "lat"), crs = 4326)

alt_grd_template_sf <- sf_NH4 %>% 
  st_bbox() %>% 
  st_as_sfc() %>% 
  st_make_grid(
  cellsize = c(2, 2),
  what = "centers"
  ) %>%
  st_as_sf() %>%
  cbind(., st_coordinates(.)) %>% 
  st_drop_geometry() %>% 
  mutate(Z = 0)



# {raster} expects a PROJ.4 string, see https://epsg.io/25833
crs_raster_format <- "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"

grd_template_raster <- grd_template %>% 
  dplyr::mutate(Z = 0) %>% 
  raster::rasterFromXYZ( 
    crs = crs_raster_format)

alt_grd_template_raster <- alt_grd_template_sf %>% 
  raster::rasterFromXYZ(
     crs = crs_raster_format
  )


fit_TPS <- fields::Tps( # using {fields}
  x = as.matrix(temp_data[, c("long", "lat")]), # accepts points but expects them as matrix
  Y = temp_data$value,  # the dependent variable
  miles = FALSE     # EPSG 25833 is based in meters
)




```
