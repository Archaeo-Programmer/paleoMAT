---
title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE)

# devtools::load_all()
library(paleomat)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. 

However, improvements in these techniques, and the increasing breadth of paleoclimatic proxies available have furthered our understanding of the effects of climate-driven variability on past societies. 

This program allows you to reconstruct the climate for multiple locations across North America (when data are sufficient to do). In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008) so that there will be columns of taxa with counts, as well as metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too.

# Get fossil, modern, and coretop pollen data.
```{r function_model, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs. 
gpids <- 
  neotoma::get_table(table.name='GeoPoliticalUnits')

NAID <-  
  gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('United States', 'Canada', 'Mexico'),
                GeoPoliticalUnit == 'country') %$%
  GeoPoliticalID

# BEGIN TESTING for MP_metadata_counts
# AZID <-  
#   gpids %>%
#   dplyr::filter(GeoPoliticalName %in% c('Arizona'),
#                 GeoPoliticalUnit == 'state') %$%
#   GeoPoliticalID
# 
# if(!file.exists(here::here("vignettes/data/AZ_metadata_counts.rds"))){
#   AZ_metadata_counts <- 
#   get_modern_pollen(gpid = AZID) %T>%
#   readr::write_rds(here::here("vignettes/data/AZ_metadata_counts.rds"))
# }
# AZ_metadata_counts <- 
#   readr::read_rds(here::here("vignettes/data/AZ_metadata_counts.rds"))

# END TESTING.
  
if(!file.exists(here::here("vignettes/data/MP_metadata_counts.rds"))){
  MP_metadata_counts <- 
  get_modern_pollen(gpid = NAID) %T>%
  readr::write_rds(here::here("vignettes/data/MP_metadata_counts.rds"))
}
MP_metadata_counts <- 
  readr::read_rds(here::here("vignettes/data/MP_metadata_counts.rds"))

if(!file.exists(here::here("vignettes/data/NAfossil_metadata_counts.rds"))){
NAfossil_metadata_counts <- 
  get_fossil_pollen(gpid = NAID) %T>%
  readr::write_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))
}
NAfossil_metadata_counts <- 
  readr::read_rds(here::here("vignettes/data/NAfossil_metadata_counts.rds"))

coreTops <- 
  paleomat::get_coreTops() %>% 
  dplyr::rename(
    dataset.id = DatasetID,
    site.id = SiteID, 
    sample.id = SampleID
  )

```

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

CT_metadata_counts <- 
  dplyr::left_join(coreTops,
                   NAfossil_metadata_counts,
                   by  = c("dataset.id", "site.id", "sample.id")) %>% 
  dplyr::mutate(type = "core top") %>% 
  dplyr::arrange(dataset.id)

```

The Neotoma Modern Pollen Database contains `r nrow(MP_counts)` samples, representing `r ncol(MP_counts)` different pollen taxa.

## Combine Core Top and Modern Pollen Data

```{r Combine_CT_MP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Now the two dataframes can be combined using the function, then sort the rows by dataset.id. The distinct function is used to make sure there is no duplicate data from combining the two dataframes. Then, we convert all NA values to zero.
MPCT_metadata_counts <- 
  dplyr::bind_rows(MP_metadata_counts, CT_metadata_counts) %>%
  dplyr::arrange(dataset.id) # %>%
#distinct(dataset.id, .keep_all = TRUE)

# Splitting these apart so that all of the NAs for the taxa counts can be changed to zero. Then combine back together.
MPCT_metadata <- 
  MPCT_metadata_counts[,1:11]
MPCT_counts <- 
  MPCT_metadata_counts[,c(1:3, 12:ncol(MPCT_metadata_counts))] %>% 
  dplyr::mutate_all(funs(ifelse(is.na(.), 0, .)))

MPCT_metadata_counts <- 
  dplyr::left_join(MPCT_metadata, 
                   MPCT_counts, 
                   by = c("dataset.id", "site.id", "sample.id"))

# Sort the taxa to be in alphabetical order.
MPCT_metadata_counts <- 
  MPCT_metadata_counts[,c(names(MPCT_metadata_counts)[1:11],
                          sort(names(MPCT_metadata_counts)[12:ncol(MPCT_metadata_counts)]))]

```

## Remove Coretops data from the fossil pollen dataset
```{r removeCT, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

NAfossil_metadata_counts <- 
  dplyr::anti_join(NAfossil_metadata_counts, 
                   coreTops, 
                   by = c("dataset.id", "site.id", "sample.id"))

```

# Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).  

```{r load_Prsim, echo = FALSE, warning=FALSE}
# Load Prism Climate data.

# Returns clim_wide.
paleomat::load_prism(MPCT_metadata_counts, 
                     here::here("vignettes/data"), 
                     "/Volumes/DATA/PRISM/LT81_800M/")

```

## Clean up datasets to remove NAs for PRISM data

Need to further clean up the datasets to remove any rows with NA values. Although currently working with United States data, we do not currently have climate (prism) data for Alaska, which is the reason for all rows with NAs.

```{r removePrismNAs, echo=FALSE, warning=FALSE}

# Get row number in first column that have NA values.
NAindex <- 
  which(is.na(clim_wide[,2]), arr.ind=FALSE)

# Remove rows with NA values for the precipitation, temperature, and GDD using the index.
Clim_noNAs <- 
  clim_wide[-(NAindex),]

rownames(Clim_noNAs) <- 
  seq(length=nrow(Clim_noNAs))

# Remove rows in Moden Pollen dataset, using the same index. This will keep the sites/locations still matched up between the climate data and the modern pollen counts.
MPCT_metadata_counts_noNAs <- 
  MPCT_metadata_counts[-(NAindex),]

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- 
  MPCT_metadata_counts_noNAs[,-(1:11)] %>% 
  dplyr::mutate_all(funs(ifelse(is.na(.), 0, .)))

```

## Checking the calibration data set

Need to change this into a function. **

To test the model, I used the `palaeoSig` package's `randomTF()` function, which tests the models against randomly sorted data. If we do get a significance for a climate variable, then we know that the model reconstruction is better than random. The model takes the proportion of variance accounted for by the actual data, and then compares it to the proportion of variance accounted for by the randomized data. Here, we use a wrapper function for [randomTF](https://github.com/NeotomaDB/Workbooks/blob/master/PollenClimate/R/sig_test.R).

The MAT method is testing whether the modern calibration is able to detect signals in each of the climate parameters. The example here uses the entire North American Modern Pollen Database, rather than a targeted data subset.

### MAT - ten closest

```{r MAT_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- 
  MPCT_metadata_counts_noNAs[,-(1:9)] %>% 
  dplyr::mutate_all(funs(ifelse(is.na(.), 0, .)))

# For some reason this fails consistently when we try to use the weighted MAT (col=2).
mat_sig <- 
  paleomat::run_tf(pollen = MPCT_counts_noNAs, 
                   climate = Clim_noNAs[,14:25],
                   func = rioja::MAT, 
                   col = 1, 
                   k = 10)

#col 1 is just the closest analogue.
mat_sig[[1]]

```

The variance explained is very low for some variables, although the p value is very low. This highlights that in some cases you may not just want to rely on the p value. This also shows the issue with temporal autocorrelation on temperature and, to some extent also, on precipitation variables.

# Reconstruction Statistics

## Reconstruction Significance

Now we can do a similar test on fossil assemblage reconstructions. This again uses the same `randomTF()` function, but the variance explained and significance will probably change given that we are using a slightly more constrained dataset.

Now, we apply a reconstruction to a real dataset.

#### MAT - ten closest

```{r MAT_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

MP_counts_noNAs <- MP_counts %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .)))

mat_reconst <- run_tf2(MP_counts_noNAs, fossil = fossil_data, 
        climate = Clim_noNAs[,13:24], 
        func = MAT, col = 2, k = 10)
 
 mat_reconst[[1]]

# for (i in 1:length(fossil_sites)) {
#   nam <- paste("mat_reconst_", fossil_sites[[i]], sep = "")
#   dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
#                  climate = Temperature,
#                  func = mat, col = 2, k = 10)
#   assign(nam, dat)
# }

```

Again, no significance for the model. This indicates that we are just not able to see a signal within the data, but this is probably due to the calibration dataset being too broad.

## Reconstruction

Once we have validated the methods, we re-run the analyses using MAT.

### Model Summary and saving values to file.

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# Create a list of all the fossil pollen sites to be reconstructed for the continental United States.
fossil_sites <- unique(NAfossil_metadata_counts$dataset.id)

# Subset the climate data to just Temperature. 
Temperature <- Clim_noNAs[,26:37]

# Now, we carry out the reconstruction for all fossil pollen sites in the continental United States The reconstructions are in one output file, which has the reconstructed temperatures and associated error.

# First, to save time, check to see if the sites have already been reconstructed, if so then this chunk will not run. If you would like to re-run the reconstructions, then simply delete the reconstruction .csv and it will run.

if (!file.exists("./output/US_reconst_temp_7analog.csv")) {
  
  mat_models_temp_7analog <- purrr::map(1:12,
                              .f = function(j){
                                rioja::MAT(y = MPCT_counts_noNAs, 
                                           x = Temperature[,j], 
                                           k = 7, 
                                           dist.method="sq.chord",
                                           lean = FALSE)
                              })
  
  US_reconst_7analog <- mat_models_temp_7analog %>%
  purrr::map_dfr(.id = "climate",
                 .f = function(the.model){
                   # Create the MAT reconstruction.
                   mat_reconst <- predict(the.model,
                                          newdata = NAfossil_metadata_counts %>%
                                            dplyr::select(-dataset.id:-pub_year),
                                          sse = TRUE)
                   
                   # Put the reconstructed data into a dataframe.
                   NAfossil_metadata_counts %>%
                     dplyr::select(dataset.id:pub_year) %>%
                     dplyr::mutate(model = "MAT") %>%
                     cbind(
                       tibble(reconst = mat_reconst$fit[,1],
                              err = mat_reconst$SEP[,1])
                     )
                 })


 write.csv(US_reconst_7analog, "./output/US_reconst_temp_7analog.csv", row.names = FALSE)
  
} else {
  
  stop("The US Reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the US directory.")
  
}


#---------------


# Do reconstruction for Growing Degree Days. 

# Subset the climate data to just GDD 
GDD_noNAs <- Clim_noNAs[,2:13]

if (!file.exists("./output/US_reconst_GDD_7analog.csv")) {
  
  mat_models_gdd_7analog <- purrr::map(1:12,
                              .f = function(j){
                                rioja::MAT(y = MPCT_counts_noNAs, 
                                           x = GDD_noNAs[,j], 
                                           k = 7, 
                                           dist.method="sq.chord",
                                           lean = FALSE)
                              })
  
  US_reconst_GDD_7analog <- mat_models_gdd_7analog %>%
  purrr::map_dfr(.id = "climate",
                 .f = function(the.model){
                   # Create the MAT reconstruction.
                   mat_reconst <- predict(the.model,
                                          newdata = NAfossil_metadata_counts %>%
                                            dplyr::select(-dataset.id:-pub_year),
                                          sse = TRUE)
                   
                   # Put the reconstructed data into a dataframe.
                   NAfossil_metadata_counts %>%
                     dplyr::select(dataset.id:pub_year) %>%
                     dplyr::mutate(model = "MAT") %>%
                     cbind(
                       tibble(reconst = mat_reconst$fit[,1],
                              err = mat_reconst$SEP[,1])
                     )
                 })


 write.csv(US_reconst_GDD_7analog, "./output/US_reconst_GDD_7analog.csv", row.names = FALSE)
  
} else {
  
  stop("The US GDD Reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the US directory.")
  
}

```

