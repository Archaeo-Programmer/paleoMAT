---
author: "Andrew Gillreath-Brown"
date: "`r format(Sys.time(), '%d %B %Y')`"
title: "paleomat: A Low-Frequency Summer Temperature Reconstruction for the United States Southwest, 3000 BC â€“ AD 2000"
html_document:
  toc: yes
  toc_depth: 2
  toc_float: yes
  number_sections: no
  code_folding: hide
output:
  html_document:
    df_print: paged
mainfont: Calibri
editor_options:
  chunk_output_type: console
keep_md: yes
---

```{r setup, include = FALSE, results = 'hide'}

knitr::opts_chunk$set(echo = TRUE)

# devtools::install()
# devtools::load_all()
library(paleomat)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. Improvements in these techniques and the increasing breadth of paleoclimatic proxies available, however, have furthered our understanding of the effects of climate-driven variability on past societies.

This program allows you to reconstruct the climate for multiple locations across the southwestern United States. In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008), so that there will be columns of taxa with counts and associated metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too. Here, we use the Neotoma package, version 1.7.4.

**Note**: While running the code below, if you have any trouble with `paleomat` datasets (e.g., `paleomat::coreTops`), then you can use `here::here` with 'data' as the root folder, then the variable name with .rda, such as `load(file = here::here("data/coreTops.rda"))`, to load the dataset. Additionally, we use `if` statement in a number of places throughout the markdown to save time on items that may take a while to run. However, if you have cloned the repository, then you can run the interior contents of those if statements to overwrite variables.

# Download Fossil and Modern Pollen Data

```{r function_model, cache = TRUE, message = FALSE, warning = FALSE}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then, get the datasets for the pollen data from each of the gpids. Here, to save time, we read in the already downloaded data from Neotoma, as it takes a while to run for the 3 countries. Data downloaded on October 23, 2021.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs. We skip this step if the fossil pollen and modern pollen datasets are present.
if (!file.exists(here::here("data/MP_metadata_counts.rda")) | !file.exists(here::here("data/NAfossil_metadata_counts.rda"))) {
gpids <-
  neotoma::get_table(table.name = 'GeoPoliticalUnits')

NAID <-
  gpids %>%
  dplyr::filter(
    GeoPoliticalName %in% c('United States', 'Canada', 'Mexico'),
    GeoPoliticalUnit == 'country'
  ) %$%
  GeoPoliticalID
}

# Get the modern pollen dataset.
if (!file.exists(here::here("data/MP_metadata_counts.rda"))) {
  MP_metadata_counts <-
    paleomat::get_modern_pollen(gpid = NAID)
  
  save(MP_metadata_counts,
       file = here::here("data/MP_metadata_counts.rda"))
}
MP_metadata_counts <- paleomat::MP_metadata_counts

# Get the fossil pollen dataset.
if (!file.exists(here::here("data/NAfossil_metadata_counts.rda"))) {
  NAfossil_metadata_counts <-
    paleomat::get_fossil_pollen(gpid = NAID)
  
  save(NAfossil_metadata_counts,
       file = here::here("data/NAfossil_metadata_counts.rda"))
}
NAfossil_metadata_counts <- paleomat::NAfossil_metadata_counts

```

The Neotoma Modern Pollen Database contains `r nrow(MP_metadata_counts)` samples for the United States, Canada, and Mexico, representing `r ncol(MP_metadata_counts %>% dplyr::select(ABIES:tail(names(.), 1)))` different pollen taxa.

# Data Cleaning for Fossil and Modern Pollen Data and Climate Data

## Load "Core Tops" Data from the Fossil Pollen Dataset

```{r load_coreTops, message = FALSE, warning = FALSE, results = 'hide'}

# Get core tops from the fossil pollen dataset and add to the modern pollen dataset. The data was originally downloaded using Tilia with a function created by Eric Grimm. The SSL certificate has now expired, but the sample numbers, dataset ID, and site ID core tops can be loaded using paleomat::coreTops.
if (!file.exists(here::here("vignettes/data/pollen_cleaned/CT_metadata_counts.rds"))) {
  CT_metadata_counts <-
    dplyr::left_join(paleomat::coreTops,
                     NAfossil_metadata_counts,
                     by  = c("dataset.id", "site.id", "sample.id")) %>%
    dplyr::mutate(type = "core top") %>%
    dplyr::arrange(dataset.id) %T>%
    readr::write_rds(here::here("vignettes/data/pollen_cleaned/CT_metadata_counts.rds"))
}
CT_metadata_counts <-
  readr::read_rds(here::here("vignettes/data/pollen_cleaned/CT_metadata_counts.rds"))

```

## Combine Core Top and Modern Pollen Data for Continental United States

```{r Combine_CT_MP, message = FALSE, warning = FALSE, results = 'hide'}

#Combine the modern and core top data, then keep only sites that fall within the continental United states.

# If you do not want to run this code chunk to create MPCT_metadata_counts_final, then you can uncomment and run the following line:
# MPCT_metadata_counts_final <-
#   readr::read_rds(here::here("vignettes/data/pollen_cleaned/MPCT_metadata_counts_final.rds"))

# Now, the two dataframes (i.e., the modern data and core top data) can be combined using this function, then sort the rows by dataset.id.
MPCT_metadata_counts <-
  dplyr::bind_rows(MP_metadata_counts, CT_metadata_counts) %>%
  dplyr::arrange(dataset.id) %>%
  dplyr::mutate(across(-c(dataset.id:pub_year), ~ tidyr::replace_na(.x, 0))) %>%
  dplyr::select(dataset.id:pub_year,
                sort(tidyselect::peek_vars())) %>%
  dplyr::filter(!is.na(long)) %>%
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326)

# Keep only modern sites in the continental United States.

# Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
states <-
  sp::spTransform(
    paleomat::states,
    sp::CRS(
      "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
    )
  )

# Filter out non-continental states and territories.
continental.states <-
  subset(
    states,
    NAME != "Alaska" &
      NAME != "Hawaii" &
      NAME != "Puerto Rico" & NAME != "U.S. Virgin Islands"
  )

# Convert sp to sf.
continental.states <- sf::st_as_sf(continental.states, crs = 4326)

# Update projection to CRS 4326, which is WGS 84.
continental.states <- sf::st_transform(continental.states, 4326)

# Need to make the geometry valid for the states shapefile.
continental.states <- sf::st_make_valid(continental.states)
continental.states_union <- sf::st_union(continental.states)

# Do an intersection and keep only the sites that are within the continental United States.
buffer <- sf::st_buffer(continental.states_union, 0)
MPCT_metadata_counts_final <-
  sf::st_intersection(MPCT_metadata_counts, buffer)

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   MPCT_metadata_counts_final,
#   here::here("vignettes/data/pollen_cleaned/MPCT_metadata_counts_final.rds"))

#------End for MPCT_metadata_counts_final--------#

```

## Remove Coretops Data from the Fossil Pollen Dataset

```{r removeCT, message = FALSE, warning = FALSE, results = 'hide'}

# Remove the core tops from the fossil pollen dataset, and then limit fossil pollen sites to just the southwestern United States.

# If you do not want to run the code for creating fossil_metadata_counts_final2, then you can uncomment and run the following line:
# fossil_metadata_counts_final2 <- readr::read_rds(here::here("vignettes/data/pollen_cleaned/fossil_metadata_counts_final2.rds"))

# Remove the coretop samples from the fossil pollen dataset.
fossil_metadata_counts <-
  dplyr::anti_join(NAfossil_metadata_counts,
                   coreTops,
                   by = c("dataset.id", "site.id", "sample.id")) %>%
  sf::st_as_sf(coords = c("long", "lat"), crs = 4326)

# Limit fossil pollen sites to just the southwestern United States.
# First, create a bounding box for the Upper United States Southwest (after Bocinsky and Kohler 2014 - https://doi.org/10.1038/ncomms6618).
bb <-
  c(
    "xmin" = -113,
    "xmax" = -105,
    "ymin" = 31.334,
    "ymax" = 38.09
  ) %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

# Do an intersection and keep only the sites that are within the southwestern United States, and remove any samples that do not have an age.
fossil_metadata_counts_final2 <-
  sf::st_intersection(fossil_metadata_counts, bb) %>%
  dplyr::filter(!is.na(age))

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   fossil_metadata_counts_final2,
#   here::here("vignettes/data/pollen_cleaned/fossil_metadata_counts_final2.rds"))

#------End for fossil_metadata_counts_final2--------#



# Limit samples to post 5500 BP.
fossil_west_post5500 <- paleomat::fossil_west_post5500
# If you want to recreate fossil_west_post5500, then you can uncomment and run the following code:
# fossil_west_post5500 <-
#   fossil_metadata_counts_final2 %>%
#   dplyr::filter(age <= 5500)

# If you want to re-save fossil_west_post5500.
#save(fossil_west_post5500, file = here::here("data/fossil_west_post5500.rda"))

#------End for fossil_west_post5500--------#



# Cleaning up the fossil dataset.

# If you do not want to run the code for creating fossil_final, then you can uncomment and run the following line:
# fossil_final <-
#   readr::read_rds(here::here("vignettes/data/pollen_cleaned/fossil_final.rds"))

fossil_final <-
  fossil_metadata_counts_final2 %>%
  # Remove any rows that do not have age data.
  dplyr::filter(!is.na(age)) %>%
  dplyr::select(!c(site.id, site.name:pub_year)) %>%
  tibble::as_tibble() %>%
  # For the pollen columns, replace any NA with a 0.
  dplyr::mutate(across(-c(dataset.id, geometry), ~ tidyr::replace_na(.x, 0))) %>%
  # For the pollen columns, replace any negative number with a 0 (in case of any errors).
  dplyr::mutate(dplyr::across(!c(dataset.id, geometry), ~ ifelse(.x < 0, 0, .x))) %>%
  # Nest the pollen data into a nested dataframe.
  tidyr::nest(pollen.counts = -c(dataset.id, geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(dataset.id, pollen = pollen.counts) %>%
  # Add column that shows the total number of samples for each core.
  mutate(n_samples = purrr::map_dbl(pollen, nrow))

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(fossil_final,
#                  here::here("vignettes/data/pollen_cleaned/fossil_final.rds"))


#------End for fossil_final--------#

```

## Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).

```{r extract_prism_normals, warning = FALSE}

# Extract Prism climate data for modern pollen sites and limit to the western United States.

# If you do not want to run the code for creating MPCT_climate_final, then you can uncomment and run the following line:
# MPCT_climate_final <-
#   readr::read_rds(here::here("vignettes/data/pollen_cleaned/MPCT_climate_final.rds"))

MPCT_climate <-
  MPCT_metadata_counts_final %>%
  dplyr::select(sample.id) %>%
  paleomat::extract_prism_normals() %>%
  sf::st_drop_geometry() %>%
  dplyr::left_join(MPCT_metadata_counts_final, by = "sample.id") %>%
  dplyr::select(sample.id,
                prism.normals,
                geometry,
                ABIES:XANTHIUM) %>%
  tidyr::nest(pollen.counts = -c(sample.id, prism.normals, geometry)) %>%
  sf::st_as_sf() %>%
  dplyr::select(sample.id,
                climate = prism.normals,
                pollen = pollen.counts)

# Do an intersection and keep only the sites that are within the western continental United States.
MPCT_climate_final <-
  sf::st_intersection(MPCT_climate, paleomat::west_in_states) %>%
  dplyr::select(-FID)

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   MPCT_climate_final,
#   here::here("vignettes/data/pollen_cleaned/MPCT_climate_final.rds")
# )

```

## Finalize Modern and Fossil Datasets
```{r finalize-datasets, cache = TRUE, results = 'asis', warning = FALSE}

# Modern Dataset

# Clean up modern pollen and climate dataframe to prepare for transfer function and reconstruction diagnostics. Here, we transform the pollen data into proportions.

# If you do not want to run the code for creating MPCT_proportions, then you can uncomment and run the following line:
# MPCT_proportions <- paleomat::MPCT_proportions

MPCT_proportions <-
  MPCT_climate_final %>%
  dplyr::mutate(
    pollen =
      purrr::map(pollen,
                 function(x) {
                   x / rowSums(x)
                 }),
    climate =
      purrr::map(climate,
                 function(x) {
                   x %>%
                     dplyr::select(month, tavg)
                 })
  )

# If you need to write the data to file, then you can uncomment and run the following.
# save(MPCT_proportions, file = here::here("data/MPCT_proportions.rda"))


# Put the modern pollen data into a dataframe and make the sample.id as the row names.
spp <- dplyr::bind_rows(MPCT_proportions$pollen) %>%
  dplyr::bind_cols(sample.id = MPCT_proportions$sample.id, .) %>%
  tibble::column_to_rownames("sample.id")

# Put the modern climate data into a dataframe and make the sample.id as the row names.
env <- MPCT_proportions %>%
  dplyr::select(sample.id, climate) %>%
  tidyr::unnest(climate) %>%
  # Pull just the climate (tavg) values as a vector.
  dplyr::pull(tavg)

# Turn climate value (tavg) vector into a named vector with the sample ids.
names(env) <- c(MPCT_proportions$sample.id)



# Fossil Dataset

# If you do not want to run the code for creating fossil_pollen, then you can uncomment and run the following line:
#fossil_pollen <- paleomat::fossil_pollen

# Prepare fossil pollen dataset for transfer function and reconstruction diagnostics.
fossil_final$pollen <-
  purrr::map(fossil_final$pollen, function(x)
    tibble::column_to_rownames(x, "sample.id"))

# Transform fossil pollen data into proportions, as we did with the modern pollen data above.
fossil_pollen <- fossil_final %>%
  dplyr::mutate(pollen =
                  purrr::map(pollen,
                             function(x) {
                               x / rowSums(x)
                             }))
  
  # If you need to write the data to file, then you can uncomment and run the following.
  #save(fossil_pollen, file = here::here("data/fossil_pollen.rda"))

```

# Transfer Function and Reconstruction Diagnostics and Reconstruction

Here, we use several transfer function and reconstruction diagnostics, such as squared residual length, the proportion of variance in the fossil data explained by an environmental reconstruction, and the no-analog threshold.

## Squared-residual Length

```{r squared_residual_length, cache = TRUE, results = 'asis', warning = FALSE}

# Step 1: We use squared residual length. Here, we return the residual length for each sample, and the 0.95 quantile.

# If you do not want to run the code for creating rlen_results, then you can uncomment and run the following line:
# rlen_results <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/rlen_results.rds"))

rlen_results <-
  purrr::map(
    fossil_pollen$pollen,
    .f = function(x) {
      rlen <- analogue::residLen(spp, env, x, method = "cca")
      
      list(res_lengths = stack(rlen$passive),
           quantile_95 = as.numeric(quantile(rlen$train, probs = 0.95)))
    }
  )
# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   rlen_results,
#   here::here("vignettes/data/diagnostics_reconstruction/rlen_results.rds"))

# Extract the 0.95 quantile baseline to be used for sample removal.
rlen_ninety5quant <- rlen_results[[1]]$quantile_95

# Put the squared residual length and sample.id into one dataframe.
rlen_results <- purrr::map_dfr(rlen_results, `[[`, 1) %>%
  dplyr::rename("Squared_res_lengths" = 1, "sample.id" = 2)

# Extract the sample.ids for samples that will need to be removed since they did not meet the 0.95 threshold.
rlen_removal <-
  as.numeric(levels(rlen_results$sample.id))[rlen_results$Squared_res_lengths > rlen_ninety5quant]

# Results from rlen with sample.id for last 5500 years.
rlen_full_results1 <- rlen_results %>%
  dplyr::mutate(sample.id = as.integer(levels(sample.id))) %>%
  dplyr::filter(sample.id %in% fossil_west_post5500$sample.id)

# If you do not want to run the code for creating rlen_full_results2, then you can uncomment and run the following line:
# rlen_full_results2 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/rlen_full_results2.rds"))

# Putting in the rlen results into a list structure.
rlen_full_results2 <-
  structure(list(res_lengths = rlen_full_results1,
                 quantile_95 = rlen_ninety5quant),
            class = "list")

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   rlen_full_results2,
#   here::here("vignettes/data/diagnostics_reconstruction/rlen_full_results2.rds"))

# Creating a copy of fossil pollen to remove samples that did not meet the 0.95 threshold from the fossil pollen dataset.
fos <- fossil_pollen
fos$pollen <-
  purrr::map(
    fos$pollen,
    ~ .x %>% tibble::rownames_to_column("sample.id") %>% dplyr::filter(!sample.id %in% rlen_removal) %>% tibble::column_to_rownames("sample.id")
  )

# Cleaning up fos to get the number of samples that are within each site. If a site is left with 0 or 1 samples, then it is removed.

# If you do not want to run the code for creating fos2, then you can uncomment and run the following line:
# fos2 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/fos2.rds"))

# Remove any sites that have 0 or 1 samples from the fossil pollen dataset.
fos2 <- fos %>%
  dplyr::select(-n_samples) %>%
  mutate(rows = purrr::map_dbl(pollen, ~ nrow(.x))) %>%
  filter(!rows %in% c(0, 1))

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(fos2,
#                  here::here("vignettes/data/diagnostics_reconstruction/fos2.rds"))

```

## randomTF Testing

```{r randomTF, cache = TRUE, results = 'asis', warning = FALSE}

# Step 2: We use Random TF to remove sites where tJuly cannot explain at least 90% of the variance.
# This line takes approximately 40 minutes to run (depending on your system). If you would like to re-run, then you can run the interior contents of the if statement. 
if (!file.exists(here::here(
  "vignettes/data/diagnostics_reconstruction/sig_results2_final.rds"
))) {
  sig_results2_full <-
    purrr::map2(fos2$pollen, fos2$dataset.id, function(x, y) {
      output <- palaeoSig::randomTF(
        spp = spp,
        env = data.frame(tjul = env),
        fos = x,
        n = 10000,
        fun = MAT,
        col = 1
      )
      prop <- output$EX
      names(prop) <- y
      sig_out <- output$sig
      names(sig_out) <- y
      return(list(prop, sig_out))
    })
  
  # Extract the proportion explained variance and the dataset ID.
  sig_results2_proportions <- sapply(sig_results2_full, "[[", 1) %>%
    stack(.) %>%
    dplyr::rename(dataset.id = ind, proportion = values)
  
  # Extract the p-values and the dataset ID.
  sig_results2_pvalues <- sapply(sig_results2_full, "[[", 2) %>%
    stack(.) %>%
    dplyr::rename(dataset.id = ind, pvalue = values)
  
  # Put the proportion of variance and p-value into the same dataframe.
  sig_results2_final <-
    dplyr::left_join(sig_results2_proportions, sig_results2_pvalues, by = "dataset.id") %>%
    dplyr::select(dataset.id, proportion, pvalue) %>%
    dplyr::mutate(dataset.id = as.numeric(levels(dataset.id)))
  
  readr::write_rds(
    sig_results2_final,
    here::here(
      "vignettes/data/diagnostics_reconstruction/sig_results2_final.rds"
    )
  )
}
sig_results2_final <-
  readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/sig_results2_final.rds"))

# Pull out the dataset IDs that have p-values less than the 0.10.
only_90b <- sig_results2_final %>%
  dplyr::filter(pvalue <= 0.10) %>%
  dplyr::pull(dataset.id)

# Finally, subset the fossil pollen dataset by keeping only the dataset IDs that passed randomTF.
fos3_90 <- fos %>%
  dplyr::filter(dataset.id %in% only_90b)

```

## MAT Transfer Function and Prediction

```{r MAT_transfer_prediction, cache = TRUE, results = 'asis', warning = FALSE}

# Step 3: Run MAT and do prediction/reconstruction.

# Remove any columns in the modern pollen dataset that have 0 for the pollen taxa.
pollen_input <- spp %>%
  dplyr::select(which(colSums(.) > 0))

# Remove any columns in the fossil pollen dataset that have 0 for the pollen taxa.
fossil_input <- do.call(rbind, unname(fos3_90$pollen)) %>%
  dplyr::select(which(colSums(.) > 0))

# Keep only columns that are the same between the modern and fossil pollen dataset, as they need to be the same to run MAT.
keep_cols <- colnames(pollen_input[colSums(pollen_input) > 0])
keep_cols <- keep_cols[keep_cols %in% colnames(fossil_input)]

pollen_input <- pollen_input[, keep_cols]
fossil_input <- fossil_input[, keep_cols]

# Save output of MAT.
# If you do not want to run the code for creating swap.mat2, then you can uncomment and run the following line:
# swap.mat2 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/mat_calibration.rds"))

# Create the transfer function using mat from the analogue package.
swap.mat2 <- analogue::mat(pollen_input, env, method = "SQchord")

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   swap.mat2,
#   here::here("vignettes/data/diagnostics_reconstruction/mat_calibration.rds"))


# Here, we run the prediction twice, so that we can extract the minimum dissimilarity and distances from the predict.mat object.

# If you do not want to run the code for creating rlgh.mat2, then you can uncomment and run the following line:
# rlgh.mat2 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/mat_prediction.rds"))

# Create the transfer function using mat from the analogue package.
rlgh.mat2 <-
  analogue:::predict.mat(object = swap.mat2,
                         newdata = fossil_input,
                         k = 4)

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   rlgh.mat2,
#   here::here("vignettes/data/diagnostics_reconstruction/mat_prediction.rds"))


# Here, we run the prediction again in order to get the bootstrap estimates.

# If you do not want to run the code for creating rlgh.mat2b4, then you can uncomment and run the following line:
# rlgh.mat2b4 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/mat_prediction_bootstrap.rds"))


# Create the transfer function using mat from the analogue package.
rlgh.mat2b4 <-
  analogue:::predict.mat(
    object = swap.mat2,
    newdata = fossil_input,
    k = 4,
    bootstrap = TRUE
  )

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   rlgh.mat2b4,
#   here::here("vignettes/data/diagnostics_reconstruction/mat_prediction_bootstrap.rds"))


# Here, we pull out the data from the predict.mat object. First, we pull out the predicted values for the four best analogues. In the analogue package, the analogues have cumulative means, so we only need to extract the fourth record here.
mat_prediction_4 <-
  as.data.frame(rlgh.mat2b4$predictions$model$predicted[4,]) %>%
  tibble::rownames_to_column("sample.id") %>%
  dplyr::rename(value = 2) %>%
  # Pull out the bootstrap predicted values.
  dplyr::left_join(
    .,
    as.data.frame(rlgh.mat2b4[["predictions"]][["bootstrap"]][["predicted"]][, 4]) %>% tibble::rownames_to_column("sample.id") %>%
      dplyr::rename(bootstrap = 2),
    by = "sample.id"
  ) %>%
  # Pull out the bootstrap rmsep.
  dplyr::left_join(
    .,
    as.data.frame(rlgh.mat2b4[["predictions"]][["sample.errors"]][["rmsep"]][, 4]) %>% tibble::rownames_to_column("sample.id") %>%
      dplyr::rename(rmsep = 2),
    by = "sample.id"
  ) %>%
  # Pull out the bootstrap s1 errors.
  dplyr::left_join(
    .,
    as.data.frame(rlgh.mat2b4[["predictions"]][["sample.errors"]][["s1"]][, 4]) %>% tibble::rownames_to_column("sample.id") %>%
      dplyr::rename(s1 = 2),
    by = "sample.id"
  )

```

## No Analog Threshold (or Minimum Dissimilarity)

```{r no_analog, cache = TRUE, results = 'asis', warning = FALSE}


# Step 4: Remove any fossil pollen samples that have a minimum dissimilarity that is greater than the 5% quantile.

# If you do not want to run the code for creating minDC_full_results, then you can uncomment and run the following line:
# minDC_full_results <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/minDC_full_results.rds"))

# Get a list of the samples that remain in the fossil pollen dataset after rlen and the randomTF removal steps.
pre_minDC_samples <- rlen_full_results1 %>%
  dplyr::filter(!sample.id %in% rlen_removal) %>%
  dplyr::left_join(
    .,
    fossil_west_post5500 %>%
      dplyr::select(dataset.id, site.id, sample.id),
    by = "sample.id"
  ) %>%
  dplyr::filter(dataset.id %in% only_90b) %>%
  dplyr::pull(sample.id)

# Get the minimum dissimilarity for each sample.
rlgh.mdc <- analogue::minDC(rlgh.mat2)

# Put the results into a list.
minDC_results <- stack(rlgh.mdc$minDC) %>%
  dplyr::mutate(ind = as.integer(levels(ind))) %>%
  dplyr::filter(ind %in% pre_minDC_samples) %>%
  dplyr::left_join(
    .,
    fossil_west_post5500 %>%
      dplyr::select(sample.id, dataset.id, site.id, age) %>%
      dplyr::mutate(age = paleomat::BPtoBCAD(age)) %>%
      sf::st_drop_geometry(),
    by = c("ind"  = "sample.id")
  )

# Put the results into a list structure.
minDC_full_results <- structure(list(minDC = minDC_results,
                                     quantiles = rlgh.mdc$quantiles),
                                class = "list")

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   minDC_full_results,
#   here::here("vignettes/data/diagnostics_reconstruction/minDC_full_results.rds"))



# Put data into a list. Create list of samples that did and did not meet the threshold, and subset the fossil pollen data to the samples that did pass the minDC test.

# If you do not want to run the code for creating minDC_summary, then you can uncomment and run the following line:
# minDC_summary <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/minDC_summary.rds"))

# Get sample IDs and dissimilarity for samples that did not pass the threshold (i.e., samples that will need to be removed).
remove_samples <- as.data.frame(rlgh.mdc$minDC) %>%
  tibble::rownames_to_column("sample.id") %>%
  dplyr::rename(dissimilarity = 2) %>%
  dplyr::filter(dissimilarity > as.numeric(rlgh.mat2$quantiles["5%"]))

# Remove the samples from the dataset.
mat_prediction_cleaned <- mat_prediction_4 %>%
  dplyr::filter(!sample.id %in% remove_samples$sample.id)

# Final Sample set.
final_approved_samples <- minDC_full_results$minDC %>%
  dplyr::filter(!ind %in% remove_samples$sample.id) %>%
  dplyr::pull(ind)

# Put into a list structure.
minDC_summary <- list(
  remove_samples = remove_samples,
  mat_prediction_cleaned = mat_prediction_cleaned,
  final_approved_samples = final_approved_samples
)

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   minDC_summary,
#   here::here("vignettes/data/diagnostics_reconstruction/minDC_summary.rds"))

```

## Update Age Models

```{r update_age_models, cache = TRUE, results = 'asis', warning = FALSE}

# Clean up fossil pollen data to create the final dataset of samples.

# If you do not want to run the code for creating fos_90, then you can uncomment and run the following line:
# fos_90 <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/fos_90.rds"))

fos_90 <- mat_prediction_cleaned %>%
  dplyr::mutate(sample.id = as.integer(sample.id)) %>%
  dplyr::filter(sample.id %in% final_approved_samples) %>%
  dplyr::left_join(.,
                   fossil_metadata_counts_final %>%
                     dplyr::select(dataset.id:pub_year),
                   by = "sample.id") %>%
  dplyr::filter(age <= 5500) %>%
  dplyr::left_join(paleomat::updated_age_models,
                   by = c("site.id", "dataset.id", "sample.id")) %>%
  dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
  dplyr::select(-c(age.y, age.x)) %>%
  dplyr::mutate(age = round(age)) %>%
  sf::st_as_sf()

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(fos_90,
#                  here::here("vignettes/data/diagnostics_reconstruction/fos_90.rds"))







# COMBO #3 - ONLY NO ANALOGUE REMOVAL.

# fos <- fossil_pollen
# fos2 <- fos %>% 
#   dplyr::select(-n_samples) %>% 
#   mutate(rows = map_dbl(pollen, ~ nrow(.x))) %>% 
#   filter(!rows %in% c(0, 1))
# 
# pollen_input <- spp %>% 
#   dplyr::select(which(colSums(.) > 0))
# 
# fossil_input <- do.call(rbind, unname(fos2$pollen)) %>% 
#   dplyr::select(which(colSums(.) > 0))
# 
# keep_cols <- colnames(pollen_input[colSums(pollen_input) > 0])
# keep_cols <- keep_cols[keep_cols %in% colnames(fossil_input)]
# 
# pollen_input <- pollen_input[, keep_cols]
# fossil_input <- fossil_input[, keep_cols]
# 
# 
# swap.mat2 <- mat(pollen_input, env, method = "SQchord")
# 
# rlgh.mat2 <- predict(object = swap.mat2, newdata = fossil_input)
# 
# mat_prediction <- as.data.frame(rlgh.mat2$predictions$model$predicted[4,]) %>%
#   tibble::rownames_to_column("sample.id") %>%
#   rename(value = 2)
# 
# rlgh.mdc <- minDC(rlgh.mat2)
# 
# remove_samples <- as.data.frame(rlgh.mdc$minDC) %>%
#   tibble::rownames_to_column("sample.id") %>%
#   rename(dissimilarity = 2) %>%
#   filter(dissimilarity > as.numeric(rlgh.mat2$quantiles["5%"]))
# 
# mat_prediction_cleaned <- mat_prediction %>%
#   filter(!sample.id %in% remove_samples$sample.id)
# 
# fos_90 <- mat_prediction_cleaned %>% 
#   mutate(sample.id = as.integer(sample.id)) %>% 
#   dplyr::left_join(., fossil_metadata_counts_final %>% 
#   dplyr::select(dataset.id:pub_year), by = "sample.id") %>% 
#   #dplyr::mutate(error = new_output_recont_fos3_90$SEP.boot[, "MAT"]) %>% 
#   filter(age <= 5500) %>% 
#   dplyr::left_join(updated_age_models, by = c("site.id", "dataset.id", "sample.id")) %>%
#   dplyr::mutate(age = dplyr::coalesce(age.y, age.x)) %>%
#   dplyr::select(-age.y, -age.x) %>%
#   dplyr::mutate(age = round(age)) %>% 
#   # filter(site.id == 246) %>% 
#   sf::st_as_sf()
# 
# write.csv(fos_90, "Tim_NoAnalogue_Only_1best_analogs.csv", row.names = FALSE)

```

# Anomaly Calculation

```{r anomaly_calculation, cache = TRUE, results = 'asis', warning = FALSE}

# Extract the modern mean temperature at each fossil pollen site, and then calculate the anomalies.

# If you do not want to run the code for creating fos_90, then you can uncomment and run the following line:
# site.preds.unlisted.anom <- readr::read_rds(here::here("vignettes/data/diagnostics_reconstruction/final_paleomat_dataset3.rds"))

# Put lat and long into individual columns.
tavg_new_90 <- fos_90 %>%
  dplyr::mutate(long = unlist(purrr::map(.$geometry, 1)),
                lat = unlist(purrr::map(.$geometry, 2))) %>%
  # Remove/drop geometry. 
  dplyr::select(-geometry) %>%
  sf::st_drop_geometry() %>%
  # Create a date column with BC/AD ages.
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  # Remove any data older than 5500 years old.
  dplyr::filter(age <= 5500) %>%
  # Create 100 year periods. 
  dplyr::group_by(period = cut(date, breaks = seq(-5500, 2100, 100))) %>%
  # Get a midpoint for each period.
  dplyr::mutate(mid_period = paleomat::get_midpoints(period))

# Extract the modern mean July temperature (PRISM data) from each fossil pollen site. 
modern_temperature <-
  as.data.frame(
    raster::extract(
      x =  paleomat::temp.raster,
      y = tavg_new_90 %>% dplyr::ungroup() %>%
        dplyr::select(long, lat)
    )
  ) %>%
  dplyr::rename(modern = 1)

# Bind the modern data to fossil pollen dataset.
site.preds.unlisted <-
  dplyr::bind_cols(tavg_new_90, modern = modern_temperature$modern)

# Calculate the temperature anomaly and the bootstrapped temperature anomaly.
site.preds.unlisted.anom <- site.preds.unlisted %>%
  dplyr::rowwise() %>%
  dplyr::mutate(anom = value - modern) %>%
  dplyr::mutate(anom_bootstrap = bootstrap - modern)

# If you need to write the data to file, then you can uncomment and run the following.
# readr::write_rds(
#   site.preds.unlisted.anom,
#   here::here("vignettes/data/diagnostics_reconstruction/final_paleomat_dataset3.rds"))

```

For figures and tables in the journal article, please see "Paleomat Figures.Rmd".

