---
title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE)

#update.packages(ask=F, repos = "http://cran.rstudio.com")

if(!require("FedData")) install.packages("FedData")
if(!require("purrr")) install.packages("purrr")

packages <- c("magrittr", "tidyverse", "purrrlyr", "reshape2", # For tidy data
              "foreach", "doParallel", # Packages for parallel processeing
              "rgdal", "sp", "raster", "leaflet", # For spatial data
              "palaeoSig", "rioja", "analogue", "neotoma", "prism", # For MAT and climate data
              "ggplot2", # For plotting
              "rebus", "Hmisc", # For other useful functions
              "rgdal",
              "knitr" # For rmarkdown
              )

purrr::walk(.x = packages,
            .f = FedData::pkg_test)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. 

However, improvements in these techniques, and the increasing breadth of paleoclimatic proxies available have furthered our understanding of the effects of climate-driven variability on past societies. 

This program allows you to reconstruct the climate for multiple locations across North America (when data are sufficient to do). In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008) so that there will be columns of taxa with counts, as well as metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too.

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from Neotoma

```{r load_modernPollen, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Load and compile modern pollen data from the Neotoma Paleoecological database.

# Compile function for the modern pollen data from Neotoma.
compile_MP <- function(x){
  tibble(
    dataset = x$dataset$dataset.meta$dataset.id,
    site.name = x$dataset$site$site.name,
    depth = x$sample.meta$depth,
    lat = x$dataset$site$lat,
    long = x$dataset$site$long)
}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs. 
gpids <- get_table(table.name='GeoPoliticalUnits')
  
NAID <-  gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('United States', 'Canada'),
              GeoPoliticalUnit == 'country') %$%
  GeoPoliticalID

# Get the modern pollen datasets for North America, download the data, and compile the taxa list using Williams and Shuman (2008) (i.e., 'WS64'), then compile the downloads.
MP_datasets <- get_dataset(datasettype='pollen surface sample', gpid = NAID) %>% 
  get_download()

# Get the modern pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.
# Then, combine the rows from the datasets together so that they are now in one tibble.
MP_metadata <- MP_datasets %>%
  map(compile_MP) %>%
  bind_rows()

# Rename first column name (dataset) to site.id.
colnames(MP_metadata)[1] <- 'site.id'

# Get publication data (if available) of the modern pollen datasets. If a date is not available, then a date of 1990 will be assigned.
MP_pubDate <- MP_datasets %>% 
  get_publication()

# # Create an empty data frame to store the output into a dataframe, which will be built iteratively.
# Pub_Year <- data.frame(site.id = integer(),
#                       pub_year = integer())
# 
# # Create another empty data frame that will temporarily store the data from each iteration, which will then be bound to th Pub_Year dataframe.
# tmpFrame <- data.frame(site.id = integer(),
#                       pub_year = integer())

# A nested for loop that will loop through each pollen site, then 'dat' will store each publication record for a given site (might be zero or 7 or higher). The double brackets are used because the publications records for a site are in a list of a list. 'site.id' is used to store the name of the site, which is the name of the list for each site (hence only single brackets). The nested for loop will go through each publication record in a site before going back to the 'i' loop for another site. So, it goes through the length of publication records for a site (length of dat). The year is stored, which double brackets are required to get through another list and get to the data frames in the list. The id and year are stored in the tmpFrame, which is then bound to the Pub_Year through each iteration of the loop.
# for (i in 1:length(MP_pubDate)) {
#   dat <- MP_pubDate[[i]]
#   site.id <- names(MP_pubDate[i])
#   for (j in 1:length(dat)) {
#     pub_year <- dat[[j]]$meta$year
#     tmpFrame <- as.data.frame(cbind(site.id = site.id, pub_year = pub_year))
#     Pub_Year <- as.data.frame(rbind(Pub_Year, tmpFrame))
#   }
# }

Pub_Year <- MP_pubDate %>%
  purrr::map_dfr(.id = site.id,
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 })

# Create an index of what the type (factor) is currently, so that it can be replaced by numeric.
# indx <- sapply(Pub_Year, is.factor)

# Replace the indx for Pub_Year. Use lapply to apply the function of changing the factor and character to a numeric. This will allow to use dplyr::slice below for taking the minimum publication year.
# Pub_Year[indx] <- as.data.frame(lapply(Pub_Year[indx], function(x) as.numeric(as.character(x))))

# Replace all NA values with 1990. This is so the climate can be calculated for 1961:1990 (the 30 years prior to the publication date).
# Pub_Year[is.na(Pub_Year)] <- 1990
Pub_Year %<>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                site.id = as.integer(site.id))

# Group each id together (since sites can have multiple publication years), then only keep the minimum (earliest) publication year for each site (id).
MP_pubDate <- Pub_Year %>% 
    dplyr::group_by(site.id) %>% 
    dplyr::slice(which.min(pub_year))# %>% 
    # as_tibble()

# Get the modern pollen datasets in North America and use map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble.
# KB: added the site ids
MP_counts <- MP_datasets %>%
  map("counts") %>%
  map(as_tibble) %>%
  bind_rows() %>% 
  compile_taxa(list.name = 'WhitmoreFull') %>% 
  as_tibble() %>%
  dplyr::mutate(site.id = MP_datasets %>% # Add the site ids
                  names() %>%
                  as.integer())

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge the metadata with the publication year, using the site.id to match, just to double check.
MP_metadata_pub <- dplyr::left_join(MP_metadata,
                                    MP_pubDate,
                                    by  = "site.id")

# Then, bind the metadata_pub to the modern pollen counts.
# KB: Changed to left_join, which is safer in case row order changes
MP_metadata_counts <- dplyr::left_join(MP_metadata_pub,
                                MP_counts,
                                by  = "site.id")

```

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Get the fossil pollen datasets for the United States and Canada, then download the sites. Next, get the chronology data for each site, so that we can identify the core tops from each site (if available). Next, use lapply to limit to the first data frame in the list of lists (each site has multiple dataframes associated with it); so we limit to the chron.control dataframe. Next, we can strip away the upper most list, which is just the site ID, but we are still left with "siteID.chron.control", as the name of the dataframe.
NAfossil_datasets <- neotoma::get_dataset(ageold = 41000, 
                                  ageyoung = -50, 
                                  gpid = NAID,
                                  datasettype = 'pollen')

NAfossil.chron <- NAfossil_datasets %>% 
                  get_chroncontrol() %>% 
                  lapply("[", 1) %>% 
                  unlist(recursive=F)

# Get a list of the site IDs that are identifying the chron.control dataframes. Here, we get the unique site ID and turn it into a list. 
NAfd_chronList <- lapply(names(NAfossil.chron), function(x) x) %>%
  unlist()

# Next, we remove the string ".chron.control" that is part of each dataframe name, then turn it into a dataframe.
NAfd_chronList <- sapply(strsplit(NAfd_chronList, split='.', fixed=TRUE), function(x) (x[1])) %>% 
  as.data.frame() %>% 
  set_colnames("site.ID")


# Create a loop to go through the list of dataframes, and add a new column, which will hold the site ID. Then, bind all the dataframes together to create one large dataframe of the chron.control with all sites.

# Set an initial empty dataframe and add new site ID column.
coreTops <- NAfossil.chron[[1]] %>% cbind(site.ID="site.ID") %>% dplyr::slice(0)

for(i in 1:length(NAfossil.chron))
{
  tmpFrame <- cbind(NAfossil.chron[[i]], site.ID=as.character(NAfd_chronList[i,1]))
  coreTops <- rbind(coreTops, tmpFrame)
}

# Get only rows that have "Core top" in the description, since these will be added to our modern pollen dataset.
coreTops <- coreTops[grep("Core top", coreTops$control.type), ]

# Create a list of site IDs in order to download only those fossil pollen sites.
coreTopsites <- as.vector(coreTops$site.ID)

# Download the sites that have core tops. 
CT_pubDate <- get_download(as.numeric(coreTopsites)) %>% 
  get_publication()

# Create an empty data frame to store the output into a dataframe, which will be built iteratively.
CTPub_Year <- data.frame(site.id=numeric(),
                      pub_year=numeric())

# Create another empty data frame that will temporarily store the data from each iteration, which will then be bound to th Pub_Year dataframe.
CTtmpFrame <- data.frame(site.id=numeric(),
                      pub_year=numeric())

# A nested for loop that will loop through each pollen site, then 'dat' will store each publication record for a given site (might be zero or 7 or higher). The double brackets are used because the publications records for a site are in a list of a list. 'site.id' is used to store the name of the site, which is the name of the list for each site (hence only single brackets). The nested for loop will go through each publication record in a site before going back to the 'i' loop for another site. So, it goes through the length of publication records for a site (length of dat). The year is stored, which double brackets are required to get through another list and get to the data frames in the list. The id and year are stored in the tmpFrame, which is then bound to the CTPub_Year through each iteration of the loop.
for (i in 1:length(CT_pubDate)) {
  dat <- CT_pubDate[[i]]
  site.id <- names(CT_pubDate[i])
  for (j in 1:length(dat)) {
    pub_year <- dat[[j]]$meta$year
    CTtmpFrame <- as.data.frame(cbind(site.id = site.id, pub_year = pub_year))
    CTPub_Year <- as.data.frame(rbind(CTPub_Year, CTtmpFrame))
  }
}

# Create an index of what the type (factor) is currently, so that it can be replaced by numeric.
CTindx <- sapply(CTPub_Year, is.factor)

# Replace the indx for Pub_Year. Use lapply to apply the function of changing the factor and character to a numeric. This will allow to use dplyr::slice below for taking the minimum publication year.
CTPub_Year[CTindx] <- as.data.frame(lapply(CTPub_Year[CTindx], function(x) as.numeric(as.character(x))))

# Replace all NA values with 1990. This is so the climate can be calculated for 1961:1990 (the 30 years prior to the publication date).
CTPub_Year[is.na(CTPub_Year)] <- 1990

# Group each id together (since sites can have multiple publication years), then only keep the minimum publication year for each site (id).
CT_pubDate <- CTPub_Year %>% 
    group_by(site.id) %>% 
    slice(which.min(pub_year)) %>% 
    as_tibble()

# Download the United States and Canada site fossil datasets and compile taxa into one dataframe.
NAfossil_download <- NAfossil_datasets %>% 
  get_download() %>%
  compile_taxa(list.name = "WhitmoreFull") %>%
  compile_downloads()

# Use site IDs from coreTopsites to get only those fossil pollen sites from the downloaded and compiled fossil datasets (still in one dataframe), which is the variable, NAfossil_download (this was all of the fossil pollen locations in the United States and Canada).
NAfossil_CT <- NAfossil_download[NAfossil_download$.id %in% coreTopsites,]

# Next, need to identify the row at each site that has the core top. Then, extract that row of data for each site.
NACT <- subset(coreTops, select = c("site.ID", "depth", "age"))
colnames(NACT)[1] <- ".id"
NACT2 <- merge(NAfossil_CT, NACT, by  = c(".id", "depth", "age"))

colnames(CT_pubDate)[1] <- ".id"

# Add publication year to the core tops data.
NACT2 <- merge(NACT2, CT_pubDate, by = ".id")

# Drop columns that are not in the modern pollen data above.
NACT2 <- subset(NACT2, select = - c(age, age.young, age.old, date.type, dataset))

# Rename first column name (dataset) to site.id to match modern pollen data.
colnames(NACT2)[1] <- "site.id"

# Switch site name and depth columns. Then, rename those two headings.
NACT2[ , c(2,3)] <- NACT2[ , c(3,2)]
colnames(NACT2)[c(2, 3)] <- c("site.name", "depth")

# Change dataset column data type from character to integer.
NACT2$site.id <- as.integer(NACT2$site.id)

# Additional pollen taxa from the modern surface samples should be added, since we need to have the same column headings before adding the core tops data to the rest of the modern pollen data.
NACT3 <- NACT2 %>% 
  bind_rows(as.data.frame(MP_metadata_counts[1,]))

# The last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
NACT3 <- NACT3[-nrow(NACT3),]

# Change any NA values to zero.
NACT3[is.na(NACT3)] <- 0

# Additional pollen taxa from the fossil samples should be added to the modern dataset, since we need to have the same column headings before combining the core tops data to the the modern pollen data.
MP_metadata_counts_Update <- MP_metadata_counts %>% 
  bind_rows(as.data.frame(NACT3[1,]))

# The last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
MP_metadata_counts_Update <- MP_metadata_counts_Update[-nrow(MP_metadata_counts_Update),]

# Change any NA values to zero.
MP_metadata_counts_Update[is.na(MP_metadata_counts_Update)] <- 0

# Re-order the column names to that of the modern pollen data.
NACT4 <- NACT3[,colnames(MP_metadata_counts_Update)]

MP_metadata_counts_CTs <- rbind(MP_metadata_counts_Update, NACT3)

```

The Neotoma Modern Pollen Database contains `r nrow(MP_counts)` samples, representing `r ncol(MP_counts)` different pollen taxa.

## Map Modern Pollen Sample Locations

To examine locations of the modern pollen data, we can use `leaflet` to plot the modern pollen locations.

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -101.05,
          lat = 36.68,
          zoom = 3)

Modernlocations <- MP_datasets %>% get_site

map %>% addMarkers(lng = Modernlocations$long, lat = Modernlocations$lat, 
                   popup = paste0('<b>', as.character(Modernlocations$site.name), '</b><br>',
                                  '<a href=http://apps.neotomadb.org/explorer/?datasetid=', MP_datasets, '>Neotoma Database</a>'))


# Map Core top locations.
map %>% addMarkers(lng = NACT3$long, lat = NACT3$lat, 
                   popup = paste0('<b>', as.character(NACT3$site.name), '</b><br>'))

map %>% addMarkers(lng = NAfossil_CT$long, lat = NAfossil_CT$lat, 
                   popup = paste0('<b>', as.character(NAfossil_CT$site.name), '</b><br>'))
                                  

# #Find locations that have missing climate data
# missingLocations <- d[d$ID %in% missing2 ,]
# 
# map %>% addMarkers(lng = missingLocations$long, lat = missingLocations$lat, 
#                    popup = paste0('<b>', as.character(missingLocations$site.name), '</b><br>',
#                                   '<a href=http://apps.neotomadb.org/explorer/?datasetid=', MP_datasets, '>Neotoma Database</a>'))

```


# Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).  

```{r load_Prsim, echo = FALSE, warning=FALSE}
# Load Prism Climate data.

# Get Modern Pollen locational data.
names <- rownames(MP_counts)

#Extract longitude and latitude data.
lons <- unlist(lapply(all,FUN=function(x){MP_metadata_counts$long}))
lats <- unlist(lapply(all,FUN=function(x){MP_metadata_counts$lat}))

# Bind longitude and latitude and convert to SpatialPoints.
coords <- cbind(LONGITUDE=lons,LATITUDE=lats)
coordsPoints <- SpatialPoints(coords, proj4string=CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Create a SpatialPointsDataFrame of the samples and clean up locations.
# points <- SpatialPointsDataFrame(coords,data.frame(NAME=names,MP_counts), 
#                                  proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))
# locations <- points[!duplicated(coordinates(points)),]


### Script for PRISM climate extraction adapted from R. Kyle Bocinsky, Johnathan Rush, Keith W. Kintigh, and Timothy A. Kohler, 2016. Exploration and Exploitation in the Macrohistory of the Prehispanic Pueblo Southwest. Science Advances.

## This script extracts data from the ~800 m monthly PRISM dataset for the modern pollen locations. It first defines the extent, chops that extent into 800 m (30 arc-second) PRISM cells, and subdivides the extent into 14,440 (120x120) cell chunks for computation. (the chunks are 1x1 degree). These chunks are saved for later computation.

## Set the working directory to the directory of this file!
setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/vignettes")

# Create an output directory
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

# This MUST point at an original LT81 dataset available from the PRISM climate group (http://www.prism.oregonstate.edu).
PRISM800.DIR <- "/Volumes/DATA/PRISM/LT81_800M/"


# Specify a directory for extraction
EXTRACTION.DIR <- list.files(paste0(PRISM800.DIR), recursive=TRUE, full.names=T)

# The climate parameters to be extracted
types <- c("ppt", "tmin","tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# (Down)Load the states shapefile form the National Atlas
if(!dir.exists("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statep010")){
  dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/", showWarnings = F, recursive = T)
  download.file("https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/statesp010g.shp_nt00938.tar.gz", destfile="/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", mode='wb')
  untar("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", exdir="/Volumes/DATA/NATIONAL_ATLAS/statesp010g")
}
states <- readOGR("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938", layer='statesp010g')

# Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
states <- sp::spTransform(states, sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))
# Get the extent (i.e., the continental United States)
extent.states <- raster::extent(states)
# Floor the minimums, ceiling the maximums.
extent.states@xmin <- floor(extent.states@xmin)
extent.states@ymin <- floor(extent.states@ymin)
extent.states@xmax <- ceiling(extent.states@xmax)
extent.states@ymax <- ceiling(extent.states@ymax)

# Get list of all file names in the prism directory.
monthly.files <- list.files(paste0(PRISM800.DIR), recursive=T, full.names=T)

# Use the rebus package to generate a regular expression for a number range, which will allow for the extraction of specific years in the prism data (i.e., 1961-1990).
rx <- number_range(1961, 1990)

# Now use the expression stored in rx, and trim list of files to to 1961:1990.
monthly.files <- grep(rx, monthly.files, value = TRUE, perl = TRUE)

# Trim to only file names that are rasters.
monthly.files <- grep("*\\.bil$", monthly.files, value=TRUE)
monthly.files <- grep("spqc", monthly.files, value=TRUE, invert=T)
monthly.files <- grep("/cai", monthly.files, value=TRUE)

# Generate the raster stack.
type.list <- raster::stack(monthly.files,native=F,quick=T)

#END Bocinsky et al. 2016 adapted script.

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
climate.points <- raster::extract(x = type.list, y = coordsPoints, df = TRUE)

# Convert climate data from wide to long format, but keep ID so that we can remember the location that each record came from. Then, we use the separate function to split apart the long prism file names, of which some of the new columns we will not need; therefore, they are labeled as garbage, and these get deleted. Finally, we separate the yearmonth into two columns. This will allow for these to be grouped together to do summarizing below. sep = 4 is the location in the string of six numbers (yearmonth) to separate.
data_long <- melt(climate.points,
        # ID variables - all the variables to keep but not split apart.
    id.vars="ID") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "variable", "garbage2", "garbage3", "garbage4", "yearmonth"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage")) %>% 
  separate(col = "yearmonth", into = c("year", "month"), sep = 4)

# Take the long format data, then group by ID, which is a unique location, the variable (i.e., precipitiation, min. temp., and max temp.), and each month (e.g., 01 for January, 02 for February, etc.). So, we essentially get a 30 year average for each of the 12 months for the 3 variables at each unique location.
data_avgs <- data_long %>%
  dplyr::group_by(ID, variable, month) %>% 
  dplyr::summarize(mean = mean(value))

# The precipitation averages are extracted into its own dataframe.
ppt_avgs <- data_avgs %>% 
  dplyr::filter(variable == "ppt")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
tmp_avgs <- data_avgs %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(ID, month) %>% 
  dplyr::summarize(mean = mean(mean)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(ID, variable, month, mean)

# Now, combine the precipitation and temperature data back into one dataframe, and convert from long back to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. The, rename all the column names. 
clim_wide <- bind_rows(ppt_avgs, tmp_avgs) %>% 
  dcast(ID ~ variable + month, value.var="mean") %>% 
  dplyr::select(-one_of("ID")) %>% 
  set_colnames(c('pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))
  
```

# Load WorldClim Climate Data

[WorldClim 2.0](http://worldclim.org/version2) has 31 years (1970-2000) of averaged climate data and here we use a resolution of 30 seconds (~1 km^2^).

```{r load_WordClim, echo = FALSE, warning=FALSE}
# Load WorldClim Climate data.

## Set the working directory to the directory of this file!
setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/vignettes")

# Create an output directory
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

# This MUST point at an original LT81 dataset available from the PRISM climate group (http://www.prism.oregonstate.edu).
WC.DIR <- "/Volumes/DATA/WORLDCLIM_2.0/"

# Specify a directory for extraction
EXTRACTION.DIR <- "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data"

# The climate parameters to be extracted
types <- c("ppt", "tmin","tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# Get list of all file names in the prism directory.
wc.files <- list.files(paste0(WC.DIR), recursive=T, full.names=T)

wc.files <- grep("*\\.tif$", wc.files, value=TRUE)

# Generate the raster stack.
wc.list <- raster::stack(wc.files,native=F,quick=T)

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
WCclimate.points <- raster::extract(x = wc.list, y = coordsPoints, df = TRUE)


WCdata_long <- melt(WCclimate.points,
        # ID variables - all the variables to keep but not split apart.
    id.vars="ID") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "garbage2", "variable", "month"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage"))

# The precipitation averages are extracted into its own dataframe.
WCppt_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "prec")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
WCtmp_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(ID, month) %>% 
  dplyr::summarize(value = mean(value)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(ID, variable, month, value)

# Now, combine the precipitation and temperature data back into one dataframe, and convert from long back to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. The, rename all the column names. 
WCclim_wide <- bind_rows(WCppt_avgs, WCtmp_avgs) %>% 
  dcast(ID ~ variable + month, value.var="value") %>% 
  dplyr::select(-one_of("ID")) %>% 
  set_colnames(c('pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))

```

## Clean up datasets to remove NAs

Need to further clean up the datasets to remove any rows with NA values. Although currently working with United States data, we do not currently have climate (prism) data for Alaska, which is the reason for all rows with NAs.

```{r echo=FALSE, warning=FALSE}

# Get row number in first column that have NA values.
NAindex <- which(is.na(clim_wide[,1]), arr.ind=FALSE)

# Remove rows with NA values, using the index.
Clim_noNAs <- clim_wide[-(NAindex),]

# Remove rows in Moden Pollen dataset, using the same index. This will keep the sites/locations still matched up between the climate data and the modern pollen counts.
MP_counts_noNAs <- MP_counts[-(NAindex),]

```

Modern Pollen data contains `r nrow(MP_counts_noNAs)` samples, representing `r ncol(MP_counts_noNAs)` different pollen taxa. 

### Checking the calibration data set

To test the model, I used the `palaeoSig` package's `randomTF()` function, which tests the models against randomly sorted data. If we do get a significance for a climate variable, then we know that the model reconstruction is better than random. The model takes the proportion of variance accounted for by the actual data, and then compares it to the proportion of variance accounted for by the randomized data. Here, we use a wrapper function for [randomTF](https://github.com/NeotomaDB/Workbooks/blob/master/PollenClimate/R/sig_test.R).

The MAT method is testing whether the modern calibration is able to detect signals in each of the climate parameters. The example here uses the entire North American Modern Pollen Database, rather than a targeted data subset.

#### WA - Monotone Deshrinking

```{r WA_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# This is a wrapper for the randomTF function.
source('../R/sig_test.R')

wa_sig <- run_tf(pollen = MP_counts_noNAs, 
                 climate = Clim_noNAs[,13:24], 
                 func = WA, col = 1 , mono = TRUE)

wa_sig[[1]]
```

The columns are standard weighted average. One positive of using monotone shrinking is that it does not suffer from spatial-autocorrelation like with using MAT. Therefore, there has been quite a bit of criticism against MAT. You can do a spatial variogram of the model, then test to see if it is significant. MAT gives you a root mean error that is much higher than what it probably really is. Sometimes it is just because the cores are so close to one another, more so than the assemblages reflect climate. Consider weighted averaging (partially squared not really worth it. ) Weighted averaging with the monotone shrinking is very fast.

#### MAT - ten closest

```{r MAT_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# For some reason this fails consistently when we try to use the weighted MAT (col=2).
mat_sig <- run_tf(pollen = MP_counts_noNAs, 
                  climate = Clim_noNAs[,13:24],
                  func = MAT, col = 1, k = 10)
#col 1 is just the closest analogue.
mat_sig[[1]]

```

The variance explained is very low for some variables, although the p value is very low. This highlights that in some cases you may not just want to rely on the p value. This also shows the issue with temporal autocorrelation on temperature and, to some extent also, on precipitation variables.

# Reconstruction Statistics

## Reconstruction Significance

Now we can do a similar test on fossil assemblage reconstructions. This again uses the same `randomTF()` function, but the variance explained and significance will probably change given that we are using a slightly more constrained dataset.

### Obtaining a fossil pollen record from Neotoma

Now, we apply a reconstruction to a real dataset. The record has coverage across multiple timescales.  For this example, we will restrict the analysis to a record from Colorado.

```{r, SW_sites, echo=TRUE, message=FALSE, warning=FALSE}

# Use gpids to get the Four Corners area (or geopolitical unit) in North America.
# Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names 
# with associated IDs. 
gpids <- get_table(table.name='GeoPoliticalUnits')

# Currently using a work around to get data from specific sites by defining the states in 
# the Neotoma GeoPolitical Units table (http://api.neotomadb.org/apdx/geopol.htm). It 
# occasionally will not pull in the correct data. This is the same thing that was done 
# with the modern pollen data from Neotoma above.
SWID <-  gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('Colorado', 'Utah', 'Arizona', 'New Mexico'),
              GeoPoliticalUnit == 'state') %$%
  GeoPoliticalID

# We want to restrict to fossil pollen datasets for the Four Corners from the last 
# 2000 years.
UUSS_sites <- neotoma::get_dataset(ageold = 2000, 
                                  ageyoung = -50, 
                                  gpid = SWID,
                                  datasettype = 'pollen')

# Here, we map the dataset IDs to the FourCorners, so that we can use this for mapping 
# below.
datasets <- UUSS_sites %>% map_int(function(x) x$dataset.meta$dataset.id)

# Get all fossil pollen datasets for United States and Canada from the last 2000 years.
NA_sites <- neotoma::get_dataset(ageold = 2000, 
                                  ageyoung = -50, 
                                  gpid = NAID,
                                  datasettype = 'pollen')

```

This returns `r length(UUSS_sites)` sites. The `neotoma` package provides plotting capabilities, but `leaflet` allows for more interactive plotting.

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

library(leaflet)

# Here, we set up how we want our map to look like.
map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -109.05,
          lat = 31.68,
          zoom = 4)

# Now, we need to get the site data for the Four Corners. This is another step from 
# Neotoma.
locations <- UUSS_sites %>% get_site

# If you wanted a simple map, then you would just need lat and long. However, I have made 
# the addition, so that you can click on each site and get the site name and can also be 
# redirected to Neotoma webpage to get more information about each site.
map %>% addMarkers(lng = locations$long, 
                   lat = locations$lat, 
                   popup = paste0('<b>', 
                   as.character(locations$site.name), 
                   '</b><br>',
                   
                   '<a href=http://apps.neotomadb.org/explorer/?datasetid=', 
                   datasets, 
                   '>Neotoma Database</a>'))

# For plotting all modern pollen locations.
# Modernmap <- leaflet(width = 800, height = 800) %>%
#   addTiles %>%
#   setView(lng = -109.05,
#           lat = 34.28,
#           zoom = 2)

# Modernmap %>% addMarkers(lng = Location$Longitude, lat = Location$Latitude)

# For plotting all fossil pollen sites in United States and Canada. 
# NA_site_locations <- NA_sites %>% get_site
# 
# NAfossil_map <- leaflet(width = 800, height = 800) %>%
#   addTiles %>%
#   setView(lng = -109.05,
#           lat = 34.28,
#           zoom = 2)
# 
# NAfossil_map %>% addMarkers(lng = NA_site_locations$long, lat = NA_site_locations$lat)


```

#### Obtaining a fossil pollen record for one site in Neotoma

Data record `12` within the Upper United States Southwest data represents Eldora Fen, Colorado, recorded by Louis J.Maher Jr. The record spans 7,900 years, which provides some information for environmental change at the site.

```{r echo = TRUE}

# Another step from Neotoma, we need to download the data for the site.
EldoraFen <- get_download(UUSS_sites[[9]])

# Here we can also use Stratiplot to look at the pollen profile and change over time for 
# the Eldora Fen site.
analogue::Stratiplot(EldoraFen$`790`$sample.meta$age ~ 
                       Juniperus + 
                       `Pinus undiff.` + 
                       Salix + 
                       Quercus,
                   data = (as.data.frame(EldoraFen$`790`$counts)), 
                   xlab = "Counts", 
                   ylab = "Age")

```

Using the complete North American dataset, we focus on a reconstruction of summer temperature for this record (`tjul`).  We also need to clean up the data sets too:

```{r, warning=FALSE, echo=FALSE, results='hide'}

# Another step in Neotoma is to compile the taxa according to the Whitmore compilation function. This allows to group the taxa in a particular way.
fossil_data <-  EldoraFen %>%
  compile_taxa(list.name = "WhitmoreFull") %>%
  
  compile_downloads()

# Prediction requires the addition of all Pollen taxa:

# First, additional pollen taxa from the Pollen dataset of the North American Pollen Dataset should be added.
fossil_data <- fossil_data %>% 
  bind_rows(as.data.frame(MP_counts_noNAs[1,]))

#Second, the last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
fossil_data <- fossil_data[-nrow(fossil_data),]

#Third, change any NA values to zero.
fossil_data[is.na(fossil_data)] <- 0

#Last, re-order the column names to that of the North American Pollen Dataset.
fossil_data <- fossil_data[,colnames(MP_counts_noNAs)]

```

#### Obtaining fossil pollen records for all sites in UUSS dataset

Using the complete North American dataset, we focus on a reconstruction of temperature for these records.  We also need to clean up the data sets too:

```{r, warning=FALSE, echo=FALSE, results='hide'}


# Ideally would like to loop through all of the UUSS sites then store those as separate variables; so that each site could go through reconstruction significance to see if it is possible to actually reconstruct that site. Then, we would like to do the reconstruction and figure for each individual site that can be reconstructed. Prediction also requires the addition of all Pollen taxa.
site_names <- as.list(names(UUSS_sites))

# Create an empty list for all the fossil sites to hold the new variable names.
fossil_sites <- list()

for (i in 1:length(UUSS_sites)) {
  nam <- paste("fossil_", site_names[[i]], sep = "")
  dat <- UUSS_sites[[i]] %>%
          get_download() %>% 
          compile_taxa(list.name = "WhitmoreFull") %>%
          compile_downloads() %>% 
          # First, additional pollen taxa from the Pollen dataset of the North American Pollen Dataset should be added.
          bind_rows(as.data.frame(MP_counts_noNAs[1,]))
  # Second, the last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
  dat <- dat[-nrow(dat),]
  # Third, change any NA values to zero.
  dat[is.na(dat)] <- 0
  # Last, re-order the column names to that of the North American Pollen Dataset.
  dat <- dat[,colnames(MP_counts_noNAs)]
  assign(nam, dat)
  # Create a list of the fossil sites with the new names of the variables, which is "fossil_...".
  fossil_sites[i] <- nam
}

```

#### WA - Monotone Deshrinking

```{r WA_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# These are for the one site reconstruction.
# wa_reconst <- run_tf2(pollen = MP_counts_noNAs, fossil = fossil_data,
#        climate = Clim_noNAs[,13:24],
#        func = WA, col = 1, mono = TRUE)
#  
# wa_reconst[[1]]

for (i in 1:length(fossil_sites)) {
  nam <- paste("wa_reconst_", fossil_sites[[i]], sep = "")
  dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
       climate = Clim_noNAs[,13:24],
       func = WA, col = 1, mono = TRUE)
  assign(nam, dat)
}

```


#### MAT - ten closest

```{r MAT_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# mat_reconst <- run_tf2(MP_counts_noNAs, fossil = fossil_data, 
#        climate = Clim_noNAs[,13:24], 
#        func = MAT, col = 2, k = 10) 
# 
# mat_reconst[[1]]

for (i in 1:length(fossil_sites)) {
  nam <- paste("mat_reconst_", fossil_sites[[i]], sep = "")
  dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
       climate = Clim_noNAs[,13:24],
       func = mat, col = 2, k = 10)
  assign(nam, dat)
}

```

Again, no significance for the model. This indicates that we are just not able to see a signal within the data, but this is probably due to the calibration dataset being too broad.

## Reconstruction

Once we have validated the methods, we re-run the analyses using the two methods, MAT and WA.

### Model Summary

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This often takes a long time, so using a work around to make it a little quicker.

if ("mat_reconst.RDS" %in% list.files("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output")) {
  mat_reconst <- readRDS(file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/mat_reconst.RDS')
} else {
  mat_reconst <- predict(rioja::MAT(y = MP_counts_noNAs, 
                                    x = Clim_noNAs[,1], 
                                    k = 10, 
                                    dist.method="sq.chord",
                                    lean = FALSE),
                         newdata = fossil_data, 
                         sse = TRUE)
  saveRDS(mat_reconst, file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/mat_reconst.RDS')
}


# Currently the rioja::WA function cannot run when species data have zero abundances. Therefore, here we determine what columns in the modern pollen data have zero column sums, and get an index of those columns, then delete the columns. The same index is used to delete the columns from the fossil pollen data.
dd <- unname(which((colSums(MP_counts_noNAs, na.rm=T) < 0.000001), arr.ind = TRUE))
MP_counts_noNAs2 <- MP_counts_noNAs[,-(dd)]
fossil_data2 <- fossil_data[,-(dd)]


if ("wa_reconst.RDS" %in% list.files("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output")) {
  wa_reconst <- readRDS(file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
} else {
  wa_reconst <-  predict(rioja::WA(y = tran(MP_counts_noNAs2, method = 'proportion'), 
                                   x = Clim_noNAs[1,], lean = FALSE),
                         newdata = fossil_data2, sse = TRUE)
  saveRDS(wa_reconst, file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
}

```


```{r plot_reconst, fig.width=8, fig.height=9, echo=FALSE}

# Here, the data is put into one data frame to allow for ease of plotting.
get_clim <-  data.frame(age = EldoraFen$`790`$sample.meta$age,
                        reconst = mat_reconst$fit[,1],
                         err     = mat_reconst$SEP[,1],
                         model   = rep("MAT", 
                                       each = nrow(fossil_data)),
                         climate = 'tjul')

# Now, we use ggplot to plot the July temperature reconstruction.
ggplot(get_clim, aes(x = age, y = reconst)) + 
  geom_line(aes(col = model)) +
  geom_ribbon(aes(ymin = reconst - err, 
                  ymax = reconst + err, 
                  group = model), 
              alpha = 0.2) +
  
    facet_wrap(~climate, scales = 'free', ncol = 2) +
  
    xlab("Age - kyr BP") +
  
    ylab("Reconstructed Parameter")

```

# Saving to file

Save values to file.
```{r save_reconstruction, echo=FALSE}

write.csv(get_clim %>% 
            dplyr::select(-one_of("err")) %>% 
            tidyr::unite(col = "model_climate", c("model","climate"), sep = "_", remove = TRUE) %>% 
            tidyr::spread(model_climate, reconst), 
          "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/clim_reconst.csv")

write.csv(get_clim %>% 
            dplyr::select(-one_of("reconst")) %>% 
            tidyr::unite(col = "model_climate", c("model","climate"), sep = "_", remove = TRUE) %>% 
            tidyr::spread(model_climate, err), 
          "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/clim_err.csv")

```
