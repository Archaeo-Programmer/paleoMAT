---
title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE)

#update.packages(ask=F, repos = "http://cran.rstudio.com")

if(!require("FedData")) install.packages("FedData")
if(!require("purrr")) install.packages("purrr")

packages <- c("magrittr", "tidyverse", "purrrlyr", "reshape2", "dplyr", "plyr", # For tidy data
              "foreach", "doParallel", # Packages for parallel processeing
              "rgdal", "sp", "raster", "leaflet", # For spatial data
              "palaeoSig", "rioja", "analogue", "neotoma", "prism", # For MAT and climate data
              "ggplot2", "RColorBrewer", "plotly", # For plotting
              "rebus", "Hmisc", # For other useful functions
              "rgdal",
              "knitr", # For rmarkdown
              "zoo", "Hmisc", # Date conversions
              "rcarbon", # Converting BP to BC/AD 
              "visreg" # For regressions
)

purrr::walk(.x = packages,
            .f = FedData::pkg_test)

setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat")

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. 

However, improvements in these techniques, and the increasing breadth of paleoclimatic proxies available have furthered our understanding of the effects of climate-driven variability on past societies. 

This program allows you to reconstruct the climate for multiple locations across North America (when data are sufficient to do). In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008) so that there will be columns of taxa with counts, as well as metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too.

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from Neotoma

```{r load_modernPollen, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Load and compile modern pollen data from the Neotoma Paleoecological database.

# Compile function for the modern pollen data from Neotoma.
compile_pollen <- function(x){
  tibble(
    dataset.id = x$dataset$dataset.meta$dataset.id,
    site.id = x$dataset$site.data$site.id,
    sample.id = x$sample.meta$sample.id,
    site.name = x$dataset$site$site.name,
    depth = x$sample.meta$depth,
    lat = x$dataset$site$lat,
    long = x$dataset$site$long,
    elev = x$dataset$site$elev, 
    age = x$sample.meta$age)
}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs. 
gpids <- neotoma::get_table(table.name='GeoPoliticalUnits')

NAID <-  gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('United States', 'Canada'),
                GeoPoliticalUnit == 'country') %$%
  GeoPoliticalID

# This is only used to count the number of records in the contiguous US, which ignores Alaksa and Hawaii. However, the data needs to be retained, as it can be used for WorldClim Data, but not PRISM.
# USID <-  gpids %>%
#   dplyr::filter(HigherGeoPoliticalID %in% "6129",
#                 Rank == "2") %>%
#   dplyr::filter(GeoPoliticalName != 'Alaska' & GeoPoliticalName != 'Hawaii') %$%
#   GeoPoliticalID

# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/MP_datasets.Rds")){
  neotoma::get_dataset(datasettype='pollen surface sample', gpid = NAID) %>% 
    neotoma::get_download() %>%
    readr::write_rds("./data/raw_data/MP_datasets.Rds")
}
MP_datasets <- readr::read_rds("./data/raw_data/MP_datasets.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/MP_pubs.Rds")){
  MP_datasets %>% 
    get_publication() %>%
    readr::write_rds("./data/raw_data/MP_pubs.Rds")
}
MP_pubs <- readr::read_rds("./data/raw_data/MP_pubs.Rds")

# Get the modern pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.
# Then, combine the rows from the datasets together so that they are now in one tibble.
MP_metadata <- MP_datasets %>%
  map(compile_pollen) %>%
  bind_rows() %>%
  dplyr::mutate(type = "surface sample")

MP_pub_date <- MP_pubs %>%
  purrr::map_dfr(.id = "dataset.id",
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 }) %>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                dataset.id = as.integer(dataset.id)) %>% 
  dplyr::group_by(dataset.id) %>% 
  dplyr::slice(which.min(pub_year))

# Get the modern pollen datasets in North America and use map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble.
# KB: added the site ids
MP_counts <- MP_datasets %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  bind_rows(.id = "dataset.id") %>% 
  dplyr::mutate(dataset.id = as.integer(dataset.id))

# Sort the taxa to be in alphabetical order. 
MP_counts <- MP_counts[,c(names(MP_counts)[1], sort(names(MP_counts)[2:ncol(MP_counts)]))]

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge the metadata with the publication year, using the dataset.id to match, just to double check.
MP_metadata_counts <- dplyr::left_join(MP_metadata,
                                       MP_pub_date,
                                       by  = "dataset.id") %>%
  dplyr::left_join(MP_counts,
                   by  = "dataset.id") %>%
  dplyr::arrange(dataset.id)


```

## Process North American Fossil Pollen Data.

```{r FossilPollen, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Get the fossil pollen datasets for the United States and Canada, then download the sites. Next, get the chronology data for each site, so that we can identify the core tops from each site (if available). Next, use lapply to limit to the first data frame in the list of lists (each site has multiple dataframes associated with it); so we limit to the chron.control dataframe. Next, we can strip away the upper most list, which is just the site ID, but we are still left with "siteID.chron.control", as the name of the dataframe.

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/NAfossil_datasets.Rds")){
  neotoma::get_dataset(gpid = NAID,
                       datasettype = 'pollen') %>% 
    neotoma::get_download() %>%
    readr::write_rds("./data/raw_data/NAfossil_datasets.Rds")
}
NAfossil_datasets <- readr::read_rds("./data/raw_data/NAfossil_datasets.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/NAfossil_pubs.Rds")){
  NAfossil_datasets %>% 
    get_publication() %>%
    readr::write_rds("./data/raw_data/NAfossil_pubs.Rds")
}
NAfossil_pubs <- readr::read_rds("./data/raw_data/NAfossil_pubs.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
# For some reason, some of the sites are missing chronologies, which is causing this to fail.
if(!file.exists("./data/raw_data/NAfossil_chron.Rds")){
  NAfossil_datasets %>% 
    get_chroncontrol() %>%
    readr::write_rds("./data/raw_data/NAfossil_chron.Rds")
}
NAfossil_chron <- readr::read_rds("./data/raw_data/NAfossil_chron.Rds")

# Get the fossil pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.Then, combine the rows from the datasets together so that they are now in one tibble.
NAfossil_metadata <- NAfossil_datasets %>%
  purrr::map(compile_pollen) %>%
  bind_rows() %>%
  dplyr::mutate(type = "fossil") %>% 
  mutate(dataset.id = as.integer(dataset.id))

NAfossil_metadata$site.id <- as.integer(NAfossil_metadata$site.id)

# Get earliest publication dates for each fossil pollen site.
NAfossil_pub_date <- NAfossil_pubs %>%
  purrr::map_dfr(.id = "dataset.id",
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 }) %>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                dataset.id = as.integer(dataset.id)) %>% 
  dplyr::group_by(dataset.id) %>% 
  dplyr::slice(which.min(pub_year))

NAfossil_pub_date$dataset.id <- as.integer(NAfossil_pub_date$dataset.id)

# Get the fossil datasets in North America and use purrr::map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble. We remove dataset.id because dplyr::left_join cannot process having multiple rows with the same dataset.id.
NAfossil_counts <- NAfossil_datasets %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  purrr::imap(.f = function(x,y){
    cbind(NAfossil_datasets[[y]]$sample.meta %>%
            dplyr::select(dataset.id, sample.id),
          x)
  }) %>%
  bind_rows() %>% 
  # select(-ends_with("dataset.id")) %>%
  mutate_all(funs(ifelse(is.na(.), 0, .))) %>%
  as_tibble()

# Sort the taxa to be in alphabetical order. 
NAfossil_counts <- NAfossil_counts[,c(names(NAfossil_counts)[1:2], sort(names(NAfossil_counts)[3:ncol(NAfossil_counts)]))]

# Check that Modern pollen dataset has the same columns as the fossil pollen dataset.
# sameVariables <- function(x,y) {
#     for (i in names(x)) {
#         if (!(i %in% names(y))) {
#             print('Warning: Names are not the same!')
#             break
#         }
#         else if(i==tail(names(y),n=1)) {
#             print('Names are identical.')
#         }
#     }
# }
# 
# sameVariables(NAfossil_counts, MPCT_counts_noNAs)

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge (using a left_join) the metadata with the publication year, using the dataset.id to match, just to double check.
NAfossil_metadata_pub <- dplyr::left_join(NAfossil_metadata,
                                          NAfossil_pub_date,
                                          by  = "dataset.id")

# The fossil counts are just bound to the metadata_pub because it is not possible to do a left join when multiple rows have the same dataset.id.
NAfossil_metadata_counts <- dplyr::left_join(NAfossil_metadata_pub, NAfossil_counts,
                                             by = c("dataset.id" = "dataset.id","sample.id")) %>%
  dplyr::arrange(dataset.id)

```

## Process fossil pollen sites in Mexico, as one site falls right on the US-Mexico Border.

```{r FossilPollen_MexicoBorder, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

mexicoID <- gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('Mexico'),
                GeoPoliticalUnit == 'country') %$%
  GeoPoliticalID

mexico_data <- neotoma::get_dataset(gpid = mexicoID,
                       datasettype = 'pollen') %>% 
    neotoma::get_download()

mexico_pub <- mexico_data %>% 
    get_publication()

# Get the fossil pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.Then, combine the rows from the datasets together so that they are now in one tibble.
mexicoFossil_metadata <- mexico_data %>%
  purrr::map(compile_pollen) %>%
  bind_rows() %>%
  dplyr::mutate(type = "fossil") %>% 
  mutate(dataset.id = as.integer(dataset.id))

mexicoFossil_metadata$site.id <- as.integer(mexicoFossil_metadata$site.id)

# Get earliest publication dates for each fossil pollen site.
mexico_pub_date <- mexico_pub %>%
  purrr::map_dfr(.id = "dataset.id",
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 }) %>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                dataset.id = as.integer(dataset.id)) %>% 
  dplyr::group_by(dataset.id) %>% 
  dplyr::slice(which.min(pub_year))

mexico_pub_date$dataset.id <- as.integer(mexico_pub_date$dataset.id)

# Get the fossil datasets in North America and use purrr::map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble. We remove dataset.id because dplyr::left_join cannot process having multiple rows with the same dataset.id.
mexicofossil_counts <- mexico_data %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  purrr::imap(.f = function(x,y){
    cbind(mexico_data[[y]]$sample.meta %>%
            dplyr::select(dataset.id, sample.id),
          x)
  }) %>%
  bind_rows() %>% 
  # select(-ends_with("dataset.id")) %>%
  mutate_all(funs(ifelse(is.na(.), 0, .))) %>%
  as_tibble()

# Sort the taxa to be in alphabetical order. 
mexicofossil_counts <- mexicofossil_counts[,c(names(mexicofossil_counts)[1:2], sort(names(mexicofossil_counts)[3:ncol(mexicofossil_counts)]))]

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge (using a left_join) the metadata with the publication year, using the dataset.id to match, just to double check.
mexico_metadata_pub <- dplyr::left_join(mexicoFossil_metadata,
                                          mexico_pub_date,
                                          by  = "dataset.id")

# The fossil counts are just bound to the metadata_pub because it is not possible to do a left join when multiple rows have the same dataset.id.
mexico_metadata_counts <- dplyr::left_join(mexico_metadata_pub, mexicofossil_counts,
                                             by = c("dataset.id" = "dataset.id","sample.id")) %>%
  dplyr::arrange(dataset.id)

# Extract the one site that is along the border to add to modern calibrated dataset.
CTmexico_metadata_counts <- mexico_metadata_counts %>% 
  dplyr::filter(sample.id == 214610)

```

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Currently, to target the core top data from the fossil pollen (or "pollen") data, I am using a service online ( https://tilia.neotomadb.org/retrieve/doc2/), which allows for me to target the core tops through the SKOPE_GetSurfaceSampleData method (use selection #3 for core-top samples; 7 for surface samples). Then, I copied the json file and used an online service to convert the json file to a csv, which I am reading in here.
coreTops <- read.csv("./data/surface_coretop_Minmeta.csv") %>% 
  dplyr::distinct()

# 
CT_metadata_counts <- dplyr::left_join(coreTops,
                                       NAfossil_metadata_counts,
                                       by  = c("dataset.id", "site.id", "sample.id")) %>% 
  dplyr::bind_rows(CTmexico_metadata_counts) %>% 
  dplyr::mutate(type = "core top") %>% 
  dplyr::arrange(dataset.id)

```

The Neotoma Modern Pollen Database contains `r nrow(MP_counts)` samples, representing `r ncol(MP_counts)` different pollen taxa.

## Combine Core Top and Modern Pollen Data

```{r Combine_CT_MP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# This function is no longer needed. It looks like rbind.fill from plyr already does this.

# Function to combine the core top and surface sample data, since one dataset may have more columns than the other, which would happen if not all species were represented in one dataset. Therefore, here we make sure that all unique columns are represented in the combined dataframe.
# rbind.all.columns <- function(x, y) {
#   x.diff <- setdiff(colnames(x), colnames(y))
#   y.diff <- setdiff(colnames(y), colnames(x))
# 
#   x[, c(as.character(y.diff))] <- 0
#   y[, c(as.character(x.diff))] <- 0
# 
#   return(rbind(x, y))
# }

# Now the two dataframes can be combined using the function, then sort the rows by dataset.id. The distinct function is used to make sure there is no duplicate data from combining the two dataframes. Then, we convert all NA values to zero.
MPCT_metadata_counts <- dplyr::bind_rows(MP_metadata_counts, CT_metadata_counts) %>%
  dplyr::arrange(dataset.id) # %>%
  #distinct(dataset.id, .keep_all = TRUE)

# Splitting these apart so that all of the NAs for the taxa counts can be changed to zero. Then combine back together.
MPCT_metadata <- MPCT_metadata_counts[,1:11]
MPCT_counts <- MPCT_metadata_counts[,c(1:3, 12:ncol(MPCT_metadata_counts))] %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .)))

MPCT_metadata_counts <- dplyr::left_join(MPCT_metadata, MPCT_counts, by = c("dataset.id", "site.id", "sample.id"))

# Sort the taxa to be in alphabetical order.
MPCT_metadata_counts <- MPCT_metadata_counts[,c(names(MPCT_metadata_counts)[1:11], sort(names(MPCT_metadata_counts)[12:ncol(MPCT_metadata_counts)]))]

```

## Remove Coretops data from the fossil pollen dataset
```{r removeCT, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

NAfossil_metadata_counts <-  anti_join(NAfossil_metadata_counts, coreTops, by = c("dataset.id", "site.id", "sample.id"))

```

## Map Modern Pollen Sample Locations

To examine locations of the modern pollen data, we can use `leaflet` to plot the modern pollen locations.

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

black.point <- makeIcon(iconUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/BlackDot.svg/2000px-BlackDot.svg.png", iconWidth = 10, iconHeight = 10)

map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -101.05,
          lat = 11.68,
          zoom = 2)

map %>% addMarkers(lng = MPCT_metadata_counts$long, lat = MPCT_metadata_counts$lat, icon = black.point, 
                   popup = paste0('<b>', as.character(MPCT_metadata_counts$site.name), '</b><br>',
                                  '<a href=http://apps.neotomadb.org/explorer/?datasetid=', MPCT_metadata_counts$dataset.id, '>Neotoma Database</a>'))



```

# Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).  

```{r load_Prsim, echo = FALSE, warning=FALSE}
# Load Prism Climate data.

# Temporarily bypassing the core tops code and setting the surface pollen samples equal to the other variable. Once the core top data is added in, then I should delete this line of code.
# MPCT_metadata_counts <- MP_metadata_counts

# Get Modern Pollen locational dataset.id numbers.
names <- MPCT_metadata_counts$sample.id

#Extract longitude and latitude data.
lons <- unlist(lapply(all,FUN=function(x){MPCT_metadata_counts$long}))
lats <- unlist(lapply(all,FUN=function(x){MPCT_metadata_counts$lat}))

# Bind longitude and latitude and convert to SpatialPoints.
coords <- cbind(LONGITUDE=lons,LATITUDE=lats)
coordsPoints <- SpatialPoints(coords, proj4string=CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Create a SpatialPointsDataFrame of the samples and clean up locations.
# points <- SpatialPointsDataFrame(coords,data.frame(NAME=names,MP_counts), 
#                                  proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))
# locations <- points[!duplicated(coordinates(points)),]


### Script for PRISM climate extraction adapted from R. Kyle Bocinsky, Johnathan Rush, Keith W. Kintigh, and Timothy A. Kohler, 2016. Exploration and Exploitation in the Macrohistory of the Prehispanic Pueblo Southwest. Science Advances.

## This script extracts data from the ~800 m monthly PRISM dataset for the modern pollen locations. It first defines the extent, chops that extent into 800 m (30 arc-second) PRISM cells, and subdivides the extent into 14,440 (120x120) cell chunks for computation. (the chunks are 1x1 degree). These chunks are saved for later computation.

## Set the working directory to the directory of this file!
#setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat")

# Create an output directory
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

# This MUST point at an original LT81 dataset available from the PRISM climate group (http://www.prism.oregonstate.edu).
PRISM800.DIR <- "/Volumes/DATA/PRISM/LT81_800M/"

# Specify a directory for extraction
EXTRACTION.DIR <- list.files(paste0(PRISM800.DIR), recursive=TRUE, full.names=T)

# The climate parameters to be extracted
types <- c("ppt", "tmin","tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# (Down)Load the states shapefile form the National Atlas
# if(!dir.exists("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statep010")){
#   dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/", showWarnings = F, recursive = T)
#   download.file("https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/statesp010g.shp_nt00938.tar.gz", destfile="/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", mode='wb')
#   untar("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", exdir="/Volumes/DATA/NATIONAL_ATLAS/statesp010g")
# }

# (Down)Load the states shapefile form the National Atlas
if(!file.exists("./data/raw_data/statesp010g.shp_nt00938.tar.gz")){
  download.file("https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/statesp010g.shp_nt00938.tar.gz",
              destfile = "./data/raw_data/statesp010g.shp_nt00938.tar.gz",
              mode='wb')
}
untar("./data/raw_data/statesp010g.shp_nt00938.tar.gz",
      exdir="./data/raw_data/statesp010g")


states <- readOGR("./data/raw_data/statesp010g/statesp010g.shp", layer='statesp010g')

# Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
states <- sp::spTransform(states, sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Get the extent (i.e., the continental United States)
extent.states <- raster::extent(states)

# Floor the minimums, ceiling the maximums.
extent.states@xmin <- floor(extent.states@xmin)
extent.states@ymin <- floor(extent.states@ymin)
extent.states@xmax <- ceiling(extent.states@xmax)
extent.states@ymax <- ceiling(extent.states@ymax)

# Get list of all file names in the prism directory.
monthly.files <- EXTRACTION.DIR

# Trim to only file names that are rasters.
monthly.files <- grep("*\\.bil$", monthly.files, value=TRUE)
monthly.files <- grep("spqc", monthly.files, value=TRUE, invert=T)
monthly.files <- grep("/cai", monthly.files, value=TRUE)

# Generate the raster stack.
type.list <- raster::stack(monthly.files,native=F,quick=T)

#END Bocinsky et al. 2016 adapted script.

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/climate.points.Rds")){
  raster::extract(x = type.list, y = coordsPoints, df = TRUE) %>%
    readr::write_rds("./data/raw_data/climate.points.Rds")
}
climate.points <- readr::read_rds("./data/raw_data/climate.points.Rds")

# Replace the first column of IDs with sample.id. This will make it easier to match dataframes. This helps to check the data and not assume that the rows climate data are in the same order as the pollen data.
climate.points$ID <- names

# Rename the ID column to sample.id.
climate.points <- dplyr::rename(climate.points, sample.id = ID)

# Convert climate data from wide to long format, but keep ID so that we can remember the location that each record came from. Then, we use the separate function to split apart the long prism file names, of which some of the new columns we will not need; therefore, they are labeled as garbage, and these get deleted. Finally, we separate the yearmonth into two columns. This will allow for these to be grouped together to do summarizing below. sep = 4 is the location in the string of six numbers (yearmonth) to separate.
data_long <- melt(climate.points,
                  # sample.id variables - all the variables to keep but not split apart.
                  id.vars="sample.id") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "variable", "garbage2", "garbage3", "garbage4", "yearmonth"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage")) %>% 
  separate(col = "yearmonth", into = c("year", "month"), sep = 4) 


# Now use the long format data to create an additional variable, called Growing Degree Days (GDD). A function has been created here to convert the tmin and tmax data into GDD, using the data_long dataframe.

# Calculate GDD monthly. This is an adapted function from Bocinsky et al. 2016 (calc_gdd_monthly).

calc_gdd_monthly <- function(temp, t.base, t.cap=NULL, multiplier=1, to_fahrenheit=T, to_round=F){
  if(nrow(dplyr::filter(temp, variable == "tmin"))!=nrow(dplyr::filter(temp, variable == "tmax"))){
    stop("tmin and tmax must have same number of observations!")
  }
  
  tmin <- dplyr::filter(temp, variable == "tmin")
  tmax <- dplyr::filter(temp, variable == "tmax")
  
  t.base <- t.base*multiplier
  if(!is.null(t.cap)){
    t.cap <- t.cap*multiplier
  }
  
    # Floor tmax and tmin at Tbase
    tmin["value"] <- lapply(tmin["value"], function(x) { x[x<t.base] <- t.base; return(x) })
    tmax["value"] <- lapply(tmax["value"], function(x) { x[x<t.base] <- t.base; return(x) })
    
    # Cap tmax and tmin at Tut
    if(!is.null(t.cap)){
      tmin["value"] <- lapply(tmin["value"], function(x) { x[x>t.cap] <- t.cap; return(x) })
      tmax["value"] <- lapply(tmax["value"], function(x) { x[x>t.cap] <- t.cap; return(x) })
    }
    
    temp_long <- left_join(tmin, tmax, by = c("sample.id", "year", "month")) %>% 
      dplyr::select(-starts_with("variable.")) %>% 
      plyr::rename(c("value.x" = "tmin", "value.y" = "tmax"))
    
    temp_long$GDD <- (((temp_long$tmin + temp_long$tmax) / 2) - t.base)
    temp_long <- temp_long %>% dplyr::select(-starts_with("tm"))
    
    # Combine month and year column.
    temp_long$Date <- zoo::as.yearmon(paste(temp_long$year, temp_long$month), "%Y %m")
    
    # Multiply by days per month, and convert to Fahrenheit GDD
    temp_long$GDD <- temp_long$GDD * Hmisc::monthDays(temp_long$Date) / multiplier
    
    if(to_fahrenheit){
      temp_long$GDD <- temp_long$GDD * 1.8
    }
    
    if(to_round){
    temp_long$GDD <- round(temp_long$GDD)
    }
    
    temp_long <- temp_long %>% 
      dplyr::select(-starts_with("Date")) %>% 
      dplyr::mutate(variable = "GDD") %>% 
      dplyr::rename(value = GDD) %>% 
      dplyr::select(sample.id, variable, year, month, value)
  
  return(temp_long)
}

temp_long <- calc_gdd_monthly(data_long, t.base = 10, t.cap=30, multiplier=1, to_fahrenheit=F, to_round=F)

# Now rbind the GDD data to the other 3 climate variables (tmin, tmax, and ppt) to create one dataframe with all of the climate variables.
data_long <- rbind(data_long, temp_long)


# Need to filter to 30 years prior from the publication date of each pollen surface sample site.
# Possibly loop through for each site, determine the publication date, then take the previous 30 years of PRISM data and do avgs

# for (i in 1:length(MPCT_metadata_counts$sample.id)) {
# 
#   
#   
# }

# Eventually, this will be replaced to represent the 30 years prior to publication or collection date, but for now we use 1961 to 1990.
data_30yr <- data_long %>% 
  dplyr::filter(year >= 1961 & year <= 1990)

# Take the long format data, then group by ID, which is a unique location, the variable (i.e., precipitiation, min. temp., max temp., and GDD), and each month (e.g., 01 for January, 02 for February, etc.). So, we essentially get a 30 year average for each of the 12 months for the 3 variables at each unique location.
data_avgs <- data_30yr %>%
  dplyr::group_by(sample.id, variable, month) %>% 
  dplyr::summarize(mean = mean(value))

# The precipitation averages are extracted into its own dataframe.
ppt_avgs <- data_avgs %>% 
  dplyr::filter(variable == "ppt")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
tmp_avgs <- data_avgs %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(sample.id, month) %>% 
  dplyr::summarize(mean = mean(mean)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(sample.id, variable, month, mean)

# The GDD averages are extracted into its own dataframe.
gdd_avgs <- data_avgs %>% 
  dplyr::filter(variable == "GDD")

# Now, combine the precipitation, temperature, and GDD data back into one dataframe, and convert from long to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. Then, rename all the column names. 
clim_wide <- rbind(gdd_avgs, ppt_avgs, tmp_avgs) %>% 
  dcast(sample.id ~ variable + month, value.var="mean") %>% 
  #dplyr::select(-one_of("dataset.id")) %>% 
  set_colnames(c('sample.id', 'gjan', 'gfeb', 'gmar', 'gapr', 'gmay', 'gjun', 'gjul', 'gaug', 'gsep', 'goct', 'gnov', 'gdec', 'pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))


```

# Load WorldClim Climate Data

[WorldClim 2.0](http://worldclim.org/version2) has 31 years (1970-2000) of averaged climate data and here we use a resolution of 30 seconds (~1 km^2^).

```{r load_WordClim, echo = FALSE, warning=FALSE}
# Load WorldClim Climate data.

# ## Set the working directory to the directory of this file!
# setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/vignettes")
# 
# # Create an output directory
# dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

WC.DIR <- "/Volumes/DATA/WORLDCLIM_2.0/"

if(!file.exists("../data/raw_data/wc2.0_30s_bio.zip"))
  download.file("http://data.biogeo.ucdavis.edu/data/worldclim/v2.0/tif/base/wc2.0_30s_bio.zip",
                destfile = "../data/raw_data/wc2.0_30s_bio.zip")

# Specify a directory for extraction
# EXTRACTION.DIR <- "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data"

# The climate parameters to be extracted
types <- c("ppt", "tmin", "tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
# dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# Get list of all file names in the prism directory.
wc.files <- list.files(paste0(WC.DIR), recursive=T, full.names=T)

wc.files <- grep("*\\.tif$", wc.files, value=TRUE)

# Generate the raster stack.
wc.list <- raster::stack(wc.files,native=F,quick=T)

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
WCclimate.points <- raster::extract(x = wc.list, y = coordsPoints, df = TRUE)

# Replace the first column of IDs with dataset.id. This will make it easier to match dataframes. This helps to check the data and not assume that the rows climate data are in the same order as the pollen data.
WCclimate.points$ID <- names

# Rename first column to dataset.id.
colnames(WCclimate.points)[1] <- "dataset.id"

WCdata_long <- melt(WCclimate.points,
                    # ID variables - all the variables to keep but not split apart.
                    id.vars="dataset.id") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "garbage2", "variable", "month"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage"))

# The precipitation averages are extracted into its own dataframe.
WCppt_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "prec")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
WCtmp_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(dataset.id, month) %>% 
  dplyr::summarize(value = mean(value)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(dataset.id, variable, month, value)

# Now, combine the precipitation and temperature data back into one dataframe, and convert from long back to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. The, rename all the column names. 
WCclim_wide <- bind_rows(WCppt_avgs, WCtmp_avgs) %>% 
  dcast(dataset.id ~ variable + month, value.var="value") %>% 
  #dplyr::select(-one_of("dataset.id")) %>% 
  set_colnames(c('dataset.id', 'pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))

```

## Clean up datasets to remove NAs for PRISM data

Need to further clean up the datasets to remove any rows with NA values. Although currently working with United States data, we do not currently have climate (prism) data for Alaska, which is the reason for all rows with NAs.

```{r removePrismNAs, echo=FALSE, warning=FALSE}

# Get row number in first column that have NA values.
NAindex <- which(is.na(clim_wide[,2]), arr.ind=FALSE)

# Remove rows with NA values for the precipitation, temperature, and GDD using the index.
Clim_noNAs <- clim_wide[-(NAindex),]
rownames(Clim_noNAs) <- seq(length=nrow(Clim_noNAs))

# Remove rows in Moden Pollen dataset, using the same index. This will keep the sites/locations still matched up between the climate data and the modern pollen counts.
MPCT_metadata_counts_noNAs <- MPCT_metadata_counts[-(NAindex),]

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- MPCT_metadata_counts_noNAs[,-(1:11)] %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .)))

```

Modern Pollen data contains `r nrow(MPCT_counts_noNAs)` samples, representing `r ncol(MPCT_counts_noNAs)` different pollen taxa. 

### Checking the calibration data set

To test the model, I used the `palaeoSig` package's `randomTF()` function, which tests the models against randomly sorted data. If we do get a significance for a climate variable, then we know that the model reconstruction is better than random. The model takes the proportion of variance accounted for by the actual data, and then compares it to the proportion of variance accounted for by the randomized data. Here, we use a wrapper function for [randomTF](https://github.com/NeotomaDB/Workbooks/blob/master/PollenClimate/R/sig_test.R).

The MAT method is testing whether the modern calibration is able to detect signals in each of the climate parameters. The example here uses the entire North American Modern Pollen Database, rather than a targeted data subset.

#### WA - Monotone Deshrinking

```{r WA_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# This is a wrapper for the randomTF function.
source('../R/sig_test.R')

wa_sig <- run_tf(pollen = MPCT_counts_noNAs, 
                 climate = Clim_noNAs[,14:25], 
                 func = WA, col = 1 , mono = TRUE)

wa_sig[[1]]
```

The columns are standard weighted average. One positive of using monotone shrinking is that it does not suffer from spatial-autocorrelation like with using MAT. Therefore, there has been quite a bit of criticism against MAT. You can do a spatial variogram of the model, then test to see if it is significant. MAT gives you a root mean error that is much higher than what it probably really is. Sometimes it is just because the cores are so close to one another, more so than the assemblages reflect climate. Consider weighted averaging (partially squared not really worth it. ) Weighted averaging with the monotone shrinking is very fast.

#### MAT - ten closest

```{r MAT_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- MPCT_metadata_counts_noNAs[,-(1:9)] %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .)))

# For some reason this fails consistently when we try to use the weighted MAT (col=2).
mat_sig <- run_tf(pollen = MPCT_counts_noNAs, 
                  climate = Clim_noNAs[,14:25],
                  func = MAT, col = 1, k = 10)
#col 1 is just the closest analogue.
mat_sig[[1]]

```

The variance explained is very low for some variables, although the p value is very low. This highlights that in some cases you may not just want to rely on the p value. This also shows the issue with temporal autocorrelation on temperature and, to some extent also, on precipitation variables.

# Reconstruction Statistics

## Reconstruction Significance

Now we can do a similar test on fossil assemblage reconstructions. This again uses the same `randomTF()` function, but the variance explained and significance will probably change given that we are using a slightly more constrained dataset.


### Process Four Corners Fossil Pollen Data.

```{r FourCorners_FP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Determine extents for the Four Corner states. Will want to make this a changeable parameter for web app.
FourCorners <- states[states$NAME %in% c("Arizona", "Colorado", "New Mexico", "Utah"),]

# Get the extent (i.e., the Four Corner states)
extent.FourCorners <- raster::extent(FourCorners)

# Subset fossil dataset. Put in 38 for ymax so that this is the northern extent of the Upland United States Southwest. Here we limit the NA fossil dataset to the UUSS and for the past 5000 years.
UUSS_sitesOLDER <- subset(NAfossil_metadata_counts, long >= extent.FourCorners@xmin & long <= extent.FourCorners@xmax & lat >= extent.FourCorners@ymin & lat <= 38 & age <= 5000)

# Temporarily removing sites that only have one row of data, since the reconstruction won't run because it gets coerced to 1 dimension.
#UUSS_sites <- subset(UUSS_sites, !(UUSS_sites$dataset.id==1717 | UUSS_sites$dataset.id==3608 | UUSS_sites$dataset.id==3611 | UUSS_sites$dataset.id==15106 | UUSS_sites$dataset.id==15298 | UUSS_sites$dataset.id==15368))

# Alphabetize the taxa. Double-checking.
UUSS_sites <- UUSS_sitesOLDER[ , c(names(UUSS_sitesOLDER)[1:11], sort(names(UUSS_sitesOLDER)[12:ncol(UUSS_sitesOLDER)]))]

```

This returns `r length(unique(UUSS_sites$dataset.id))` sites. The `neotoma` package provides plotting capabilities, but `leaflet` allows for more interactive plotting.

### Map the fossil pollen sites for the Four Corners.

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

library(leaflet)

black.point <- makeIcon(iconUrl = "https://upload.wikimedia.org/wikipedia/commons/thumb/1/11/BlackDot.svg/2000px-BlackDot.svg.png", iconWidth = 10, iconHeight = 10)

# Here, we set up how we want our map to look like.
map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -109.05,
          lat = 29.68,
          zoom = 5)

# If you wanted a simple map, then you would just need lat and long. However, I have made 
# some additions, so that you can click on each site and get the site name and can also be 
# redirected to Neotoma webpage to get more information about each site.
map %>% addMarkers(lng = UUSS_sites$long, 
                   lat = UUSS_sites$lat, 
                   icon = black.point,
                   popup = paste0('<b>', 
                                  as.character(UUSS_sites$site.name), 
                                  '</b><br>',
                                  '<a href=http://apps.neotomadb.org/explorer/?datasetid=', 
                                  UUSS_sites$dataset.id, 
                                  '>Neotoma Database</a>')) %>% 
  # This is to show the Bocinsky et al. 2016 study area.
  addRectangles(-113, 32, -105, 38, 
                fillColor = "transparent", 
                color = "grey", 
                weight = 3)

```

Now, we apply a reconstruction to a real dataset. 

#### WA - Monotone Deshrinking

```{r WA_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# These are for the one site reconstruction.
# wa_reconst <- run_tf2(pollen = MP_counts_noNAs, fossil = fossil_data,
#        climate = Clim_noNAs[,13:24],
#        func = WA, col = 1, mono = TRUE)
#  
# wa_reconst[[1]]

# for (i in 1:length(fossil_sites)) {
#   nam <- paste("wa_reconst_", fossil_sites[[i]], sep = "")
#   dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
#                  climate = Clim_noNAs[,13:24],
#                  func = WA, col = 1, mono = TRUE)
#   assign(nam, dat)
# }

```

#### MAT - ten closest

```{r MAT_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

MP_counts_noNAs <- MP_counts %>% 
  mutate_all(funs(ifelse(is.na(.), 0, .)))

mat_reconst <- run_tf2(MP_counts_noNAs, fossil = fossil_data, 
        climate = Clim_noNAs[,13:24], 
        func = MAT, col = 2, k = 10)
 
 mat_reconst[[1]]

# for (i in 1:length(fossil_sites)) {
#   nam <- paste("mat_reconst_", fossil_sites[[i]], sep = "")
#   dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
#                  climate = Temperature,
#                  func = mat, col = 2, k = 10)
#   assign(nam, dat)
# }

```

Again, no significance for the model. This indicates that we are just not able to see a signal within the data, but this is probably due to the calibration dataset being too broad.

## Reconstruction

Once we have validated the methods, we re-run the analyses using the two methods, MAT and WA.

### Model Summary and saving values to file.

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# Create a list of all the fossil pollen sites to be reconstructed for the UUSS. 
fossil_sites <- unique(UUSS_sites$dataset.id)

# Subset the climate data to just Temperature. 
Temperature <- Clim_noNAs[,26:37]

# Now, we carry out the reconstruction for all fossil pollen sites in the UUSS. The reconstructions are in one output file, which has the reconstructed temperatures and associated error.

# First, to save time, check to see if the sites have already been reconstructed, if so then this chunk will not run. If you would like to re-run the reconstructions, then simply delete the reconstruction csv and it will run. 

if (!file.exists("./output/UUSS_reconst_temp_7analog.csv")) {
  
  mat_models_temp_7analog <- purrr::map(1:12,
                              .f = function(j){
                                rioja::MAT(y = MPCT_counts_noNAs, 
                                           x = Temperature[,j], 
                                           k = 7, 
                                           dist.method="sq.chord",
                                           lean = FALSE)
                              })
  
  UUSS_reconst_7analog <- mat_models_temp_7analog %>%
  purrr::map_dfr(.id = "climate",
                 .f = function(the.model){
                   # Create the MAT reconstruction.
                   mat_reconst <- predict(the.model,
                                          newdata = UUSS_sites %>%
                                            dplyr::select(-dataset.id:-pub_year),
                                          sse = TRUE)
                   
                   # Put the reconstructed data into a dataframe.
                   UUSS_sites %>%
                     dplyr::select(dataset.id:pub_year) %>%
                     dplyr::mutate(model = "MAT") %>%
                     cbind(
                       tibble(reconst = mat_reconst$fit[,1],
                              err = mat_reconst$SEP[,1])
                     )
                 })


 write.csv(UUSS_reconst_7analog, "./output/UUSS_reconst_temp_7analog.csv", row.names = FALSE)
  
} else {
  
  stop("The UUSS Reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the UUSS directory.")
  
}


#---------------


# Do reconstruction for Growing Degree Days. 

# Subset the climate data to just GDD 
GDD_noNAs <- Clim_noNAs[,2:13]

if (!file.exists("./output/UUSS_reconst_GDD_7analog.csv")) {
  
  mat_models_gdd_7analog <- purrr::map(1:12,
                              .f = function(j){
                                rioja::MAT(y = MPCT_counts_noNAs, 
                                           x = GDD_noNAs[,j], 
                                           k = 7, 
                                           dist.method="sq.chord",
                                           lean = FALSE)
                              })
  
  UUSS_reconst_GDD_7analog <- mat_models_gdd_7analog %>%
  purrr::map_dfr(.id = "climate",
                 .f = function(the.model){
                   # Create the MAT reconstruction.
                   mat_reconst <- predict(the.model,
                                          newdata = UUSS_sites %>%
                                            dplyr::select(-dataset.id:-pub_year),
                                          sse = TRUE)
                   
                   # Put the reconstructed data into a dataframe.
                   UUSS_sites %>%
                     dplyr::select(dataset.id:pub_year) %>%
                     dplyr::mutate(model = "MAT") %>%
                     cbind(
                       tibble(reconst = mat_reconst$fit[,1],
                              err = mat_reconst$SEP[,1])
                     )
                 })


 write.csv(UUSS_reconst_GDD_7analog, "./output/UUSS_reconst_GDD_7analog.csv", row.names = FALSE)
  
} else {
  
  stop("The UUSS GDD Reconstruction is already completed. If you would like to re-run the reconstructions, then delete the file from the UUSS directory.")
  
}


# Now, we extract out the maize growing season months.

UUSS_reconst_GDD_7analog <- read.csv("./output/UUSS_reconst_GDD_7analog.csv")

UUSS_reconst_GDD_7analog_total <- UUSS_reconst_GDD_7analog %>% 
  dplyr::filter(climate >= 5 & climate <= 9) %>% 
  dplyr::group_by(dataset.id, sample.id, depth) %>% 
  dplyr::summarise_at(c("reconst", "err"), funs(sum, mean)) %>% 
  dplyr::select(dataset.id, sample.id, depth, reconst_sum, err_mean)

UUSS_reconst_GDD_7analog_meta <- UUSS_reconst_GDD_7analog %>% 
  dplyr::filter(climate == 5) %>%
  dplyr::select(-c(reconst, err, climate))

UUSS_GDD_7analog <- left_join(UUSS_reconst_GDD_7analog_meta, UUSS_reconst_GDD_7analog_total, by = c("dataset.id", "sample.id", "depth"))




#--------

# # Filter to one site to do the reconstruction.
#   site <- dplyr::filter(UUSS_sites, dataset.id == fossil_sites[1])
#   site <- site[,-(1:8)] %>%
#     mutate_all(funs(ifelse(is.na(.), 0, .)))
#   Changecols = c(1:ncol(site))
#   site[,Changecols] = apply(site[,Changecols], 2, function(x) as.numeric(as.integer(x)))
#   MPCT_counts_noNAs[,Changecols] = apply(MPCT_counts_noNAs[,Changecols], 2, function(x) as.numeric(as.integer(x)))
# 
# 
# 
# mat_reconst_sig <- purrr::map(1:length(UUSS_sites$dataset.id),
#                               .f = function(k){
#                               run_tf2(pollen = MPCT_counts_noNAs, fossil = site,
#                climate = Temperature[,1:12],
#                func = mat, col = 2, k = 10)
# })


# -----------------------------------------------------------------------------------------------
# Need to work on WA reconstruction
# Currently the rioja::WA function cannot run when species data have zero abundances. Therefore, here we determine what columns in the modern pollen data have zero column sums, and get an index of those columns, then delete the columns. The same index is used to delete the columns from the fossil pollen data.
# 
# dd <- unname(which((colSums(MPCT_counts_noNAs, na.rm=T) < 0.000001), arr.ind = TRUE))
# MPCT_counts_noNAs2 <- MPCT_counts_noNAs[,-(dd)]
# fossil_data2 <- fossil_data[,-(dd)]
# 
# 
# if ("wa_reconst.RDS" %in% list.files("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output")) {
#   wa_reconst <- readRDS(file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
# } else {
#   wa_reconst <-  predict(rioja::WA(y = tran(MPCT_counts_noNAs2, method = 'proportion'), 
#                                    x = Clim_noNAs[1,], lean = FALSE),
#                          newdata = fossil_data2, sse = TRUE)
#   saveRDS(wa_reconst, file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
# }

```

### Summary of the results. 

```{r summary_reconst, echo=FALSE}
UUSS_reconst_7analog <- read.csv("./output/UUSS_reconst_temp_7analog.csv")
#UUSS_reconst <- read.csv("./output/UUSS_reconst_temp.csv")

# First, convert the years BP in the age column to BC/AD dates, which we create a new column called date.

UUSS_reconst_7analog <- UUSS_reconst_7analog %>% 
  dplyr::mutate(date = BPtoBCAD(UUSS_reconst_7analog$age))

UUSS_GDD_7analog <- UUSS_GDD_7analog %>% 
  dplyr::mutate(date = BPtoBCAD(UUSS_GDD_7analog$age))

# Do 100 year average temperatures across all months.
# 200 yr interval: -3100,2100,200 and -3000, 2000, 200. For 100 yr interval: -3100,2100,100 and -3050, 1950, 100
# UUSS_100yrAvgs <- UUSS_reconst_7analog %>% 
#      dplyr::group_by(period=cut(date, breaks=seq(-3100,2100,100))) %>%
#      dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
#      dplyr::arrange(as.numeric(period)) %>% 
#      dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
#      dplyr::mutate(midP = seq(-3000, 2000, 200))
     
# Do 100 year average temperatures across growing season months (May through September).
UUSS_100yrAvgs_7analog_GS <- UUSS_reconst_7analog %>%
     dplyr::filter(climate >= 5 & climate <= 9) %>% 
     dplyr::group_by(period=cut(date, breaks=seq(-3100,2100,100))) %>% 
     dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
     dplyr::arrange(as.numeric(period)) %>% 
     dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
     dplyr::mutate(midP = seq(-3050, 1950, 100))

# UUSS_100yrAvgs_7analog_GS_BP <- UUSS_reconst_7analog %>%
#      dplyr::filter(site.name == "Beef Pasture") %>%
#      dplyr::filter(climate >= 5 & climate <= 9) %>% 
#      dplyr::group_by(period=cut(date, breaks=seq(-3100,2100,200))) %>% 
#      dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
#      dplyr::arrange(as.numeric(period)) %>% 
#      dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
#      dplyr::mutate(midP = seq(-3000, 2000, 200))

# Do 100 year average temperatures for the warmest month of the year, July.
UUSS_100yrAvgs_7analog_July <- UUSS_reconst_7analog %>% 
     dplyr::filter(climate == 7) %>% 
     dplyr::group_by(period=cut(date, breaks=seq(-3100,2100,100))) %>% 
     dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
     dplyr::arrange(as.numeric(period)) %>% 
     dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
     dplyr::mutate(midP = seq(-3050, 1950, 100))

# Do 100 year average temperatures for the coldest month of the year, January.
# UUSS_100yrAvgs_January <- UUSS_GDD_7analog %>% 
#      dplyr::filter(climate == 1) %>% 
#      dplyr::group_by(period=cut(date, breaks=seq(-100,2100,100))) %>% 
#      dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
#      dplyr::arrange(as.numeric(period)) %>% 
#      dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
#      dplyr::mutate(midP = seq(-50, 2050, 100))

# Calculate GDD for the growing season (May through September).
UUSS_100yrAvgs_GDD_7analog <- UUSS_GDD_7analog %>% 
     dplyr::group_by(period=cut(date, breaks=seq(-3100,2100,100))) %>% 
     dplyr::summarise_at(c("reconst_sum", "err_mean", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
     dplyr::arrange(as.numeric(period)) %>% 
     dplyr::select(-c(err_mean_n, elev_n, err_mean_unique_sites, elev_unique_sites)) %>% 
     dplyr::mutate(midP = seq(-3050, 1950, 100))

```


### Plotting the results. 

```{r plot_reconst, fig.width=8, fig.height=9, echo=FALSE}

library(plotly)

#UUSS_reconst <- read.csv("./output/UUSS_reconst.csv")

# Plot monthly temperatures for growing season months.

temp_UUSS_100yrAvgs_7analog_GS <- UUSS_100yrAvgs_7analog_GS
# temp_UUSS_100yrAvgs_7analog_GS[2,17] <- 1
# temp_UUSS_100yrAvgs_7analog_GS[21,7] <- 2000

UUSS_100yrAvgs_7analog_GS_plot <- ggplot(data=(dplyr::filter(temp_UUSS_100yrAvgs_7analog_GS[1:46, ])), aes(x=midP, y=reconst_mean, label = reconst_n, label2 = reconst_unique_sites)) + 
      #scale_color_gradient(low="blue", high="red") +
      geom_ribbon(aes(x=midP, ymin = reconst_mean - err_mean,
                      ymax = reconst_mean + err_mean,
                      alpha = 0.2),
                  fill = "grey",
                  colour="dark grey", 
                  show.legend = F ) +
      geom_line(colour="red", size = 3.5) +
      xlab("Year BC/AD") +
      ylab("Temperature °C") +
      scale_x_continuous(breaks=seq(-3100,2100,200), minor_breaks = seq(-3000, 2000, 200)) +
      scale_y_continuous(breaks=seq(10,19,1), limits = c(10, 19)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=18, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 18, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
      labs(col = "Temperature (°C)")
      
    # Save the plot for GS.
    ggsave(UUSS_100yrAvgs_7analog_GS_plot, file = "UUSS_100yrAvgs_7analog_GS_plot.tiff", path = "./output/UUSS/analog_7/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
    
    # Create html files that have the interactive plots from plotly.
    UUSS_100yrAvgs_7analog_GS_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_7analog_GS_plot, originalData = TRUE)
    f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_7analog_GS_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_7analog_GS_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))

     
temp_UUSS_100yrAvgs_7analog_July <- UUSS_100yrAvgs_7analog_July
# temp_UUSS_100yrAvgs_7analog_July[2,7] <- 0
# temp_UUSS_100yrAvgs_7analog_July[21,7] <- 2000

# Plot the warmest temperature month, July.
UUSS_100yrAvgs_7analog_July_plot <- ggplot(data=(dplyr::filter(temp_UUSS_100yrAvgs_7analog_July[1:46, ])), aes(x=midP, y=reconst_mean, label = reconst_n, label2 = reconst_unique_sites)) +
      #scale_color_gradient(low="blue", high="red") +
      geom_ribbon(aes(x=midP, ymin = reconst_mean - err_mean,
                      ymax = reconst_mean + err_mean,
                      alpha = 0.2),
                  fill = "grey",
                  colour="dark grey", 
                  show.legend = F ) +
      geom_line(colour="red", size = 3.5) +
      xlab("Year (BC/AD)") +
      ylab("Temperature (°C)") +
      scale_x_continuous(breaks=seq(-3100,2100,200), minor_breaks = seq(-3000, 2000, 200)) +
      scale_y_continuous(breaks=seq(12.5, 22.5, 1), limits = c(12.5, 22.5)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=18, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 18, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
      labs(col = "Temperature (°C)")
      
    # Save the plot for one month.
    ggsave(UUSS_100yrAvgs_7analog_July_plot, file = "UUSS_100yrAvgs_7analog_July_plot.tiff", path = "./output/UUSS/analog_7/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
    
    # Create html files that have the interactive plots from plotly.
    UUSS_100yrAvgs_7analog_July_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_7analog_July_plot, originalData = TRUE)
    f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_7analog_July_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_7analog_July_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))


# Plot the coldest temperature month, January.
# UUSS_100yrAvgs_January_plot <- ggplot(data=UUSS_100yrAvgs_January, aes(x=midP, y=reconst_mean, label = reconst_n, label2 = reconst_unique_sites)) +
#       #scale_color_gradient(low="blue", high="red") +
#       geom_ribbon(aes(x=midP, ymin = reconst_mean - err_mean,
#                       ymax = reconst_mean + err_mean,
#                       alpha = 0.2),
#                   fill = "grey",
#                   colour="dark grey", 
#                   show.legend = F) +
#       geom_line(colour="red", size = 3.5) +
#       xlab("Year (BC/AD)") +
#       ylab("Temperature (°C)") +
#       scale_x_continuous(breaks=seq(-100,2100,100), minor_breaks = seq(-50, 2050, 200)) +
#       scale_y_continuous(breaks=seq(-17,30,5), limits = c(-17, 30)) +
#       theme_bw() + 
#       theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=22, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
#       panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 22, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
#       labs(col = "Temperature (°C)")
#       
#     # Save the plot for one month.
#     ggsave(UUSS_100yrAvgs_January_plot, file = "UUSS_100yrAvgs_January_plot.tiff", path = "./output/UUSS/months/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
#     
#     # Create html files that have the interactive plots from plotly.
#     UUSS_100yrAvgs_January_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_January_plot)
#     f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_January_plot_ggplotly.html"
#      htmlwidgets::saveWidget(UUSS_100yrAvgs_January_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))
     
  
# Plot GDD
UUSS_100yrAvgs_GDD_plot <- ggplot(data=(dplyr::filter(UUSS_100yrAvgs_GDD_7analog[1:46, ])), aes(x=midP, y=reconst_sum_mean, color = reconst_sum_mean, label = reconst_sum_n, label2 = reconst_sum_unique_sites)) +
  geom_ribbon(aes(x=midP, ymin = reconst_sum_mean - err_mean_mean,
                      ymax = reconst_sum_mean + err_mean_mean,
                      alpha = 0.2),
                  fill = "grey",
                  colour="dark grey", 
                  show.legend = F) +
      geom_line(colour="black", size = 3.5) +
      xlab("Year (BC/AD)") +
      ylab("Growing Degree Days") +
      scale_x_continuous(breaks=seq(-3100,2100,200), minor_breaks = seq(-3000, 2000, 200)) +
      scale_y_continuous(breaks = seq(1150,1550,50)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text = element_text(size=15, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 15, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica"))
    
    # Save the plot for one month.
    ggsave(UUSS_100yrAvgs_GDD_plot, file = "UUSS_100yrAvgs_GDD_plot.tiff", path = "./output/UUSS/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
    
    # Create html files that have the interactive plots from plotly.
    UUSS_100yrAvgs_GDD_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_GDD_plot)
     f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_GDD_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_GDD_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))
     

# Plot the avg. elevation of the fossil pollen sites used in the reconstruction for each 100 year time bin. 
UUSS_100yrAvgs_Elevation_plot <- ggplot(data=(dplyr::filter(UUSS_100yrAvgs_GDD_7analog[1:46, ])), aes(x=midP, y=elev_mean)) +
      geom_line(size = 3.5) +
      xlab("Year (BC/AD)") +
      ylab("Elevation (m)") +
      scale_x_continuous(breaks=seq(-3100,2100,200), minor_breaks = seq(-3000, 2000, 200)) +
      scale_y_continuous(breaks=seq(2500,3200,100)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=18, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 20, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 20, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"))
      
    # Save the plot for one month.
    ggsave(UUSS_100yrAvgs_Elevation_plot, file = "UUSS_100yrAvgs_Elevation_plot.tiff", path = "./output/UUSS/", units="in", width=19, height=12, dpi=600, compression = 'lzw')


# ----------------------------------------------------------
     # # Loop to produce plots of each month for a site.
# 
#   UUSS_reconst <- read.csv("./output/UUSS_reconst.csv")
    
# for (i in 1:12) {  
#     
#     UUSS_100yrAvgs_month <- UUSS_reconst %>% 
#      dplyr::filter(climate == i) %>% 
#      dplyr::group_by(period=cut(date, breaks=seq(-100,2100,100))) %>% 
#      dplyr::summarise_at(c("reconst", "err", "elev"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
#      dplyr::arrange(as.numeric(period)) %>% 
#      dplyr::select(-c(err_n, elev_n, err_unique_sites, elev_unique_sites)) %>% 
#      dplyr::mutate(midP = seq(-50, 2050, 100))
#     
#     UUSS_100yrAvgs_month_plot <- ggplot(data=(dplyr::filter(UUSS_100yrAvgs_month)), aes(x=midP, y=reconst_mean, label = reconst_n, label2 = reconst_unique_sites)) +
#       geom_ribbon(aes(x=midP, ymin = reconst_mean - err_mean,
#                       ymax = reconst_mean + err_mean,
#                       alpha = 0.2),
#                   fill = "grey",
#                   colour="dark grey", 
#                   show.legend = F ) +
#       geom_line(colour="red", size = 3.5) +
#       xlab("Year (BC/AD)") +
#       ylab("Temperature (°C)") +
#       scale_x_continuous(breaks=seq(-100,2100,100), minor_breaks = seq(-50, 2050, 200)) +
#       scale_y_continuous(breaks=seq(-17,30,4), limits = c(-17, 30)) +
#       theme_bw() + 
#       theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=22, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
#       panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 22, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
#       labs(col = "Temperature (°C)")
#       
#     # Save the plot for one month.
#     ggsave(UUSS_100yrAvgs_month_plot, file = paste0("UUSS_100yrAvgs_month_"[i], sep = "", "_plot.tiff"), path = "./output/UUSS/months/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
#     
#     # Create html files that have the interactive plots from plotly.
#     UUSS_100yrAvgs_month_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_month_plot, originalData = TRUE)
#     f<-paste0("./output/UUSS/plotly_htmls/UUSS_100yrAvgs_month_"[i], sep = "", "_plot_ggplotly.html")
#      htmlwidgets::saveWidget(UUSS_100yrAvgs_month_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))
# }

```


# Regressions 

```{r regressions, echo=FALSE}


GDD_fit_7analog <- lm(reconst_sum_mean ~ elev_mean, data = UUSS_100yrAvgs_GDD_7analog)
tiff(file = "./Figures/GDD_elev_regression_7analog.tiff", height = 12, width = 15, units = 'in', 
     compression = "lzw", res = 600)
visreg(GDD_fit_7analog, xlab = "Elevation (m)", ylab = "Growing Degree Days", points=list(cex=.8))

dev.off()

summary(GDD_fit_7analog)

# Graph the residuals through time.
UUSS_100yrAvgs_GDD_7analog_res <- UUSS_100yrAvgs_GDD_7analog %>% 
  dplyr::mutate(residuals = GDD_fit_7analog$residuals)

# Plot GDD
temp_UUSS_100yrAvgs_GDD_7analog <- UUSS_100yrAvgs_GDD_7analog_res
# temp_UUSS_100yrAvgs_GDD_7analog[2,7] <- 0
# temp_UUSS_100yrAvgs_GDD_7analog[21,7] <- 2000

UUSS_100yrAvgs_GDD_7analog_Residuals_plot <- ggplot(data=(dplyr::filter(temp_UUSS_100yrAvgs_GDD_7analog[1:46, ])), aes(x=midP, y=residuals)) +
      geom_rect(aes(xmin=-3100,xmax=1500,ymin=-210,ymax=0,fill="blue"),alpha=0.009) +
      geom_rect(aes(xmin=-3100,xmax=1500,ymin=0,ymax=210,fill="red"),alpha=0.009) +
      scale_fill_identity() +
      geom_ribbon(aes(x=midP, ymin = residuals - sigma(GDD_fit_7analog),
                      ymax = residuals + sigma(GDD_fit_7analog),
                      alpha = 0.2),
                  fill = "grey",
                  colour="dark grey", 
                  show.legend = F) +
      geom_line(colour="black", size = 3.5) +
      geom_line(size = 3.5, color = "black") +
      xlab("Year BC/AD") +
      ylab("Growing Degree Days Anomaly (controlling for Elevation)") +
      scale_x_continuous(breaks=seq(-3100,2100,200), minor_breaks = seq(-3000, 2000, 200)) +
      scale_y_continuous(breaks = seq(-210,210, 30)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=16, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 20, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 20, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "grey 70"), axis.line = element_line(colour = "black"))
      
    # Save the plot for one month.
    ggsave(UUSS_100yrAvgs_GDD_7analog_Residuals_plot, file = "UUSS_100yrAvgs_GDD_7analog_Residuals_plot.tiff", path = "./output/UUSS/analog_7/", units="in", width=22, height=13, dpi=600, compression = 'lzw')
    
     UUSS_100yrAvgs_GDD_7analog_Residuals_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_GDD_7analog_Residuals_plot)
     f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_GDD_7analog_Residuals_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_GDD_7analog_Residuals_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))



# Do regression for temperature.
tempAll_fit <- lm(reconst_mean ~ elev_mean, data = UUSS_100yrAvgs_7analog_GS)
visreg(tempAll_fit, xlab = "Elevation (m)", ylab = "Temperature (°C)")
summary(tempAll_fit)

# tempJuly_fit <- lm(reconst_mean ~ elev_mean, data = UUSS_100yrAvgs_July)
# visreg(tempJuly_fit, xlab = "Elevation (m)", ylab = "Temperature (°C)")
# summary(tempJuly_fit)
# 
# 
# UUSS_temp_GDD <- UUSS_100yrAvgs_GS %>% 
#   dplyr::mutate(GDD = UUSS_100yrAvgs_GDD$reconst_sum_mean)
# 
# temp_GDD <- lm(GDD ~ reconst_mean, data = UUSS_temp_GDD)
# visreg(temp_GDD, ylab = "Growing Degree Days", xlab = "Temperature (°C)")
# summary(temp_GDD)


```


```{r UUSS_map2}

##updated 6/12/17 with new naming system of NHD classes (no longer NHD$NHD, now NHD$_)
##distill function does not work, but still seems to produce pdf with "dev.off"
#devtools::install_github("bocinsky/FedData")
library(FedData)
pkg_test("maptools")
pkg_test("maps")
pkg_test("RColorBrewer")
pkg_test("parallel")
pkg_test("entropy")
pkg_test("gtools")
pkg_test("UScensus2010")
pkg_test("xtable")
pkg_test("png")
pkg_test("TeachingDemos")
pkg_test("gdata")
pkg_test("matrixStats")
pkg_test("PBSmapping")
pkg_test("berryFunctions")
pkg_test("rgdal")

#setwd("/Volumes/VILLAGE/LAURA_FAUNA/Gini_Figures/")
#setwd("~/Desktop")
library(raster)
# Load some mapping functions Bocinsky wrote
junk <- lapply(list.files("./src", full.names=T),source)

fig.height <- 5.5 # inches
fig.width <- 4.5 # inches
buffer <- 0.2 # degrees
fig.asp <- fig.width/fig.height

master.proj <- CRS("+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs")


studyArea <- polygon_from_extent(extent(-114.8163 ,-102.0416,31.3319,42.00156),
                                 "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
east <- -114.8163
west <- -102.0416
north <- 42.00156
south <- 31.3319

# extent.states
# extent.FourCorners
# coordsPoints

#NED <- get_ned(template=studyArea, label="UUSS",raw.dir="/Volumes/DATA/NED", extraction.dir="/Volumes/DATA/NED/EXTRACTIONS/")

# Rivers, creeks, and washes
#NHD <- get_nhd(template=studyArea, label="UUSS",raw.dir="/Volumes/DATA/NHD/", extraction.dir="/Volumes/DATA/NHD/EXTRACTIONS/UUSS/")
NHD2 <- NHD
NHD$'_Flowline' <- NHD$'_Flowline'[NHD$'_Flowline'$GNIS_Nm %in% c("Colorado River",
                                                                  "Rio Grande",
                                                                  "Gila River", 
                                                                  "Salt River"),]

NHD$'_Flowline' <- rgeos::gLineMerge(NHD$'_Flowline')


# get 2500-m contour
# NED.2500 <- rasterToContour(NED,levels=2500)
# NED.2500 <- as(PolySet2SpatialPolygons(SpatialLines2PolySet(NED.2500)),"SpatialPolygons")
# NED.2500 <- unlist(lapply( NED.2500@polygons , slot , "Polygons" ))
# NED.2500 <- NED.2500[sapply(NED.2500,function(x){x@area >= .001})]
# 
# nCoords <- nrow(NED.2500[["3"]]@coords)
# NED.2500[["3"]]@coords <- rbind(NED.2500[["3"]]@coords[1:(nCoords-1),],c(west,north),NED.2500[["3"]]@coords[nCoords,])
# nCoords <- nrow(NED.2500[["27"]]@coords)
# NED.2500[["27"]]@coords <- rbind(NED.2500[["27"]]@coords[1:(nCoords-1),],c(east,north),NED.2500[["27"]]@coords[nCoords,])
# nCoords <- nrow(NED.2500[["41"]]@coords)
# NED.2500[["41"]]@coords <- rbind(NED.2500[["41"]]@coords[1:(nCoords-1),],c(east,north),NED.2500[["41"]]@coords[nCoords,])
# 
# NED.2500[["41"]]@hole <- TRUE
# NED.2500[["41"]]@ringDir <- as.integer(-1)
# NED.2500[["41"]]@coords <- NED.2500[["41"]]@coords[nrow(NED.2500[["41"]]@coords):1,]
# 
# NED.2500 <- SpatialPolygons(list(Polygons(NED.2500,"polys")),proj4string=CRS(projection(NED)))
# NED.2500 <- rgeos::gUnaryUnion(NED.2500)


# States
# states <- readOGR("/Volumes/DATA/NATIONAL_ATLAS/statep010/", layer='statep010')
# states <- states[states$STATE %in% c("Utah","New Mexico"),]
# states <- raster::crop(states,rgeos::gUnaryUnion(spTransform(studyArea,CRS(projection(states)))))

# FourCorners

# utah.state.ycoord <- ymin(extent(rgeos::gIntersection(spTransform(studyArea,CRS(projection(FourCorners))),FourCorners[FourCorners$NAME=="Utah",])))
# 
# NM.state.ycoord <- ymax(extent(rgeos::gIntersection(spTransform(studyArea,CRS(projection(FourCorners))),FourCorners[FourCorners$NAME=="New Mexico",])))
# 
# colorado.state.ycoord <- ymin(extent(rgeos::gIntersection(spTransform(studyArea,CRS(projection(FourCorners))),FourCorners[FourCorners$NAME=="Colorado",])))
# 
# arizona.state.ycoord <- ymax(extent(rgeos::gIntersection(spTransform(studyArea,CRS(projection(FourCorners))),FourCorners[FourCorners$NAME=="Arizona",])))



# Towns
towns <- readOGR("/Volumes/DATA/NATIONAL_ATLAS/citiesx020/", layer='citiesx020')
projection(towns) <- CRS(projection(FourCorners))
towns <- raster::crop(towns,rgeos::gUnaryUnion(spTransform(studyArea,CRS(projection(towns)))))
towns <- towns[towns$NAME %in% c("Denver", "Santa Fe", "Salt Lake City", "Phoenix"),]

pointsize <- 8

pdf(file='Scale.pdf', width=fig.width, height=fig.height, bg="white", pointsize=pointsize)
# quartz(width=fig.width, height=fig.height, bg="white", pointsize=pointsize)

# VEPIIN prcp
par(mai=c(0,0,0,0),
    oma=c(0,0,0,0),
    lend=2,
    ljoin=0,
    xpd=F)
plot.extent <- extent(studyArea)
plot(1, type='n', xlab="", ylab="", xlim=c(-114.8163,xmax(plot.extent)),ylim=c(ymin(plot.extent),ymax(plot.extent)), xaxs="i", yaxs="i", axes=FALSE, main='')


colors <- paste0(colorRampPalette(brewer.pal(9,"Greens"))(40),c("00","0D","1A","26","33","40","4D","59","66","73","80")[11])
plot(NED, col=colors, axes=F)

plot(studyArea, border='white', add=T)
#plot(NHD$Flowline, add=T)


# plot(sites, pch=19, add=T)
plot(states, lty=2, add=T)


plot(coordsPoints, pch=1, lwd=1, col = "black", add=T)


inch.x <- (xmax(plot.extent)-xmin(plot.extent))/(fig.width-par('mai')[2]-par('mai')[4])
inch.y <- (ymax(plot.extent)-ymin(plot.extent))/(fig.height-par('mai')[1]-par('mai')[3])


text(-109.4 +(0.05 * inch.x),37.0+(0.05 * inch.y),labels="Utah", adj=c(0,0), col="black", font=1, cex=1.25)
text(-109.4 +(0.05 * inch.x),37.0-(0.03 * inch.y),labels="Arizona", adj=c(0,1), col="black", font=1, cex=1.25)
text(-108.7+(0.05 * inch.x),37.0+(0.05 * inch.y),labels="Colorado", adj=c(1,0), col="black", font=1, cex=1.25)
text(-108.52-(0.05 * inch.x),37.0-(0.03 * inch.y),labels="New Mexico", adj=c(1,1), col="black", font=1, cex=1.25)



text(x=-109.3, y=37.18, labels="Colorado R.", font=1, cex=1, srt=-23)
text(x=-108.68, y=37.59, labels="Rio Grande", font=1, cex=1, srt=-23)
text(x=-106.1, y=35.55, labels="Salt R.", font=2, cex=1.25, srt=0)
text(x=-106.1, y=35.55, labels="Gila R.", font=2, cex=1.25, srt=0)

plot(towns, pch=22, bg='white', add=T)
text(towns,labels=towns$NAME, pos=c(4,2,1,3,4,4,1,4,4,4), font=2, cex=7/8)


scalebar(d=20, cex=0.075/(pointsize/100), font=2, xy = c(-107.5, 35.6), label="20 km", lwd=2, lend=1)
north.width <- 0.1
north.height <- 0.25
inch.xy <- c(0.1,0.15)
n.ratio <- 5/4
arrows(x0=xmax(plot.extent)-1.8*inch.xy[1]*inch.x,y0=ymin(plot.extent)+0.9*inch.xy[2]*inch.y,x1=xmax(plot.extent)-1.8*inch.xy[1]*inch.x,y1=ymin(plot.extent)+0.9*(inch.xy[2]+north.height)*inch.y, length=(north.width/2)/sin(pi/4), angle=45, lwd=n.ratio*north.width/(pointsize/100), lend=1)

text(labels="N",x=xmax(plot.extent)-0.18*inch.x,y=ymin(plot.extent)+0.12*inch.y,adj=c(0.5,0),cex=n.ratio*north.width/(pointsize/100), font=2)

dev.off()


# -------------------

```

```{r UUSS_map}

##### FIG_1.pdf #####
# This increases the number of vertices of the polygon for appropriate transformation
SWUS.extent <- extent.FourCorners
SWUS.poly <- polygon_from_extent(SWUS.extent,proj4string="+proj=longlat +datum=WGS84")
# SWUS.poly.lam <- SpatialPolygons(list(Polygons(list(Polygon(coordinates(suppressWarnings(spsample(as(SWUS.poly,"SpatialLines"),n=1000000,type="regular"))))),ID="1")),proj4string=CRS(projection(SWUS.poly)))
# SWUS.poly.lam <- spTransform(SWUS.poly.lam,CRS("+proj=lcc +lat_1=37 +lon_0=-109.045225"))

#Extract longitude and latitude data.
lons_FC <- unlist(lapply(all,FUN=function(x){UUSS_sites$long}))
lats_FC <- unlist(lapply(all,FUN=function(x){UUSS_sites$lat}))

# Bind longitude and latitude and convert to SpatialPoints.
coords_FC <- cbind(LONGITUDE=lons,LATITUDE=lats)
coordsPoints_FC <- SpatialPoints(coords_FC, proj4string=CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# (Down)Load the states shapefile form the National Atlas
# if(!dir.exists("../DATA/NATIONAL_ATLAS/statesp010g")){
#   dir.create("../DATA/NATIONAL_ATLAS/", showWarnings = F, recursive = T)
#   download.file("http://dds.cr.usgs.gov/pub/data/nationalatlas/statesp010g.shp_nt00938.tar.gz", destfile="../DATA/NATIONAL_ATLAS/statesp010g.shp_nt00938.tar.gz", mode='wb')
#   untar("../DATA/NATIONAL_ATLAS/statesp010g.shp_nt00938.tar.gz", exdir="../DATA/NATIONAL_ATLAS/statesp010g")
# }
# states <- readOGR("../DATA/NATIONAL_ATLAS/statesp010g", layer='statesp010g')
states <- states[states$NAME %in% c("Colorado","Utah","New Mexico","Arizona"),]
FOUR.extent <- extent(states)
states <- spTransform(states,"+proj=lcc +lat_1=37 +lon_0=-109.045225")

# Define the plot area as a polygon around the states
sim.poly <- polygon_from_extent(rgeos::gBuffer(states, width=20000, quadsegs=1000))
sim.extent <- extent(sim.poly)

# Define the aspect ratio of the plot, and its dimensions
plot.ratio <- (ymax(sim.extent)-ymin(sim.extent))/(xmax(sim.extent)-xmin(sim.extent))
fig.width <- 4.6
legend.height <- 0.0
plot.width <- fig.width
plot.height <- plot.width/plot.ratio
fig.height <- plot.height + legend.height

# define some colors for the niche percentage
colors <- paste0(colorRampPalette(brewer.pal(9,"Greens"))(11),c("00","0D","1A","26","33","40","4D","59","66","73","80")[11])
color.breaks <- seq(from=0,to=1,by=0.1)

# Create a png of the niche percentage
# if(!file.exists("./Figures/UUSS_elev.png")){
#   NICHE.PERCENT <- raster("/Volumes/DATA/NED/EXTRACTIONS/UUSS_NED_1.tif")
#   NICHE.PERCENT <- projectRaster(NICHE.PERCENT,crs=CRS("+proj=lcc +lat_1=37 +lon_0=-109.045225"))
#   NICHE.PERCENT <- mask(NICHE.PERCENT,states)
#   
#   quartz(file='./Figures/UUSS_elev.png', width=6*plot.ratio, height=6, antialias=FALSE, bg="transparent", type='png', family="Gulim", pointsize=8, dpi=300)
#   par(mai=c(0,0,0,0))
#   plot(1, type='n', xlab="", ylab="", xlim=c(xmin(sim.extent),xmax(sim.extent)), ylim=c(ymin(sim.extent),ymax(sim.extent)), xaxs="i", yaxs="i", axes=FALSE, main='')
#   plot(NICHE.PERCENT, zlim=c(0,1), maxpixels=ncell(NICHE.PERCENT), breaks=color.breaks, col=colors, useRaster=T, legend=FALSE,  xlab="", ylab="", axes=FALSE, main='', add=T)
#   dev.off()
# }
# NICHE.PERCENT.PNG <- readPNG('./Figures/UUSS_elev.png')

# Create a png for the elevation background
#The GTOPO30 data are available from
# if(!file.exists("./Figures/FOUR_Background2.png")){
# 
#   FOUR.GTOPO <- lapply(c("w100n40","w100n90","w140n40","w140n90"),function(f){
#     if(!file.exists(paste0("../DATA/GTOPO30/",f,".zip"))){
#       download.file(paste0("http://www.webgis.com/GTOPO30/",f,".zip"),destfile=paste0("../DATA/GTOPO30/",f,".zip"))
#       unzip(paste0("../DATA/GTOPO30/",f,".zip"), exdir="../DATA/GTOPO30/")
#     }
#     return(raster(paste0("../DATA/GTOPO30/",toupper(f),".DEM")))
#   })
# 
#   FOUR.GTOPO <- raster::crop(do.call(raster::merge,FOUR.GTOPO),FOUR.extent, snap='out')*2
#   # slope <- raster::terrain(FOUR.GTOPO, opt='slope')
#   # aspect <- raster::terrain(FOUR.GTOPO, opt='aspect')
#   # FOUR.GTOPO.hill <- raster::hillShade(slope, aspect, 40, 230)
# 
#   FOUR.GTOPO.hill <- projectRaster(FOUR.GTOPO,crs=CRS("+proj=lcc +lat_1=37 +lon_0=-109.045225"))
#   FOUR.GTOPO.hill <- raster::mask(FOUR.GTOPO.hill,states)
# 
#   quartz(file='./Figures/FOUR_Background2.png', width=6*plot.ratio, height=6, antialias=FALSE, bg="white", type='png', family="Gulim", pointsize=8, dpi=300)
#   par(mai=c(0,0,0,0))
#   plot(1, type='n', xlab="", ylab="", xlim=c(xmin(sim.extent),xmax(sim.extent)), ylim=c(ymin(sim.extent),ymax(sim.extent)), xaxs="i", yaxs="i", axes=FALSE, main='')
#   plot(FOUR.GTOPO.hill, maxpixels=ncell(FOUR.GTOPO.hill), col=colors, useRaster=T, legend=FALSE,  xlab="", ylab="", axes=FALSE, main='', add=T)
#   dev.off()
#   #rm(FOUR.GTOPO.hill); gc(); gc()
# }
FOUR.background <- readPNG('./Figures/FOUR_Background2.png')

#sites.sp.slim <- sites.sp[sites.sp$Outer_Date_AD %in% 500:1400 & !duplicated(sites.sp@coords),]


pdf(file='./Figures/UUSS_Map_Fossil_Locations3.pdf', width=fig.width, height=fig.height, bg="white", pointsize=8, version="1.7")
par(bg='white',fg='black',col.lab='black', col.main='black', col.axis='black', font=2, lend='round',ljoin='round')

par(mai=c(0,0,0,0), lend='round', ljoin='round', xpd=T)
plot(1, type='n', xlab="", ylab="",xaxs='i',yaxs='i', xlim=c(0,1), ylim=c(0,1), axes=FALSE, main='')

par(mai=c(legend.height,0,0,0), xpd=F, new=T)
plot(1, type='n', xlab="", ylab="",xlim=c(xmin(sim.extent),xmax(sim.extent)), ylim=c(ymin(sim.extent),ymax(sim.extent)), xaxs="i", yaxs="i", axes=FALSE, main='')
rasterImage(FOUR.background, xleft=xmin(sim.extent), xright=xmax(sim.extent), ybottom=ymin(sim.extent), ytop=ymax(sim.extent), interpolate=F)


points(spTransform(coordsPoints_FC,CRS("+proj=lcc +lat_1=37 +lon_0=-109.045225")), pch=19, cex=0.8)

plot(states, add=T)
#plot(SWUS.poly.lam, add=T, lty=3)

label.coords <- coordinates(states)
#text(label.coords, labels=states$NAME , cex=1)

#plot(NHD$Flowline, add=T)

# Towns
# towns <- readOGR("/Volumes/DATA/NATIONAL_ATLAS/citiesx020/", layer='citiesx020')
# projection(towns) <- CRS(projection(states))
# towns <- raster::crop(towns,rgeos::gUnaryUnion(spTransform(extent.FourCorners, towns)))
# towns <- towns[towns$NAME %in% c("Denver", "Santa Fe", "Salt Lake City", "Phoenix"),]
# towns <- towns[towns$FIPS %in% c("19017", "29137", "49035", "36075"),]
# plot(towns@coords, pch=17, bg='white', col = 'red')

#points(spTransform(towns@coords,CRS("+proj=lcc +lat_1=37 +lon_0=-109.045225")), pch=17, cex=0.8)
#text(towns,labels=towns$NAME, pos=c(4,2,1,3,4,4,1,4,4,4), font=2, cex=7/8)

inch.x <- (sim.extent@xmax-sim.extent@xmin)/(fig.width-par('mai')[2]-par('mai')[4])
inch.y <- (sim.extent@ymax-sim.extent@ymin)/(fig.height-par('mai')[1]-par('mai')[3])
scalebar.new(d=100000, lonlat=F, cex=1, font=2, side='right',lab.side='right', height=0.05*inch.y, label="100 km", line.offset=c(0.05*inch.x,0.05*inch.y), xy=c(xmin(sim.extent),ymin(sim.extent)), lwd=4, lend=1)

dev.off()
# distill('./Figures/UUSS_Map_Fossil_Locations.pdf')
# distill('../FIGURES/FIG_1_EDITED.pdf') ## Manually edited the map that appears in publication.


```

``` {r Map_reconstructions}

# First, we want to create one raster layer that has the modern PRISM data. This creates a modern temperature surface that is reflecting the variability across space, which are affected by various factors (e.g., elevation). 
# Use the rebus package to generate a regular expression for a number range, which will allow for the extraction of specific years in the prism data (i.e., 1961-1990).
rx <- rebus.numbers::number_range(1961, 1990)

# Now use the expression stored in rx, and trim list of files to to 1961:1990.
monthly.files.delta <- grep(rx, monthly.files, value = TRUE, perl = TRUE)

toMatch <- c("tmin", "tmax")
monthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        monthly.files.delta, value=TRUE))

toMatch <- c("05.bil", "06.bil", "07.bil", "08.bil", "09.bil")
monthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        monthly.files.delta, value=TRUE))

# Generate the raster stack.
type.list.delta <- raster::stack(monthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
type.list.delta <- raster::crop(type.list.delta, extent.FourCorners)

# Convert the raster stack to a brick. Don't need this. Cropping converts it into a brick.
#type.list.delta.brick <- raster::brick(type.list.delta)

# Read raster data into memory.
type.list.delta.memory <- readAll(type.list.delta)

# Get the average of the raster layers and get return one raster layer.
avg.delta <- type.list.delta.memory %>% mean()


# Step 2: We want to extract the modern average temperature conditions for the fossil pollen site locations. We want to limit the comparison between the modern temperature data and the fossil pollen reconstructions to the conditions that they are in. 

tempsites <- UUSS_sites %>% 
  dplyr::select(dataset.id, long, lat) %>% 
  distinct(dataset.id, long, lat)

tempsitesID <- tempsites %>% dplyr::select(long, lat)

# templons <- unlist(lapply(all,FUN=function(x){UUSS_sites$long}))
# templats <- unlist(lapply(all,FUN=function(x){UUSS_sites$lat}))
# 
# # Bind longitude and latitude and convert to SpatialPoints.
# tempcoords <- cbind(LONGITUDE=templons,LATITUDE=templats)
tempcoordsPoints <- SpatialPoints(tempsitesID, proj4string=CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Use the raster extract function to extract out the average temperature value at a fossil pollen location.
climate.points.delta <- raster::extract(x = avg.delta, y = tempcoordsPoints, df = TRUE)
climate.points.delta$ID <- tempsites$dataset.id

climate.points.delta.avg <- mean(climate.points.delta$layer)

# Step 3: Now, take the 100 year temperature averages of the Growing Season and subtract the modern mean temperature conditions at the fossil pollen sites. This gives an anomaly and how different the past was from the present. 
UUSS_100yrDelta <- UUSS_100yrAvgs_GS %>% 
  dplyr::mutate(anomaly = reconst_mean - climate.points.delta.avg)

UUSS_100yrDelta.list <- UUSS_100yrDelta$anomaly

# Step 4: Create 22 raster layers. Add anomaly from each 100 year time bin to the modern average temperature raster (i.e., avg.delta raster layer). 
RAST.DIR <- "./output/raster/"

for (i in 1:length(UUSS_100yrDelta.list)) {
  temprast14 <- avg.delta + UUSS_100yrDelta.list[14]
  writeRaster(temprast, file=paste0(RAST.DIR, "UUSS_delta_", 14, ".tif"), format="GTiff", options=c("COMPRESS=DEFLATE", "ZLEVEL=9", "INTERLEAVE=BAND"), overwrite=T, setStatistics=FALSE)
}

raster_files <- raster("./output/raster/UUSS_delta_1.tif") 


##### FIGURE for Delta #####

#temprast2 <- 
  
  plot(avg.delta + UUSS_100yrDelta.list[21])
plot(temprast14)
plot(states,add=TRUE)
temprast14Crop <- raster::mask(temprast14, states)

# (Down)Load the states shapefile form the National Atlas
# if(!dir.exists("../DATA/NATIONAL_ATLAS/statesp010g")){
#   dir.create("../DATA/NATIONAL_ATLAS/", showWarnings = F, recursive = T)
#   download.file("http://dds.cr.usgs.gov/pub/data/nationalatlas/statesp010g.shp_nt00938.tar.gz", destfile="../DATA/NATIONAL_ATLAS/statesp010g.shp_nt00938.tar.gz", mode='wb')
#   untar("../DATA/NATIONAL_ATLAS/statesp010g.shp_nt00938.tar.gz", exdir="../DATA/NATIONAL_ATLAS/statesp010g")
# }
states <- readOGR("../DATA/NATIONAL_ATLAS/statesp010g", layer='statesp010g')

states <- states[states$NAME %in% c("Colorado","Utah","New Mexico","Arizona"),]
FOUR.extent <- extent(states)
states <- spTransform(states,"+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")

# # Define the plot area as a polygon around the states
# sim.poly <- polygon_from_extent(rgeos::gBuffer(states, width=20000, quadsegs=1000))
# sim.extent <- extent(sim.poly)
#"+proj=lcc +lat_1=37 +lon_0=-109.045225"
#+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0

# Define the aspect ratio of the plot, and its dimensions
plot.ratio <- (ymax(FOUR.extent)-ymin(FOUR.extent))/(xmax(FOUR.extent)-xmin(FOUR.extent))
fig.width <- 4.6
legend.height <- 0.0
plot.width <- fig.width
plot.height <- plot.width/plot.ratio
fig.height <- plot.height + legend.height

# define some colors for temperature
#colors <- colorRampPalette(c("#1874CD" , "#CD2626")) (8)
#color.breaks <- seq(from=0,to=1,by=0.1)
# define some colors for the average temperatures
  colors <- colorRampPalette(rev(RColorBrewer::brewer.pal(11,"RdBu")))(35)
  # color.breaks <- seq(from = 0,
  #                     to = 35,
  #                     by = 1)

pdf(file='./Figures/Scale.pdf', width=fig.width, height=fig.height, bg="white", pointsize=8, version="1.7")
par(bg='white',fg='black',col.lab='black', col.main='black', col.axis='black', font=2, lend='round',ljoin='round')

par(mai=c(0.25, 0.25, 0.25, 0.25), lend='round', ljoin='round', xpd=T)
plot(1, type='n', xlab="", ylab="",xaxs='i',yaxs='i', xlim=c(5,6), ylim=c(0,1), axes=FALSE, main='')

# par(mai=c(0.25,0.25,0.25,0.25), xpd=F, new=T)
# plot(1, type='n', xlab="", ylab="",xlim=c(xmin(FOUR.extent),xmax(FOUR.extent)), ylim=c(ymin(FOUR.extent),ymax(FOUR.extent)), xaxs="i", yaxs="i", axes=FALSE, main='')
#rasterImage(temprast14, xleft=xmin(FOUR.extent), xright=xmax(FOUR.extent), ybottom=ymin(FOUR.extent), ytop=ymax(FOUR.extent), interpolate=F)
raster::plot(temprast14, useRaster=T, legend=F, legend.shrink=0.25, xlab="", ylab="", add=T, zlim=c(0,35), 
       col=colors)
#plot(temprast14, legend.only=TRUE, inset=c(-0.2,0), col=colors, legend.shrink=0.25, legend.args=list(text='Temperature (°C)', side=4, font=2, line=2.5, cex=0.6), par(mar=c(5, 4, 4, 2) + 0.1))

#plot(states, add=T)
#plot(SWUS.poly.lam, add=T, lty=3)

label.coords <- coordinates(states)
#text(label.coords, labels=states$NAME , cex=1)


inch.x <- (FOUR.extent@xmax-FOUR.extent@xmin)/(fig.width-par('mai')[2]-par('mai')[4])
inch.y <- (FOUR.extent@ymax-FOUR.extent@ymin)/(fig.height-par('mai')[1]-par('mai')[3])
scalebar.new(d=100000, lonlat=F, cex=1, font=2, side='right',lab.side='right', height=0.05*inch.y, label="100 km", line.offset=c(0.05*inch.x,0.05*inch.y), xy=c(xmin(FOUR.extent),ymin(FOUR.extent)), lwd=4, lend=1)

dev.off()



# Plot the anomaly for GS.
temp_UUSS_100yrDelta <- UUSS_100yrDelta
temp_UUSS_100yrDelta[2,7] <- 0
temp_UUSS_100yrDelta[21,7] <- 2000

UUSS_100yrAvgs_deltaAnomaly_plot <- ggplot(data=(dplyr::filter(temp_UUSS_100yrDelta[2:21, ])), aes(x=midP, y=anomaly, label = reconst_n, label2 = reconst_unique_sites)) +
      geom_line(colour="red", size = 3.5) +
      xlab("Year (BC/AD)") +
      ylab("Anomaly (°C)") +
      scale_x_continuous(breaks=seq(0,2000,100), minor_breaks = seq(50, 1950, 200)) +
      scale_y_continuous(breaks=seq(-1,5,.5), limits = c(-1, 5)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=22, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 28, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 28, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 22, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
      labs(col = "Temperature (°C)")
      
    # Save the plot for GS.
    ggsave(UUSS_100yrAvgs_deltaAnomaly_plot, file = "UUSS_100yrAvgs_deltaAnomaly_plot.tiff", path = "./output/UUSS/", units="in", width=21, height=13, dpi=600, compression = 'lzw')
    
    # Create html files that have the interactive plots from plotly.
    UUSS_100yrAvgs_deltaAnomaly_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_deltaAnomaly_plot, originalData = TRUE)
    f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_deltaAnomaly_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_GS_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))

```


``` {r MP_temp_data_FP}

rx <- rebus.numbers::number_range(1961, 1990)

# Now use the expression stored in rx, and trim list of files to to 1961:1990.
FP.monthly.files <- grep(rx, monthly.files, value = TRUE, perl = TRUE)

toMatch <- c("tmin", "tmax")
FP.monthly.files <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FP.monthly.files,native=F,quick=T)

FP.climate.points <- raster::extract(x = FP.type.list, y = tempcoordsPoints, df = TRUE)

FP.climate.points$ID <- tempsites$dataset.id

# Rename the ID column to sample.id.
FP.climate.points <- dplyr::rename(FP.climate.points, dataset.id = ID)

# Convert climate data from wide to long format, but keep ID so that we can remember the location that each record came from. Then, we use the separate function to split apart the long prism file names, of which some of the new columns we will not need; therefore, they are labeled as garbage, and these get deleted. Finally, we separate the yearmonth into two columns. This will allow for these to be grouped together to do summarizing below. sep = 4 is the location in the string of six numbers (yearmonth) to separate.
FPdata_long <- melt(FP.climate.points,
                  # dataset.id variables - all the variables to keep but not split apart.
                  id.vars="dataset.id") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "variable", "garbage2", "garbage3", "garbage4", "yearmonth"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage")) %>% 
  separate(col = "yearmonth", into = c("year", "month"), sep = 4)

# Take the long format data, then group by ID, which is a unique location, the variable (i.e., precipitiation, min. temp., max temp., and GDD), and each month (e.g., 01 for January, 02 for February, etc.). So, we essentially get a 30 year average for each of the 12 months for the 3 variables at each unique location.
FPdata_avgs <- FPdata_long %>%
  dplyr::group_by(dataset.id, month) %>% 
  dplyr::summarize(mean = mean(value))

FPdata_avgs <- dplyr::rename(FPdata_avgs, climate = month)

UUSS_reconst_7analog$climate <- as.integer(UUSS_reconst_7analog$climate)
FPdata_avgs$climate <- as.integer(FPdata_avgs$climate)

FPdata_avgs_monthly <- dplyr::left_join(UUSS_reconst_7analog, FPdata_avgs, by= c("dataset.id", "climate"))
FPdata_avgs_monthly <- FPdata_avgs_monthly %>% 
  dplyr::mutate(date = BPtoBCAD(FPdata_avgs_monthly$age))
FPdata_avgs_monthly <- FPdata_avgs_monthly %>% dplyr::mutate(anomaly = (reconst - mean))
write.csv(FPdata_avgs_monthly, "./output/FPdata_avgs_monthly.csv", row.names = FALSE)

FPdata_avgs_monthly <- read.csv("./output/FPdata_avgs_monthly.csv")

UUSS_100yrAvgs_GDD_7analog_anomaly <- FPdata_avgs_monthly %>% 
     dplyr::filter(climate >= 5 & climate <= 9) %>% 
     dplyr::group_by(period=cut(date, breaks=seq(-100,2100,100))) %>% 
     dplyr::summarise_at(c("reconst", "err", "elev", "mean", "anomaly"), funs(mean, n(), unique_sites = n_distinct(dataset.id))) %>%
     dplyr::arrange(as.numeric(period)) %>% 
     dplyr::select(-c(err_n, elev_n, mean_n, anomaly_n, err_unique_sites, elev_unique_sites, mean_unique_sites, anomaly_unique_sites)) %>% 
     dplyr::mutate(midP = seq(-50, 2050, 100))

# Plot the anomaly for GS.
temp_UUSS_100yrAvgs_GDD_7analog_anomaly <- UUSS_100yrAvgs_GDD_7analog_anomaly
temp_UUSS_100yrAvgs_GDD_7analog_anomaly[2,7] <- 1
temp_UUSS_100yrAvgs_GDD_7analog_anomaly[21,7] <- 2000

UUSS_100yrAvgs_GDD_7analog_anomaly_plot <- ggplot(data=(dplyr::filter(temp_UUSS_100yrAvgs_GDD_7analog_anomaly[2:21, ])), aes(x=midP, y=anomaly_mean, label = reconst_n, label2 = reconst_unique_sites)) +
      geom_line(colour="black", size = 3) +
      xlab("Year AD") +
      ylab("Anomaly °C") +
      scale_x_continuous(breaks=seq(1,2000,100), minor_breaks = seq(50, 1950, 200)) +
      scale_y_continuous(breaks=seq(-1,3,.5), limits = c(-1, 3)) +
      theme_bw() + 
      theme(panel.border = element_blank(), panel.grid.major = element_blank(), axis.text=element_text(size=13, colour = "black", family = "Helvetica"), axis.title.y = element_text(size = 20, family = "Helvetica", margin = margin(t = 10, r = 20, b = 10, l = 10)), axis.title.x = element_text(size = 20, family = "Helvetica", margin = margin(t = 20, r = 10, b = 10, l = 10)),
      panel.grid.minor = element_line(colour = "light grey"), axis.line = element_line(colour = "black"), legend.text = element_text(size = 22, family = "Helvetica"), legend.title = element_text(size = 22, family = "Helvetica")) +
      labs(col = "Temperature (°C)")
      
    # Save the plot for GS.
    ggsave(UUSS_100yrAvgs_GDD_7analog_anomaly_plot, file = "UUSS_100yrAvgs_GDD_7analog_anomaly_plot.tiff", path = "./output/UUSS/analog_7/", units="in", width=13.3, height=3, dpi=200, compression = 'lzw')
    
    # Create html files that have the interactive plots from plotly.
    UUSS_100yrAvgs_GDD_7analog_anomaly_plot_ggplotly <- ggplotly(UUSS_100yrAvgs_GDD_7analog_anomaly_plot, originalData = TRUE)
    f<-"./output/UUSS/plotly_htmls/UUSS_100yrAvgs_GDD_7analog_anomaly_plot_ggplotly.html"
     htmlwidgets::saveWidget(UUSS_100yrAvgs_GDD_7analog_anomaly_plot_ggplotly, file.path(normalizePath(dirname(f)),basename(f)))

# ----------------------


#January
toMatch <- "01.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Convert the raster stack to a brick. Don't need this. Cropping converts it into a brick.
#type.list.delta.brick <- raster::brick(type.list.delta)

# Read raster data into memory.
FP.type.list.memory <- readAll(FP.type.list)

# Get the average of the raster layers and get return one raster layer.
January.avg <- FP.type.list %>% mean()


#February
toMatch <- "02.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
February.avg <- FP.type.list %>% mean()


#March
toMatch <- "03.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
March.avg <- FP.type.list %>% mean()


#April
toMatch <- "04.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
April.avg <- FP.type.list %>% mean()


#May
toMatch <- "05.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
May.avg <- FP.type.list %>% mean()


#June
toMatch <- "06.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
June.avg <- FP.type.list %>% mean()


#July
toMatch <- "07.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
July.avg <- FP.type.list %>% mean()


#August
toMatch <- "08.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
August.avg <- FP.type.list %>% mean()


#September
toMatch <- "09.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
September.avg <- FP.type.list %>% mean()


#October
toMatch <- "10.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
October.avg <- FP.type.list %>% mean()


#November
toMatch <- "11.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
November.avg <- FP.type.list %>% mean()


#December
toMatch <- "12.bil"
FPmonthly.files.delta <- unique (grep(paste(toMatch,collapse="|"), 
                        FP.monthly.files, value=TRUE))

# Generate the raster stack.
FP.type.list <- raster::stack(FPmonthly.files.delta,native=F,quick=T)

#Crop raster to the extent of the Four Corner states.
FP.type.list <- raster::crop(FP.type.list, extent.FourCorners)

# Get the average of the raster layers and get return one raster layer.
December.avg <- FP.type.list %>% mean()


# Now stack all of the monthly raster layers that are the monthly 30-year avgs. (1961-1990) for the Four Corners.
months.avg <- c(January.avg, February.avg, March.avg, April.avg, May.avg, June.avg, July.avg, August.avg, September.avg, October.avg, November.avg, December.avg)
PRISM.monthly.30yrAvg <- raster::stack(x=c(months.avg),native=F,quick=T)
PRISM.monthly.30yrAvg.brick <- raster::brick(months.avg)

RAST.DIR <- "./output/raster/"

writeRaster(PRISM.monthly.30yrAvg, filename=paste0(RAST.DIR, "PRISM.monthly.30yrAvg.tif"), format="GTiff", options=c("COMPRESS=DEFLATE", "ZLEVEL=9", "INTERLEAVE=BAND"), overwrite=T, setStatistics=FALSE)

writeRaster(PRISM.monthly.30yrAvg.brick, filename=paste0(RAST.DIR, "PRISM.monthly.30yrAvg.brick.tif"), format="GTiff", options=c("COMPRESS=DEFLATE", "ZLEVEL=9", "INTERLEAVE=BAND"), overwrite=T, setStatistics=FALSE)

```


``` {r compare_reconstructions}

Moberg <- read.delim("./data/nhtemp-moberg2005.txt", header = TRUE, sep = "\t", dec = ".")

```
