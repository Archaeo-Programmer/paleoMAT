---
title: "Upland United States Southwest Pollen-based Climate Reconstruction"
author: "Andrew Gillreath-Brown"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE, results='hide'}

knitr::opts_chunk$set(echo = TRUE)

#update.packages(ask=F, repos = "http://cran.rstudio.com")

if(!require("FedData")) install.packages("FedData")
if(!require("purrr")) install.packages("purrr")

packages <- c("magrittr", "tidyverse", "purrrlyr", "reshape2", # For tidy data
              "foreach", "doParallel", # Packages for parallel processeing
              "rgdal", "sp", "raster", "leaflet", # For spatial data
              "palaeoSig", "rioja", "analogue", "neotoma", "prism", # For MAT and climate data
              "ggplot2", # For plotting
              "rebus", "Hmisc", # For other useful functions
              "rgdal",
              "knitr" # For rmarkdown
)

purrr::walk(.x = packages,
            .f = FedData::pkg_test)

```

# Introduction

Pollen data can be used to do paleo-temperature reconstructions. However, this type of modeling can be affected by a lot of different aspects, such as paleoecological processes, chronology, and topographic effects on communities and species. 

However, improvements in these techniques, and the increasing breadth of paleoclimatic proxies available have furthered our understanding of the effects of climate-driven variability on past societies. 

This program allows you to reconstruct the climate for multiple locations across North America (when data are sufficient to do). In the program below, you can download fossil and modern data from the [Neotoma Paleoecology Database](https://www.neotomadb.org/), then compile the data using Williams and Shuman (2008) so that there will be columns of taxa with counts, as well as metadata attached to each of those records/rows. Some data in Neotoma overlaps with what was used by Whitmore et al. (2005) in the North American Modern Pollen Database, which can be obtained from one of two sources [the Laboratory for Paleoclimatology and Climatology](http://www.lpc.uottawa.ca/data/modern/) at the University of Ottawa and [the Williams Paleoecology Lab](http://www.geography.wisc.edu/faculty/williams/lab/Downloads.html) at the University of Wisconsin. However, data from the North American Pollen Database is constantly being uploaded to Neotoma, and in some cases corrections are being made to the data too.

# Load and cleanup modern data (i.e., pollen, climate, and locations)

## Modern Pollen Data from Neotoma

```{r load_modernPollen, echo = FALSE, cache=TRUE, message=FALSE, warning=FALSE}

# Load and compile modern pollen data from the Neotoma Paleoecological database.

# Compile function for the modern pollen data from Neotoma.
compile_MP <- function(x){
  tibble(
    dataset = x$dataset$dataset.meta$dataset.id,
    site.name = x$dataset$site$site.name,
    depth = x$sample.meta$depth,
    lat = x$dataset$site$lat,
    long = x$dataset$site$long)
}

# Use gpids to get the United States and Canada (or their geopolitical units) in North America. Then get the datasets for the pollen data from each of the gpids.

# Retrieve the GeoPolitical Units table, which has country, state, and county level names with associated IDs. 
gpids <- neotoma::get_table(table.name='GeoPoliticalUnits')

NAID <-  gpids %>%
  dplyr::filter(GeoPoliticalName %in% c('United States', 'Canada'),
                GeoPoliticalUnit == 'country') %$%
  GeoPoliticalID

# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/MP_datasets.Rds")){
  neotoma::get_dataset(datasettype='pollen surface sample', gpid = NAID) %>% 
    neotoma::get_download() %>%
    readr::write_rds("./data/raw_data/MP_datasets.Rds")
}
MP_datasets <- readr::read_rds("./data/raw_data/MP_datasets.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/MP_pubs.Rds")){
  MP_datasets %>% 
    get_publication() %>%
    readr::write_rds("./data/raw_data/MP_pubs.Rds")
}
MP_pubs <- readr::read_rds("./data/raw_data/MP_pubs.Rds")

# Get the modern pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.
# Then, combine the rows from the datasets together so that they are now in one tibble.
MP_metadata <- MP_datasets %>%
  map(compile_MP) %>%
  bind_rows() %>%
  dplyr::rename(site.id = dataset) %>% 
  dplyr::mutate(type = "surface sample")

MP_pub_date <- MP_pubs %>%
  purrr::map_dfr(.id = "site.id",
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 }) %>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                site.id = as.integer(site.id)) %>% 
  dplyr::group_by(site.id) %>% 
  dplyr::slice(which.min(pub_year))

# Get the modern pollen datasets in North America and use map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble.
# KB: added the site ids
MP_counts <- MP_datasets %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  bind_rows(.id = "site.id") %>% 
  dplyr::mutate(site.id = as.integer(site.id))

# Sort the taxa to be in alphabetical order. 
MP_counts <- MP_counts[,c(names(MP_counts)[1], sort(names(MP_counts)[2:ncol(MP_counts)]))]

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge the metadata with the publication year, using the site.id to match, just to double check.
MP_metadata_counts <- dplyr::left_join(MP_metadata,
                                    MP_pub_date,
                                    by  = "site.id") %>%
  dplyr::left_join(MP_counts,
                                       by  = "site.id") %>%
  dplyr::arrange(site.id)

```

## Modern Pollen Data from "Core tops" of the Fossil Pollen Dataset in Neotoma

```{r load_modernPollenFromFossil, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Get the fossil pollen datasets for the United States and Canada, then download the sites. Next, get the chronology data for each site, so that we can identify the core tops from each site (if available). Next, use lapply to limit to the first data frame in the list of lists (each site has multiple dataframes associated with it); so we limit to the chron.control dataframe. Next, we can strip away the upper most list, which is just the site ID, but we are still left with "siteID.chron.control", as the name of the dataframe.

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/NAfossil_datasets.Rds")){
  neotoma::get_dataset(ageold = 41000, 
                       ageyoung = -50, 
                       gpid = NAID,
                       datasettype = 'pollen') %>% 
    neotoma::get_download() %>%
    readr::write_rds("./data/raw_data/NAfossil_datasets.Rds")
}
NAfossil_datasets <- readr::read_rds("./data/raw_data/NAfossil_datasets.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/NAfossil_pubs.Rds")){
  NAfossil_datasets %>% 
    get_publication() %>%
    readr::write_rds("./data/raw_data/NAfossil_pubs.Rds")
}
NAfossil_pubs <- readr::read_rds("./data/raw_data/NAfossil_pubs.Rds")

# Save this so that it doesn't run again!
# You can make it run again by deleting the file.
if(!file.exists("./data/raw_data/NAfossil_chron.Rds")){
  NAfossil_datasets %>% 
    get_chroncontrol() %>%
    readr::write_rds("./data/raw_data/NAfossil_chron.Rds")
}
NAfossil_chron <- readr::read_rds("./data/raw_data/NAfossil_chron.Rds")

# KB: I think this line does almost everything you were trying to do
coreTops <- NAfossil_chron %>% 
  purrr::map_dfr(.id = "site.id",
                 "chron.control") %>%
  tibble::as_tibble() %>%
  dplyr::mutate(site.id = site.id %>%
                  as.integer()) %>%
  dplyr::filter(control.type == "Core top")

# Get the modern pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.
# Then, combine the rows from the datasets together so that they are now in one tibble.
CT_metadata <- NAfossil_datasets %>%
  magrittr::extract(coreTops$site.id %>% 
                      unique() %>% 
                      as.character()) %>%
  map(compile_MP) %>%
  purrr::map(function(x){x[1,]}) %>% # This gets the first row of each (the CT metadata)
  bind_rows() %>%
  dplyr::rename(site.id = dataset) %>% 
  dplyr::mutate(type = "core top")


CTPub_Year <- NAfossil_pubs %>%
  magrittr::extract(coreTops$site.id %>% 
                      unique() %>% 
                      as.character()) %>%
  purrr::map_dfr(.id = "site.id", 
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 })


CTPub_Year %<>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                site.id = as.integer(site.id))

# Group each id together (since sites can have multiple publication years), then only keep the minimum publication year for each site (id).
CT_pubDate <- CTPub_Year %>% 
  group_by(site.id) %>% 
  slice(which.min(pub_year))

# Download the United States and Canada site fossil datasets and compile taxa into one dataframe.
CT_counts <- NAfossil_datasets %>%
  magrittr::extract(coreTops$site.id %>% 
                      unique() %>% 
                      as.character()) %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(function(x){x[1,]}) %>% # This gets the first row of each (the CT counts)
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  bind_rows(.id = "site.id") %>% 
  dplyr::mutate(site.id = as.integer(site.id))

# Sort the taxa to be in alphabetical order. 
CT_counts <- CT_counts[,c(names(CT_counts)[1], sort(names(CT_counts)[2:ncol(CT_counts)]))]

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge the metadata with the publication year, using the site.id to match, just to double check.
CT_metadata_counts <- dplyr::left_join(CT_metadata,
                                       CT_pubDate,
                                       by  = "site.id") %>%
  dplyr::left_join(CT_counts,
                   by  = "site.id") %>%
  dplyr::arrange(site.id)

```

The Neotoma Modern Pollen Database contains `r nrow(MP_counts)` samples, representing `r ncol(MP_counts)` different pollen taxa.

## Combine Core Top and Modern Pollen Data

```{r Combine_CT_MP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Function to combine the core top and surface sample data, since one dataset may have more columns than the other, which would happen if not all species were represented in one dataset. Therefore, here we make sure that all unique columns are represented in the combined dataframe.
rbind.all.columns <- function(x, y) {
  x.diff <- setdiff(colnames(x), colnames(y))
  y.diff <- setdiff(colnames(y), colnames(x))
    
  x[, c(as.character(y.diff))] <- 0
  y[, c(as.character(x.diff))] <- 0
 
  return(rbind(x, y))
}

# Now the two dataframes can be combined using the function, then sort the rows by site.id. The distinct function is used to make sure there is no duplicate data from combining the two dataframes. Then, we convert all NA values to zero.
MPCT_metadata_counts <- rbind.all.columns(MP_metadata_counts, CT_metadata_counts) %>%
  dplyr::arrange(site.id) %>% 
  distinct(site.id, .keep_all = TRUE) %>%
  mutate_all(funs(ifelse(is.na(.), 0, .)))

# Sort the taxa to be in alphabetical order. 
MPCT_metadata_counts <- MPCT_metadata_counts[,c(names(MPCT_metadata_counts)[1:7], sort(names(MPCT_metadata_counts)[8:ncol(MPCT_metadata_counts)]))]

```

## Map Modern Pollen Sample Locations

To examine locations of the modern pollen data, we can use `leaflet` to plot the modern pollen locations.

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -101.05,
          lat = 11.68,
          zoom = 2)

map %>% addMarkers(lng = MPCT_metadata_counts$long, lat = MPCT_metadata_counts$lat, 
                   popup = paste0('<b>', as.character(MPCT_metadata_counts$site.name), '</b><br>',
                                  '<a href=http://apps.neotomadb.org/explorer/?datasetid=', MPCT_metadata_counts$site.id, '>Neotoma Database</a>'))

```

# Load Prism Climate Data

The first step is to get the locations of the Modern Pollen samples. Next, we use a PRISM climate extraction script, which is adapted from [Bocinsky et al. (2016)](https://github.com/bocinsky/Bocinsky_et_al_2016/blob/master/R/Bocinsky_ET_AL_2016_PRISM_EXTRACTOR.R).  

```{r load_Prsim, echo = FALSE, warning=FALSE}
# Load Prism Climate data.

# Get Modern Pollen locational site.id numbers.
names <- MPCT_metadata_counts$site.id

#Extract longitude and latitude data.
lons <- unlist(lapply(all,FUN=function(x){MPCT_metadata_counts$long}))
lats <- unlist(lapply(all,FUN=function(x){MPCT_metadata_counts$lat}))

# Bind longitude and latitude and convert to SpatialPoints.
coords <- cbind(LONGITUDE=lons,LATITUDE=lats)
coordsPoints <- SpatialPoints(coords, proj4string=CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Create a SpatialPointsDataFrame of the samples and clean up locations.
# points <- SpatialPointsDataFrame(coords,data.frame(NAME=names,MP_counts), 
#                                  proj4string=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"))
# locations <- points[!duplicated(coordinates(points)),]


### Script for PRISM climate extraction adapted from R. Kyle Bocinsky, Johnathan Rush, Keith W. Kintigh, and Timothy A. Kohler, 2016. Exploration and Exploitation in the Macrohistory of the Prehispanic Pueblo Southwest. Science Advances.

## This script extracts data from the ~800 m monthly PRISM dataset for the modern pollen locations. It first defines the extent, chops that extent into 800 m (30 arc-second) PRISM cells, and subdivides the extent into 14,440 (120x120) cell chunks for computation. (the chunks are 1x1 degree). These chunks are saved for later computation.

## Set the working directory to the directory of this file!
setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/vignettes")

# Create an output directory
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

# This MUST point at an original LT81 dataset available from the PRISM climate group (http://www.prism.oregonstate.edu).
PRISM800.DIR <- "/Volumes/DATA/PRISM/LT81_800M/"

# Specify a directory for extraction
EXTRACTION.DIR <- list.files(paste0(PRISM800.DIR), recursive=TRUE, full.names=T)

# The climate parameters to be extracted
types <- c("ppt", "tmin","tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# (Down)Load the states shapefile form the National Atlas
if(!dir.exists("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statep010")){
  dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/", showWarnings = F, recursive = T)
  download.file("https://prd-tnm.s3.amazonaws.com/StagedProducts/Small-scale/data/Boundaries/statesp010g.shp_nt00938.tar.gz", destfile="/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", mode='wb')
  untar("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938.tar.gz", exdir="/Volumes/DATA/NATIONAL_ATLAS/statesp010g")
}
states <- readOGR("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/statesp010g.shp_nt00938", layer='statesp010g')

# Transform the states (spatial polygons data frame) to the Coordinate Reference System (CRS) of the PRISM data.
states <- sp::spTransform(states, sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))

# Get the extent (i.e., the continental United States)
extent.states <- raster::extent(states)

# Floor the minimums, ceiling the maximums.
extent.states@xmin <- floor(extent.states@xmin)
extent.states@ymin <- floor(extent.states@ymin)
extent.states@xmax <- ceiling(extent.states@xmax)
extent.states@ymax <- ceiling(extent.states@ymax)

# Get list of all file names in the prism directory.
monthly.files <- EXTRACTION.DIR

# Use the rebus package to generate a regular expression for a number range, which will allow for the extraction of specific years in the prism data (i.e., 1961-1990).
rx <- rebus.numbers::number_range(1961, 1990)

# Now use the expression stored in rx, and trim list of files to to 1961:1990.
monthly.files <- grep(rx, monthly.files, value = TRUE, perl = TRUE)

# Trim to only file names that are rasters.
monthly.files <- grep("*\\.bil$", monthly.files, value=TRUE)
monthly.files <- grep("spqc", monthly.files, value=TRUE, invert=T)
monthly.files <- grep("/cai", monthly.files, value=TRUE)

# Generate the raster stack.
type.list <- raster::stack(monthly.files,native=F,quick=T)

#END Bocinsky et al. 2016 adapted script.

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
climate.points <- raster::extract(x = type.list, y = coordsPoints, df = TRUE)

# Replace the first column of IDs with site.id. This will make it easier to match dataframes. This helps to check the data and not assume that the rows climate data are in the same order as the pollen data.
climate.points$ID <- names

# Rename first column to site.id.
colnames(climate.points)[1] <- "site.id"

# Convert climate data from wide to long format, but keep ID so that we can remember the location that each record came from. Then, we use the separate function to split apart the long prism file names, of which some of the new columns we will not need; therefore, they are labeled as garbage, and these get deleted. Finally, we separate the yearmonth into two columns. This will allow for these to be grouped together to do summarizing below. sep = 4 is the location in the string of six numbers (yearmonth) to separate.
data_long <- melt(climate.points,
                  # site.id variables - all the variables to keep but not split apart.
                  id.vars="site.id") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "variable", "garbage2", "garbage3", "garbage4", "yearmonth"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage")) %>% 
  separate(col = "yearmonth", into = c("year", "month"), sep = 4)

# Take the long format data, then group by ID, which is a unique location, the variable (i.e., precipitiation, min. temp., and max temp.), and each month (e.g., 01 for January, 02 for February, etc.). So, we essentially get a 30 year average for each of the 12 months for the 3 variables at each unique location.
data_avgs <- data_long %>%
  dplyr::group_by(site.id, variable, month) %>% 
  dplyr::summarize(mean = mean(value))

# The precipitation averages are extracted into its own dataframe.
ppt_avgs <- data_avgs %>% 
  dplyr::filter(variable == "ppt")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
tmp_avgs <- data_avgs %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(site.id, month) %>% 
  dplyr::summarize(mean = mean(mean)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(site.id, variable, month, mean)

# Now, combine the precipitation and temperature data back into one dataframe, and convert from long back to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. The, rename all the column names. 
clim_wide <- bind_rows(ppt_avgs, tmp_avgs) %>% 
  dcast(site.id ~ variable + month, value.var="mean") %>% 
  #dplyr::select(-one_of("site.id")) %>% 
  set_colnames(c('site.id', 'pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))

```

# Load WorldClim Climate Data

[WorldClim 2.0](http://worldclim.org/version2) has 31 years (1970-2000) of averaged climate data and here we use a resolution of 30 seconds (~1 km^2^).

```{r load_WordClim, echo = FALSE, warning=FALSE}
# Load WorldClim Climate data.

## Set the working directory to the directory of this file!
setwd("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/vignettes")

# Create an output directory
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data", showWarnings = F, recursive = T)

# Load all the auxillary functions
all.functions <- lapply(list.files("./src",full.names=T),source)

# Suppress scientific notation
options(scipen=999)

# Force Raster to load large rasters into memory
raster::rasterOptions(chunksize=2e+08,maxmemory=2e+09)

# This MUST point at an original LT81 dataset available from the PRISM climate group (http://www.prism.oregonstate.edu).
WC.DIR <- "/Volumes/DATA/WORLDCLIM_2.0/"

# Specify a directory for extraction
EXTRACTION.DIR <- "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data"

# The climate parameters to be extracted
types <- c("ppt", "tmin","tmax")

##### BEGIN RAW DATA EXTRACTION #####
# Create data output directory if it doesn't already exist
dir.create("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/data/extraction", showWarnings = F, recursive = T)

# Get list of all file names in the prism directory.
wc.files <- list.files(paste0(WC.DIR), recursive=T, full.names=T)

wc.files <- grep("*\\.tif$", wc.files, value=TRUE)

# Generate the raster stack.
wc.list <- raster::stack(wc.files,native=F,quick=T)

# Use the raster extract function to extract out the monthly values at a modern pollen location in a table.
WCclimate.points <- raster::extract(x = wc.list, y = coordsPoints, df = TRUE)

# Replace the first column of IDs with site.id. This will make it easier to match dataframes. This helps to check the data and not assume that the rows climate data are in the same order as the pollen data.
WCclimate.points$ID <- names

# Rename first column to site.id.
colnames(WCclimate.points)[1] <- "site.id"

WCdata_long <- melt(WCclimate.points,
                    # ID variables - all the variables to keep but not split apart.
                    id.vars="site.id") %>% 
  tidyr::separate(col = "variable", into = c("garbage1", "garbage2", "variable", "month"), sep = "_") %>% 
  dplyr::select(-starts_with("garbage"))

# The precipitation averages are extracted into its own dataframe.
WCppt_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "prec")

#The temperature (min. and max) are extracted into its own dataframe, so that we can get the temp. average for each month. We also create a new column for the variable type (i.e., tmp). Finally, we select the data that we want to keep in the dataframe.
WCtmp_avgs <- WCdata_long %>% 
  dplyr::filter(variable == "tmin" | variable == "tmax") %>% 
  dplyr::group_by(site.id, month) %>% 
  dplyr::summarize(value = mean(value)) %>% 
  dplyr::mutate(variable = "tmp") %>% 
  dplyr::select(site.id, variable, month, value)

# Now, combine the precipitation and temperature data back into one dataframe, and convert from long back to wide format. Then, we can get rid of the ID column as it will be the same as the location dataframe. The, rename all the column names. 
WCclim_wide <- bind_rows(WCppt_avgs, WCtmp_avgs) %>% 
  dcast(site.id ~ variable + month, value.var="value") %>% 
  #dplyr::select(-one_of("site.id")) %>% 
  set_colnames(c('site.id', 'pjan', 'pfeb', 'pmar', 'papr', 'pmay', 'pjun', 'pjul', 'paug', 'psep', 'poct', 'pnov', 'pdec', 'tjan', 'tfeb', 'tmar', 'tapr', 'tmay', 'tjun', 'tjul', 'taug', 'tsep', 'toct', 'tnov', 'tdec'))

```

## Clean up datasets to remove NAs for PRISM data

Need to further clean up the datasets to remove any rows with NA values. Although currently working with United States data, we do not currently have climate (prism) data for Alaska, which is the reason for all rows with NAs.

```{r removePrismNAs, echo=FALSE, warning=FALSE}

# Get row number in first column that have NA values.
NAindex <- which(is.na(clim_wide[,2]), arr.ind=FALSE)

# Remove rows with NA values, using the index.
Clim_noNAs <- clim_wide[-(NAindex),]

# Remove rows in Moden Pollen dataset, using the same index. This will keep the sites/locations still matched up between the climate data and the modern pollen counts.
MPCT_metadata_counts_noNAs <- MPCT_metadata_counts[-(NAindex),]

```

Modern Pollen data contains `r nrow(MP_counts_noNAs)` samples, representing `r ncol(MP_counts_noNAs)` different pollen taxa. 

### Checking the calibration data set

To test the model, I used the `palaeoSig` package's `randomTF()` function, which tests the models against randomly sorted data. If we do get a significance for a climate variable, then we know that the model reconstruction is better than random. The model takes the proportion of variance accounted for by the actual data, and then compares it to the proportion of variance accounted for by the randomized data. Here, we use a wrapper function for [randomTF](https://github.com/NeotomaDB/Workbooks/blob/master/PollenClimate/R/sig_test.R).

The MAT method is testing whether the modern calibration is able to detect signals in each of the climate parameters. The example here uses the entire North American Modern Pollen Database, rather than a targeted data subset.

#### WA - Monotone Deshrinking

```{r WA_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# This is a wrapper for the randomTF function.
source('../R/sig_test.R')

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- MPCT_metadata_counts_noNAs[,-(1:7)]

wa_sig <- run_tf(pollen = MPCT_counts_noNAs, 
                 climate = Clim_noNAs[,14:25], 
                 func = WA, col = 1 , mono = TRUE)

wa_sig[[1]]
```

The columns are standard weighted average. One positive of using monotone shrinking is that it does not suffer from spatial-autocorrelation like with using MAT. Therefore, there has been quite a bit of criticism against MAT. You can do a spatial variogram of the model, then test to see if it is significant. MAT gives you a root mean error that is much higher than what it probably really is. Sometimes it is just because the cores are so close to one another, more so than the assemblages reflect climate. Consider weighted averaging (partially squared not really worth it. ) Weighted averaging with the monotone shrinking is very fast.

#### MAT - ten closest

```{r MAT_sigTesting_init, results='asis', echo=FALSE, warning=FALSE}

# Remove metadata columns to run significance test.
MPCT_counts_noNAs <- MPCT_metadata_counts_noNAs[,-(1:7)]

# For some reason this fails consistently when we try to use the weighted MAT (col=2).
mat_sig <- run_tf(pollen = MPCT_counts_noNAs, 
                  climate = Clim_noNAs[,14:25],
                  func = MAT, col = 1, k = 10)
#col 1 is just the closest analogue.
mat_sig[[1]]

```

The variance explained is very low for some variables, although the p value is very low. This highlights that in some cases you may not just want to rely on the p value. This also shows the issue with temporal autocorrelation on temperature and, to some extent also, on precipitation variables.

# Reconstruction Statistics

## Reconstruction Significance

Now we can do a similar test on fossil assemblage reconstructions. This again uses the same `randomTF()` function, but the variance explained and significance will probably change given that we are using a slightly more constrained dataset.

### Process North American Fossil Pollen Data.

```{r FossilPollen, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Compile function for the fossil pollen data from Neotoma. This is similar to the compile function for modern pollen, except that the age is added in for a site at each depth.
compile_FP <- function(x){
  tibble(
    dataset = x$dataset$dataset.meta$dataset.id,
    site.name = x$dataset$site$site.name,
    depth = x$sample.meta$depth,
    lat = x$dataset$site$lat,
    long = x$dataset$site$long, 
    age = x$sample.meta$age)
}

# Get the fossil pollen datasets in North America and use purrr::map to apply the compile function to simply return a list of the metadata of each sample.Then, combine the rows from the datasets together so that they are now in one tibble.
NAfossil_metadata <- NAfossil_datasets %>%
  purrr::map(compile_FP) %>%
  bind_rows() %>%
  dplyr::rename(site.id = dataset) %>% 
  dplyr::mutate(type = "fossil") %>% 
  mutate(site.id = as.integer(site.id))

# Get earlies publication dates for each fossil pollen site.
NAfossil_pub_date <- NAfossil_pubs %>%
  purrr::map_dfr(.id = "site.id",
                 .f = function(x){
                   tibble::tibble(pub_year = x %>%
                                    purrr::map_dbl(function(y) y$meta$year) %>% # Get the year from every element of x
                                    as.integer()) # coerce to integer
                 }) %>%
  dplyr::mutate(pub_year = ifelse(is.na(pub_year),
                                  1990,
                                  pub_year),
                site.id = as.integer(site.id)) %>% 
  dplyr::group_by(site.id) %>% 
  dplyr::slice(which.min(pub_year))

NAfossil_pub_date$site.id <- as.integer(NAfossil_pub_date$site.id)

# Get the fossil datasets in North America and use purrr::map to add in counts, then change to a tibble, then bind rows together. Then, use Neotoma's compile function for the Whitmore Full dataset of taxa, and finally change counts into a tibble. We remove site.id because dplyr::left_join cannot process having multiple rows with the same site.id.
NAfossil_counts <- NAfossil_datasets %>%
  purrr::map("counts") %>%
  purrr::map(as_tibble) %>%
  purrr::map(compile_taxa,
             list.name = "WhitmoreFull") %>%
  purrr::map(as_tibble) %>%
  bind_rows(.id = "site.id") %>% 
  select(-ends_with("site.id")) %>%
  mutate_all(funs(ifelse(is.na(.), 0, .)))

NAfossil_counts <- NAfossil_counts[,sort(names(NAfossil_counts)[1:ncol(NAfossil_counts)])]

# Check that Modern pollen dataset has the same columns as the fossil pollen dataset.
sameVariables <- function(x,y) {
    for (i in names(x)) {
        if (!(i %in% names(y))) {
            print('Warning: Names are not the same!')
            break
        }  
        else if(i==tail(names(y),n=1)) {
            print('Names are identical.')
        }
    }
}

sameVariables(NAfossil_counts, MPCT_counts_noNAs)

# Now, we have three tibbles, one has the metadata, one has the publication year, and the other has the taxa and counts data. These are both in the same order, so now we can bind them together.

# First, merge (using a left_join) the metadata with the publication year, using the site.id to match, just to double check.
NAfossil_metadata_pub <- dplyr::left_join(NAfossil_metadata,
                                    NAfossil_pub_date,
                                    by  = "site.id")

# The fossil counts are just bound to the metadata_pub because it is not possible to do a left join when multiple rows have the same site.id.
NAfossil_metadata_counts <- cbind(NAfossil_metadata_pub, NAfossil_counts,
                                       by  = "site.id") %>%
  dplyr::arrange(site.id)

```

### Process Four Corners Fossil Pollen Data.

```{r FourCorners_FP, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

# Determine extents for the Four Corner states. Will want to make this a changeable parameter for web app.
FourCorners <- states[states$NAME %in% c("Arizona", "Colorado", "New Mexico", "Utah"),]

# Get the extent (i.e., the Four Corner states)
extent.FourCorners <- raster::extent(FourCorners)

# Subset fossil dataset. Temporarily put in 40.5 for ymax so that Wyoming and Nebraska fossil sites are not included for the Four Corner states. Here we limit the NA fossil dataset to the Four Corner states and just for the past 2000 years.
UUSS_sites <- subset(NAfossil_metadata_counts, long >= extent.FourCorners@xmin & long <= extent.FourCorners@xmax & lat >= extent.FourCorners@ymin & lat <= 40.5 & age <= 2000)

```

This returns `r length(unique(UUSS_sites$site.id))` sites. The `neotoma` package provides plotting capabilities, but `leaflet` allows for more interactive plotting.

### Obtaining a fossil pollen record from Neotoma

```{r, echo = TRUE, warning=TRUE, echo=FALSE}

library(leaflet)

# Here, we set up how we want our map to look like.
map <- leaflet(width = 800, height = 800) %>% 
  addTiles %>% 
  setView(lng = -109.05,
          lat = 31.68,
          zoom = 4)

# If you wanted a simple map, then you would just need lat and long. However, I have made 
# some additions, so that you can click on each site and get the site name and can also be 
# redirected to Neotoma webpage to get more information about each site.
map %>% addMarkers(lng = UUSS_sites$long, 
                   lat = UUSS_sites$lat, 
                   popup = paste0('<b>', 
                                  as.character(UUSS_sites$site.name), 
                                  '</b><br>',
                                  '<a href=http://apps.neotomadb.org/explorer/?datasetid=', 
                                  UUSS_sites$site.id, 
                                  '>Neotoma Database</a>'))

```

Now, we apply a reconstruction to a real dataset. The record has coverage across multiple timescales. For this example, we will restrict the analysis to a record from Colorado.

#### Obtaining a fossil pollen record for one site in Neotoma

Site `790` within the Upper United States Southwest data represents Eldora Fen, Colorado, recorded by Louis J.Maher Jr. The record spans 7,900 years, which provides some information for environmental change at the site.

```{r EldoraFen, echo = TRUE}

# Filter out the Eldora Fen site from the UUSS sites.
EldoraFen <- dplyr::filter(UUSS_sites, site.name == "Eldora Fen")

# Here we can also use Stratiplot to look at the pollen profile and change over time for 
# the Eldora Fen site.
analogue::Stratiplot(EldoraFen$age ~ 
                       PICEAX + 
                       PINUSX + 
                       CYPERACE,
                     data = EldoraFen, 
                     xlab = "Counts", 
                     ylab = "Age")

```

Using the complete North American dataset, we focus on a reconstruction of summer temperature for this record (`tjul`).  We also need to clean up the data sets too:

```{r, warning=FALSE, echo=FALSE, results='hide'}

# Prediction requires the addition of all Pollen taxa:

# First, additional pollen taxa from the Pollen dataset of the North American Pollen Dataset should be added. This is to double-check; however, the taxa and number of taxa should already be the same between the two dataframes.
EldoraFen <- EldoraFen %>% 
  bind_rows(as.data.frame(MPCT_counts_noNAs[1,]))

#Second, the last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
EldoraFen <- EldoraFen[-nrow(EldoraFen),]

#Third, change any NA values to zero.
EldoraFen[is.na(EldoraFen)] <- 0

#Last, re-order the column names to that of the North American Pollen Dataset.
EldoraFen <- EldoraFen[,colnames(MPCT_counts_noNAs)]

```

#### Obtaining fossil pollen records for all sites in UUSS dataset

Using the complete North American dataset, we focus on a reconstruction of temperature for these records.  We also need to clean up the data sets too:

```{r, warning=FALSE, echo=FALSE, results='hide'}

# Ideally would like to loop through all of the UUSS sites then store those as separate variables; so that each site could go through reconstruction significance to see if it is possible to actually reconstruct that site. Then, we would like to do the reconstruction and figure for each individual site that can be reconstructed. Prediction also requires the addition of all Pollen taxa.
site_names <- as.list(names(UUSS_sites))

# Create an empty list for all the fossil sites to hold the new variable names.
fossil_sites <- list()

for (i in 1:length(UUSS_sites)) {
  nam <- paste("fossil_", site_names[[i]], sep = "")
  dat <- UUSS_sites[[i]] %>%
    get_download() %>% 
    compile_taxa(list.name = "WhitmoreFull") %>%
    compile_downloads() %>% 
    # First, additional pollen taxa from the Pollen dataset of the North American Pollen Dataset should be added.
    bind_rows(as.data.frame(MP_counts_noNAs[1,]))
  # Second, the last row should be removed, which just had values from the modern pollen data. We only added the row, so that the additonal column headings could be added.
  dat <- dat[-nrow(dat),]
  # Third, change any NA values to zero.
  dat[is.na(dat)] <- 0
  # Last, re-order the column names to that of the North American Pollen Dataset.
  dat <- dat[,colnames(MP_counts_noNAs)]
  assign(nam, dat)
  # Create a list of the fossil sites with the new names of the variables, which is "fossil_...".
  fossil_sites[i] <- nam
}

```

#### WA - Monotone Deshrinking

```{r WA_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# These are for the one site reconstruction.
# wa_reconst <- run_tf2(pollen = MP_counts_noNAs, fossil = fossil_data,
#        climate = Clim_noNAs[,13:24],
#        func = WA, col = 1, mono = TRUE)
#  
# wa_reconst[[1]]

for (i in 1:length(fossil_sites)) {
  nam <- paste("wa_reconst_", fossil_sites[[i]], sep = "")
  dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
                 climate = Clim_noNAs[,13:24],
                 func = WA, col = 1, mono = TRUE)
  assign(nam, dat)
}

```


#### MAT - ten closest

```{r MAT_sigTesting_fossil, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This is an altered wrapper for the randomTF function.
source('../R/sig_test2.R')

# mat_reconst <- run_tf2(MP_counts_noNAs, fossil = fossil_data, 
#        climate = Clim_noNAs[,13:24], 
#        func = MAT, col = 2, k = 10) 
# 
# mat_reconst[[1]]

for (i in 1:length(fossil_sites)) {
  nam <- paste("mat_reconst_", fossil_sites[[i]], sep = "")
  dat <- run_tf2(pollen = MP_counts_noNAs, fossil = get(fossil_sites[[i]]), 
                 climate = Clim_noNAs[,13:24],
                 func = mat, col = 2, k = 10)
  assign(nam, dat)
}

```

Again, no significance for the model. This indicates that we are just not able to see a signal within the data, but this is probably due to the calibration dataset being too broad.

## Reconstruction

Once we have validated the methods, we re-run the analyses using the two methods, MAT and WA.

### Model Summary

```{r clim_reconst, cache=TRUE, results='asis', echo=FALSE, warning=FALSE}

# This often takes a long time, so using a work around to make it a little quicker.

if ("mat_reconst.RDS" %in% list.files("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output")) {
  mat_reconst <- readRDS(file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/mat_reconst.RDS')
} else {
  mat_reconst <- predict(rioja::MAT(y = MPCT_counts_noNAs, 
                                    x = Clim_noNAs[,1], 
                                    k = 10, 
                                    dist.method="sq.chord",
                                    lean = FALSE),
                         newdata = fossil_data, 
                         sse = TRUE)
  saveRDS(mat_reconst, file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/mat_reconst.RDS')
}


# Currently the rioja::WA function cannot run when species data have zero abundances. Therefore, here we determine what columns in the modern pollen data have zero column sums, and get an index of those columns, then delete the columns. The same index is used to delete the columns from the fossil pollen data.
dd <- unname(which((colSums(MPCT_counts_noNAs, na.rm=T) < 0.000001), arr.ind = TRUE))
MPCT_counts_noNAs2 <- MPCT_counts_noNAs[,-(dd)]
fossil_data2 <- fossil_data[,-(dd)]


if ("wa_reconst.RDS" %in% list.files("/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output")) {
  wa_reconst <- readRDS(file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
} else {
  wa_reconst <-  predict(rioja::WA(y = tran(MPCT_counts_noNAs2, method = 'proportion'), 
                                   x = Clim_noNAs[1,], lean = FALSE),
                         newdata = fossil_data2, sse = TRUE)
  saveRDS(wa_reconst, file = '/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/wa_reconst.RDS')
}

```


```{r plot_reconst, fig.width=8, fig.height=9, echo=FALSE}

# Here, the data is put into one data frame to allow for ease of plotting.
get_clim <-  data.frame(age = EldoraFen$`790`$sample.meta$age,
                        reconst = mat_reconst$fit[,1],
                        err     = mat_reconst$SEP[,1],
                        model   = rep("MAT", 
                                      each = nrow(fossil_data)),
                        climate = 'tjul')

# Now, we use ggplot to plot the July temperature reconstruction.
ggplot(get_clim, aes(x = age, y = reconst)) + 
  geom_line(aes(col = model)) +
  geom_ribbon(aes(ymin = reconst - err, 
                  ymax = reconst + err, 
                  group = model), 
              alpha = 0.2) +
  
  facet_wrap(~climate, scales = 'free', ncol = 2) +
  
  xlab("Age - kyr BP") +
  
  ylab("Reconstructed Parameter")

```

# Saving to file

Save values to file.
```{r save_reconstruction, echo=FALSE}

write.csv(get_clim %>% 
            dplyr::select(-one_of("err")) %>% 
            tidyr::unite(col = "model_climate", c("model","climate"), sep = "_", remove = TRUE) %>% 
            tidyr::spread(model_climate, reconst), 
          "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/clim_reconst.csv")

write.csv(get_clim %>% 
            dplyr::select(-one_of("reconst")) %>% 
            tidyr::unite(col = "model_climate", c("model","climate"), sep = "_", remove = TRUE) %>% 
            tidyr::spread(model_climate, err), 
          "/Volumes/VILLAGE/SKOPEII/MAT/WORKING/paleomat/output/clim_err.csv")

```
