---
title: "Spatial_Interpolation"
author: "Andrew Gillreath-Brown"
date: "6/24/2021"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load interpolate_time function.

```{r interpolate_time, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

interpolate_time_temp <-
  function(temp_data,
           model = "tps",
           gam.smooth = NULL) {
    # Only the most extreme outliers are removed here by using 0.01 and 0.99 quantiles.
    #First, if there are too few data points, then the input is just returned.
    Q <- quantile(temp_data$value,
                  probs = c(.01, .99),
                  na.rm = FALSE)
    
    if (anyNA(Q) == TRUE) {
      eliminated <- as.data.frame(temp_data)
      
    } else {
      iqr <- IQR(temp_data$value)
      up <-  Q[2] + 1.5 * iqr # Upper Range
      low <- Q[1] - 1.5 * iqr # Lower Range
      eliminated <-
        subset(temp_data,
               temp_data$value > (Q[1] - 1.5 * iqr) &
                 temp_data$value < (Q[2] + 1.5 * iqr))
      
    }
    
    # Here, we set up the midpoint years of each century from the minimum to maximum year represented by each site.
    agemax <- max(eliminated$date)
    agemax_rounded <- agemax %>%
      DescTools::RoundTo(multiple = 100, FUN = ceiling)
    agemin <- min(eliminated$date)
    agemin_rounded <- agemin %>%
      DescTools::RoundTo(multiple = 100, FUN = floor)
    
    # predict.points <-
    #   as.data.frame(seq(agemin_rounded, agemax_rounded, by = 100))
    predict.points <-
      as.data.frame(seq((agemin_rounded + 50), (agemax_rounded - 50), by = 100))
    names(predict.points)[1] <- "date"
    
    # First, check to see if there is enough data to do a cubic spline. If not, then do a simple linear regression,
    # and predict on the linear regression.
    if (nrow(eliminated) <= 2) {
      model_fit <- lm(value ~ date, data = eliminated)
      
      predict.value <-
        as.data.frame(predict(model_fit, predict.points, se.fit = TRUE)) %>%
        dplyr::rename(value = 1)
      
      fit <- cbind(predict.points, predict.value)
      
      # # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
      # plot_fit <-
      #   interpolate_time_plots(fit, eliminated, agemin_rounded, agemax_rounded, std.err = FALSE)
      
      return(list(fit, model_fit))
      
    } else if (model == "gam") {
      if (is.null(gam.smooth) == TRUE) {
        stop(
          "You must select a smooth term for GAM. Please see https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.terms.html for options."
        )
      } else {
        # If there are at least 3 samples, then run the mgcv::gam model. First, get the smoothing dimension, which is generally n/2.
        # However, for mgcv::gam, k must be at least equal to 3. So, for any n length less than 6, then the min number is 3.
        if (length(eliminated$value) < 6) {
          smooth.dim <- 3
          
        } else {
          smooth.dim <- ceiling(length(eliminated$value) * 0.6)
          
        }
        
        model_fit <-
          mgcv::gam(
            value ~ s(date, bs = gam.smooth, k = smooth.dim),
            data = eliminated,
            family = gaussian()
          )
        #plot(model_fit)
        gam_knots <-
          model_fit$smooth[[1]]$xp  ## extract knots locations
        # Can also get more basic data for plot by saving the plot.gam as an object. Then, could save something like the standard error.
        #plot_gam <- plot.gam(model_fit)
        #plot_gam[[1]]$se
        
        predict.points <- as.data.frame(predict.points) %>%
          dplyr::rename(date = predict.points)
        
        predict.value <-
          as.data.frame(mgcv::predict.gam(
            model_fit,
            predict.points,
            type = 'response',
            se.fit = TRUE
          )) %>%
          dplyr::rename(value = 1)
        
        fit <- cbind(predict.points, predict.value)
        #
        #         plot_fit <-
        #           interpolate_time_plots(fit,
        #                                  eliminated,
        #                                  agemin_rounded,
        #                                  agemax_rounded,
        #                                  std.err = TRUE)
        
        return(list(fit, model_fit))
      }
    } else if (model == "tps") {
      if (length(eliminated$value) == 3) {
        # Tps cannot run when n = 3. Here, we set it to 3 because above we have already dealt with when n<=2, which results in applying a lm.
        fit <- interpolate_time_natural(eliminated)
        
        model_fit <- fit[[2]]
        fit <- fit[[1]]
        
        # # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
        # plot_fit <-
        #   interpolate_time_plots(fit,
        #                          eliminated,
        #                          agemin_rounded,
        #                          agemax_rounded,
        #                          std.err = FALSE)
        
      } else {
        if (length(eliminated$value) > 3 & length(eliminated$value) <= 6) {
          smooth.dim <- 4
          
        } else {
          smooth.dim <- ceiling(length(eliminated$value) * 0.6)
        }
        
        predict.points <- as.numeric(unlist(predict.points))
        
        model_fit <-
          fields::Tps(eliminated$date, eliminated$value, df = smooth.dim)
        
        fhat <- predict(model_fit, predict.points)
        
        SE <- fields::predictSE(model_fit, predict.points)
        
        fit <-
          cbind(as.data.frame(predict.points),
                as.data.frame(fhat),
                as.data.frame(SE))
        names(fit) <- c("date", "value", "se.fit")
        
        # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
        # plot_fit <-
        #   interpolate_time_plots(fit,
        #                          eliminated,
        #                          agemin_rounded,
        #                          agemax_rounded,
        #                          std.err = TRUE)
        
      }
      
      return(list(fit, model_fit))
      
    } else if (model == "natural") {
      fit <- interpolate_time_natural(eliminated)
      
      model_fit <- fit[[2]]
      fit <- fit[[1]]
      
      # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
      # plot_fit <-
      #   interpolate_time_plots(fit, eliminated, agemin_rounded, agemax_rounded, std.err = FALSE)
      
      return(list(fit, model_fit))
      
    } else {
      stop("Please choose a valid model.")
    }
    
  }


```


# Load the final reconstruction data.

```{r load_data, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

#Just temporary as this number is actually calculated in the Data_visualization.rmd
warmest.month = 15.6182926829268

# Put lat and long into individual columns.
tavg <- reconst_tavg_final %>%
  tidyr::unnest(geometry) %>%
  dplyr::group_by(sample.id, month) %>%
  dplyr::mutate(col = seq_along(sample.id)) %>% #add a column indicator
  tidyr::spread(key = col, value = geometry) %>%
  dplyr::rename(long = "1", lat = "2") %>%
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age < 5950 & age >= 0 & month == 7) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-4000, 2000, 100))) %>%
  dplyr::mutate(anom = value - warmest.month)


# After more closely examining the Beef Pasture data, the spline was heavily impacted by Petersen's Beef Pasture data. The data more or less produced an extremely smooth spline that led to underfitting. This could be the result of the age model on the Petersen data, which has way fewer dates than the Wright Beef Pasture data. Therefore, I use the Wright data for 1-2000 AD and the Petersen data for the time before 1AD (as the Wright data does not go any farther back in time).
tavg_predicted_temp <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(site.data = purrr::map(site.data,
                                       function(x) {
                                         x %>%
                                           dplyr::group_by(age) %>%
                                           dplyr::mutate(
                                             value = mean(value),
                                             error = mean(error),
                                             anom = mean(anom)
                                           ) %>%
                                           dplyr::distinct(age, .keep_all = TRUE) %>%
                                           dplyr::arrange(age)
                                       })) %>%
  # sf::st_as_sf(coords = c("long", "lat"),
  #                crs = 4326) %>%
  dplyr::mutate(mean_100yrs = purrr::map(
    .x = site.data,
    .f = function(x) {
      interpolate_time_temp(temp_data = x,
                            model = "tps")
    }
  )) %>%
  dplyr::select(-site.data) %>%
  dplyr::arrange(site.name) %>%
  unnest_wider(mean_100yrs) %>%
  dplyr::rename(mean_100yrs = 6, model_fit = 7)

# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
tavg_100yrs <- tavg_predicted_temp %>%
  dplyr::select(-model_fit) %>%
  tidyr::unnest(mean_100yrs) %>%
  dplyr::filter(date >= -1000 & date <= 1800) %>%
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)

# Get site locations.
fossil.locs <- tavg %>%
  dplyr::group_by(site.name) %>%
  dplyr::slice(1) %>%
  dplyr::select(site.id, site.name, long, lat)

#Get the elevation dataset.
NED <-
  raster::raster("/Volumes/DATA/NED/EXTRACTIONS/SKOPE_SWUS/rasters/NED_1.tif")
# Temporarily re-projecting at a lower resolution that the code runs faster.
NED <- raster::aggregate(NED, fact = 10)
NED <-
  raster::projectRaster(NED, crs = CRS("+init=epsg:4326"))



```


```{r interpolate, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

#Line for testing. Usually use tavg_100yrs$site.predictions below.
ok <- tavg_100yrs[1:2,]
ok2 <- tavg_100yrs[1, ]

tavg_preds <-
  purrr::map(
    tavg_100yrs$site.predictions,
    .f = function(x) {
      map_predictions(
        site.preds = x,
        site.locs = fossil.locs,
        elev.raster = NED,
        nfraction.df = 0.6,
        rast.extrap = FALSE
      )
    }
  )





bbox_buffer <- c(
  "xmin" = min(site.locs$long) - 1,
  "ymin" = min(site.locs$lat) - 1,
  "xmax" = max(site.locs$long) + 1,
  "ymax" = max(site.locs$lat) + 1
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

# Crop the elevation dataset to the extent of the bounding box.
elev.raster <- raster::crop(elev.raster, bbox_buffer)

# Create a bounding box around the site extent (i.e., for site.locs). This is essentially the same as bbox_buffer, but without the 1 degree buffer.
# This will be used to crop the raster output below.
bbox_limited <- c(
  "xmin" = min(site.locs$long),
  "ymin" = min(site.locs$lat),
  "xmax" = max(site.locs$long),
  "ymax" = max(site.locs$lat)
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)


# Need to extract the xy grid and put in ascending order, as the fields package expects that. The top row or first record is NA, so removing the first row/record.
elev.raster.long <- raster::xFromCol(elev.raster)
elev.raster.lat <- raster::yFromRow(elev.raster)[2:962] %>%
  sort()
elev.raster.elev <- as.matrix(elev.raster)
elev.raster.elev <- elev.raster.elev[2:962,]
# Transpose, so that rows and columns will match the long lat lists. Then, mirror the columns so that the latitude is ascending.
elev.raster.elev <- t(elev.raster.elev) %>%
  as.data.frame()
elev.raster.elev <-
  elev.raster.elev[, order(ncol(elev.raster.elev):1)] %>%
  as.matrix()
# Put long, lat, and elevation into 1 list. Then, rename to x, y, and z.
elev.raster.list <-
  list(elev.raster.long, elev.raster.lat, elev.raster.elev)
names(elev.raster.list) <- c("x", "y", "z")

#Create the grid list, which will be used in the prediction.
grid.list <-
  list(x = elev.raster.list$x, y = elev.raster.list$y)

# Next, get the number of sites, which is used to define the degrees of freedom (df).
no.sites <- nrow(site.preds)


fit_TPS <- fields::Tps(
  # Accepts points but expects them as matrix.
  x = as.matrix(site.preds[, c("long", "lat")]),
  # The dependent variable.
  Y = site.preds$anom,
  # Elevation as an independent covariate.
  Z = site.preds$elev,
  miles = TRUE
)

fit.full <-
  fields::predictSurface(fit_TPS, grid.list, ZGrid = elev.raster.list, extrap = FALSE)
fit.full <- raster(fit.full)
crs(fit.full) <- CRS('+init=EPSG:4326')

fit.full.SE <-
  fields::predictSurfaceSE(
    fit_TPS,
    grid.list,
    ZGrid = elev.raster.list,
    drop.Z = TRUE,
    extrap = FALSE
  )
fit.full.SE <- raster(fit.full.SE)
crs(fit.full.SE) <- CRS('+init=EPSG:4326')

fit.full <- raster::crop(fit.full, bbox_limited)
fit.full.SE <- raster::crop(fit.full.SE, bbox_limited)
  
```
