---
title: "Spatial_Interpolation"
author: "Andrew Gillreath-Brown"
date: "6/24/2021"
mainfont: Calibri
output:
  html_document:
    code_folding: show
    keep_md: yes
    number_sections: yes
    theme: sandstone
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load interpolate_time function.

```{r interpolate_time, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

interpolate_time_temp <-
  function(temp_data,
           model = "tps",
           gam.smooth = NULL) {
    # Only the most extreme outliers are removed here by using 0.01 and 0.99 quantiles.
    #First, if there are too few data points, then the input is just returned.
    Q <- quantile(temp_data$value,
                  probs = c(.01, .99),
                  na.rm = FALSE)
    
    if (anyNA(Q) == TRUE) {
      eliminated <- as.data.frame(temp_data)
      
    } else {
      iqr <- IQR(temp_data$value)
      up <-  Q[2] + 1.5 * iqr # Upper Range
      low <- Q[1] - 1.5 * iqr # Lower Range
      eliminated <-
        subset(temp_data,
               temp_data$value > (Q[1] - 1.5 * iqr) &
                 temp_data$value < (Q[2] + 1.5 * iqr))
      
    }
    
    # Here, we set up the midpoint years of each century from the minimum to maximum year represented by each site.
    agemax <- max(eliminated$date)
    agemax_rounded <- agemax %>%
      DescTools::RoundTo(multiple = 100, FUN = ceiling)
    agemin <- min(eliminated$date)
    agemin_rounded <- agemin %>%
      DescTools::RoundTo(multiple = 100, FUN = floor)
    
    # predict.points <-
    #   as.data.frame(seq(agemin_rounded, agemax_rounded, by = 100))
    predict.points <-
      as.data.frame(seq((agemin_rounded + 50), (agemax_rounded - 50), by = 100))
    names(predict.points)[1] <- "date"
    
    # First, check to see if there is enough data to do a cubic spline. If not, then do a simple linear regression,
    # and predict on the linear regression.
    if (nrow(eliminated) <= 2) {
      model_fit <- lm(value ~ date, data = eliminated)
      
      predict.value <-
        as.data.frame(predict(model_fit, predict.points, se.fit = TRUE)) %>%
        dplyr::rename(value = 1)
      
      fit <- cbind(predict.points, predict.value)
      
      # # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
      # plot_fit <-
      #   interpolate_time_plots(fit, eliminated, agemin_rounded, agemax_rounded, std.err = FALSE)
      
      return(list(fit, model_fit))
      
    } else if (model == "gam") {
      if (is.null(gam.smooth) == TRUE) {
        stop(
          "You must select a smooth term for GAM. Please see https://stat.ethz.ch/R-manual/R-devel/library/mgcv/html/smooth.terms.html for options."
        )
      } else {
        # If there are at least 3 samples, then run the mgcv::gam model. First, get the smoothing dimension, which is generally n/2.
        # However, for mgcv::gam, k must be at least equal to 3. So, for any n length less than 6, then the min number is 3.
        if (length(eliminated$value) < 6) {
          smooth.dim <- 3
          
        } else {
          smooth.dim <- ceiling(length(eliminated$value) * 0.6)
          
        }
        
        model_fit <-
          mgcv::gam(
            value ~ s(date, bs = gam.smooth, k = smooth.dim),
            data = eliminated,
            family = gaussian()
          )
        #plot(model_fit)
        gam_knots <-
          model_fit$smooth[[1]]$xp  ## extract knots locations
        # Can also get more basic data for plot by saving the plot.gam as an object. Then, could save something like the standard error.
        #plot_gam <- plot.gam(model_fit)
        #plot_gam[[1]]$se
        
        predict.points <- as.data.frame(predict.points) %>%
          dplyr::rename(date = predict.points)
        
        predict.value <-
          as.data.frame(mgcv::predict.gam(
            model_fit,
            predict.points,
            type = 'response',
            se.fit = TRUE
          )) %>%
          dplyr::rename(value = 1)
        
        fit <- cbind(predict.points, predict.value)
        #
        #         plot_fit <-
        #           interpolate_time_plots(fit,
        #                                  eliminated,
        #                                  agemin_rounded,
        #                                  agemax_rounded,
        #                                  std.err = TRUE)
        
        return(list(fit, model_fit))
      }
    } else if (model == "tps") {
      if (length(eliminated$value) == 3) {
        # Tps cannot run when n = 3. Here, we set it to 3 because above we have already dealt with when n<=2, which results in applying a lm.
        fit <- interpolate_time_natural_temp(eliminated)
        
        model_fit <- fit[[2]]
        fit <- fit[[1]]
        
        # # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
        # plot_fit <-
        #   interpolate_time_plots(fit,
        #                          eliminated,
        #                          agemin_rounded,
        #                          agemax_rounded,
        #                          std.err = FALSE)
        
      } else {
        if (length(eliminated$value) > 3 & length(eliminated$value) <= 6) {
          smooth.dim <- 4
          
        } else {
          smooth.dim <- ceiling(length(eliminated$value) * 0.6)
        }
        
        predict.points <- as.numeric(unlist(predict.points))
        
        model_fit <-
          fields::Tps(eliminated$date, eliminated$value, df = smooth.dim)
        
        fhat <- predict(model_fit, predict.points)
        
        SE <- fields::predictSE(model_fit, predict.points)
        
        fit <-
          cbind(as.data.frame(predict.points),
                as.data.frame(fhat),
                as.data.frame(SE))
        names(fit) <- c("date", "value", "se.fit")
        
        # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
        # plot_fit <-
        #   interpolate_time_plots(fit,
        #                          eliminated,
        #                          agemin_rounded,
        #                          agemax_rounded,
        #                          std.err = TRUE)
        
      }
      
      return(list(fit, model_fit))
      
    } else if (model == "natural") {
      fit <- interpolate_time_natural(eliminated)
      
      model_fit <- fit[[2]]
      fit <- fit[[1]]
      
      # Now, plot the results. Save as a ggplot object and will return with the rest of the data and models.
      # plot_fit <-
      #   interpolate_time_plots(fit, eliminated, agemin_rounded, agemax_rounded, std.err = FALSE)
      
      return(list(fit, model_fit))
      
    } else {
      stop("Please choose a valid model.")
    }
    
  }



# Interpolate time natural
interpolate_time_natural_temp <-
  function(eliminated) {

    model_fit <-
      spline(eliminated$date, eliminated$value, method = "natural")
    model_fit <- as.data.frame(model_fit)

    Q <- quantile(model_fit$y, probs = c(.01, .99), na.rm = FALSE)
    iqr <- IQR(model_fit$y)
    up <-  Q[2] + 1.5 * iqr # Upper Range
    low <- Q[1] - 1.5 * iqr # Lower Range
    eliminated <-
      subset(model_fit, model_fit$y > (Q[1] - 1.5 * iqr) & model_fit$y < (Q[2] + 1.5 * iqr))

    model_fit <- smooth.spline(eliminated, all.knots = TRUE)

    agemax <- max(eliminated$x)
    agemax_rounded <- agemax %>%
      DescTools::RoundTo(multiple = 100, FUN = ceiling)
    agemin <- min(eliminated$x)
    agemin_rounded <- agemin %>%
      DescTools::RoundTo(multiple = 100, FUN = floor)

    midpoints <-
      seq((agemin_rounded + 50), (agemax_rounded - 50), by = 100)
    fit <- as.data.frame(predict(model_fit, agemin:agemax)) %>%
      dplyr::filter(x >= -1000 & x <= 1799) %>%
      dplyr::group_by(mean = (row_number() - 1) %/% 100) %>%
      dplyr::mutate(mean = mean(y)) %>%
      dplyr::filter(x %in% midpoints) %>%
      dplyr::select(-y) %>%
      dplyr::rename(date = x, value = mean)

    return(list(fit, model_fit))

  }


```


# Load the final reconstruction data.

```{r load_data, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

#Just temporary as this number is actually calculated in the Data_visualization.rmd
warmest.month = 15.6182926829268

reconst_tavg_final <- readr::read_rds("/Users/andrewgillreath-brown/Dropbox/WSU/SKOPEII/model/paleomat/vignettes/data/reconst_tavg_final.rds")

# Put lat and long into individual columns.
tavg <- reconst_tavg_final %>%
  tidyr::unnest(geometry) %>%
  dplyr::group_by(sample.id, month) %>%
  dplyr::mutate(col = seq_along(sample.id)) %>% #add a column indicator
  tidyr::spread(key = col, value = geometry) %>%
  dplyr::rename(long = "1", lat = "2") %>%
  dplyr::mutate(date = paleomat::BPtoBCAD(age)) %>%
  dplyr::filter(age < 5950 & age >= 0 & month == 7) %>%
  dplyr::group_by(period = cut(date, breaks = seq(-4000, 2000, 100))) %>%
  dplyr::mutate(anom = value - warmest.month)


# After more closely examining the Beef Pasture data, the spline was heavily impacted by Petersen's Beef Pasture data. The data more or less produced an extremely smooth spline that led to underfitting. This could be the result of the age model on the Petersen data, which has way fewer dates than the Wright Beef Pasture data. Therefore, I use the Wright data for 1-2000 AD and the Petersen data for the time before 1AD (as the Wright data does not go any farther back in time).
tavg_predicted_temp <- tavg %>%
  dplyr::filter(date >= -1400 & date <= 2000) %>%
  dplyr::filter(!(dataset.id == 250 & date >= 0)) %>%
  dplyr::group_by(site.id, site.name, elev, long, lat) %>%
  tidyr::nest() %>%
  dplyr::rename(site.data = data) %>%
  dplyr::mutate(site.data = purrr::map(site.data,
                                       function(x) {
                                         x %>%
                                           dplyr::group_by(age) %>%
                                           dplyr::mutate(
                                             value = mean(value),
                                             error = mean(error),
                                             anom = mean(anom)
                                           ) %>%
                                           dplyr::distinct(age, .keep_all = TRUE) %>%
                                           dplyr::arrange(age)
                                       })) %>%
  # sf::st_as_sf(coords = c("long", "lat"),
  #                crs = 4326) %>%
  dplyr::mutate(mean_100yrs = purrr::map(
    .x = site.data,
    .f = function(x) {
      interpolate_time_temp(temp_data = x,
                            model = "tps")
    }
  )) %>%
  dplyr::select(-site.data) %>%
  dplyr::arrange(site.name) %>%
  tidyr::unnest_wider(mean_100yrs) %>%
  dplyr::rename(mean_100yrs = 6, model_fit = 7)

# Next, the site data is nested together for each 100 years so that the predictions and maps are done for each 100 years.
tavg_100yrs_temp <- tavg_predicted_temp %>%
  dplyr::select(-model_fit) %>%
  tidyr::unnest(mean_100yrs) %>%
  dplyr::filter(date >= -1000 & date <= 1800) %>%
  dplyr::group_by(date) %>%
  tidyr::nest() %>%
  dplyr::rename(site.predictions = data)

# Get site locations.
fossil.locs <- tavg %>%
  dplyr::group_by(site.name) %>%
  dplyr::slice(1) %>%
  dplyr::select(site.id, site.name, long, lat)

#Get the elevation dataset.
NED <-
  raster::raster("/Users/andrewgillreath-brown/Dropbox/WSU/SKOPEII/model/NED_1.tif")
  #raster::raster("/Volumes/DATA/NED/EXTRACTIONS/SKOPE_SWUS/rasters/NED_1.tif")
# Temporarily re-projecting at a lower resolution that the code runs faster.
NED <- raster::aggregate(NED, fact = 10)
NED <-
  raster::projectRaster(NED, crs = CRS("+init=epsg:4326"))

modern_temp <- brick("/Users/andrewgillreath-brown/Dropbox/WSU/SKOPEII/Figures/PRISM_tavg_monthly_normals.tif") %>% 
  raster::subset(7)

# Crop the elevation dataset to the extent of the bounding box.
temp.raster <- raster::crop(modern_temp, bbox_buffer)
temp.raster <-
  raster::projectRaster(temp.raster, crs = CRS("+init=epsg:4326"))



```


```{r interpolate, echo = FALSE, message=FALSE, warning=FALSE, results='hide'}

#Line for testing. Usually use tavg_100yrs$site.predictions below.
#ok <- tavg_100yrs[1:2,]
ok2 <- tavg_100yrs[18,]$site.predictions[[1]]
#hello <- tavg_100yrs[18,]
ok <- ok2 %>% dplyr::select(long, lat)


temperature = as.data.frame(raster::extract(x = temp.raster, y = ok)) %>% 
  dplyr::rename(modern = 1)
ok3 <- cbind(ok2, temperature)


temp.raster.stars <- st_as_stars(temp.raster)
names(temp.raster.stars) <- "modern"
temp.raster.stars <- st_transform(temp.raster.stars, crs = 4326)
bbox_buffer <- st_transform(bbox_buffer, crs = 4326)
temp.raster.stars <- st_crop(temp.raster.stars, bbox_buffer)

g = gstat(formula = value ~ 1, data = ok)

z = predict(g, ok2)


tavg_preds2 <-
  purrr::map(
    tavg_100yrs$site.predictions,
    .f = function(x) {
      map_predictions(
        site.preds = x,
        site.locs = fossil.locs,
        elev.raster = NED,
        nfraction.df = 0.6,
        rast.extrap = TRUE
      )
    }
  )


v <- getValues(NED)
i <- !is.na(v)
xy <- site.preds[i,]
v <- v[i]
tps2 <- Tps(as.matrix(site.preds[, c("long", "lat", "elev")]), site.preds$value)
p2 <- interpolate(Y, tps2, xyOnly=FALSE)

# as a linear covariate
tps3 <- Tps(site.preds[, c("long", "lat")], site.preds$value, Z=site.preds$elev)
p3 <- interpolate(NED, tps3, xyOnly=FALSE, fun=pfun)


bbox_buffer <- c(
  "xmin" = min(site.locs$long) - 1,
  "ymin" = min(site.locs$lat) - 0.55,
  "xmax" = max(site.locs$long) + 1,
  "ymax" = max(site.locs$lat) + 1
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

# Crop the elevation dataset to the extent of the bounding box.
elev.raster <- raster::crop(elev.raster, bbox_buffer)

# Create a bounding box around the site extent (i.e., for site.locs). This is essentially the same as bbox_buffer, but without the 1 degree buffer.
# This will be used to crop the raster output below.
bbox_limited <- c(
  "xmin" = min(site.locs$long),
  "ymin" = min(site.locs$lat),
  "xmax" = max(site.locs$long),
  "ymax" = max(site.locs$lat)
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

	
# Need to extract the xy grid and put in ascending order, as the fields package expects that. The top row or first record is NA, so removing the first row/record.
elev.raster.long <- raster::xFromCol(elev.raster)
elev.raster.lat <- raster::yFromRow(elev.raster)[2:962] %>%
  sort()
elev.raster.elev <- as.matrix(elev.raster)
elev.raster.elev <- elev.raster.elev[2:962,]
# Transpose, so that rows and columns will match the long lat lists. Then, mirror the columns so that the latitude is ascending.
elev.raster.elev <- t(elev.raster.elev) %>%
  as.data.frame()
elev.raster.elev <-
  elev.raster.elev[, order(ncol(elev.raster.elev):1)] %>%
  as.matrix()
# Put long, lat, and elevation into 1 list. Then, rename to x, y, and z.
elev.raster.list <-
  list(elev.raster.long, elev.raster.lat, elev.raster.elev)
names(elev.raster.list) <- c("x", "y", "z")

#Create the grid list, which will be used in the prediction.
grid.list <-
  list(x = elev.raster.list$x, y = elev.raster.list$y)

# Next, get the number of sites, which is used to define the degrees of freedom (df).
no.sites <- nrow(site.preds)


fit_TPS <- fields::Tps(
  # Accepts points but expects them as matrix.
  x = as.matrix(site.preds[, c("long", "lat")]),
  # The dependent variable.
  Y = site.preds$anom,
  # Elevation as an independent covariate.
  Z = site.preds$elev,
  miles = TRUE
)

fit.full <-
  fields::predictSurface(fit_TPS, grid.list, ZGrid = elev.raster.list, extrap = FALSE)
fit.full <- raster(fit.full)
crs(fit.full) <- CRS('+init=EPSG:4326')

fit.full.SE <-
  fields::predictSurfaceSE(
    fit_TPS,
    grid.list,
    ZGrid = elev.raster.list,
    drop.Z = TRUE,
    extrap = FALSE
  )
fit.full.SE <- raster(fit.full.SE)
crs(fit.full.SE) <- CRS('+init=EPSG:4326')

fit.full <- raster::crop(fit.full, bbox_limited)
fit.full.SE <- raster::crop(fit.full.SE, bbox_limited)


library(fields)
library(dplyr)
library(elevatr)
library(sf)
library(raster)
data(COmonthlyMet)
# data(PRISMelevation)

swcolorado <- c(
  "xmin" = -105,
  "ymin" = 36,
  "xmax" = -109.060253,
  "ymax" = 39
)  %>%
  sf::st_bbox() %>%
  sf::st_as_sfc() %>%
  sf::st_as_sf(crs = 4326) %>%
  sf::st_transform(crs = 4326)

elev.raster <- elevatr::get_elev_raster(swcolorado, z = 5) %>%
  raster::crop(elevation, swcolorado) %>%
  raster::aggregate(fact = 12)

# temp.points <- cbind(fossil.locs, hello2) %>%
#   dplyr::ungroup() %>% 
#   dplyr::rename("temperature" = 5) %>% 
#   dplyr::select(-site.id, -site.name) %>% 
#   dplyr::filter(long > -109 & lat > 36)
# 
# temp.points2 <- temp.points %>% 
#   sf::st_as_sf(coords = c("long", "lat"), crs = 4326) %>%
#   sf::st_transform(crs = 4326)
# 
# elev.locs <- raster::extract(elev.raster, temp.points2)
# 
# temp.points <- cbind(temp.points, elev.locs)
# temp.points <- temp.points[-11, ]

temp.points <- structure(list(long = c(-108.160375, -107.808675, -106.510525, 
-107.81571, -105.514165, -106.45052, -105.740635, -105.63166, 
-106.843695, -107.61537, -107.682735, -107.6975, -108.503889, 
-105.724665, -108.10256), lat = c(37.473105, 37.647645, 36.047665, 
37.60664, 37.569525, 37.021745, 37.71109, 38.08806, 37.611035, 
37.902105, 37.74759, 37.737778, 37.473889, 37.67773, 37.46906
), temperature = c(14.6972137463016, 14.8096938066668, 14.9004909660972, 
14.8110845322457, 9.7248919765495, 13.271146269724, 18.0169186561757, 
10.5835671771822, 10.8751447949415, 8.98234964596861, 11.8225704515042, 
11.982697080223, 19.8293764051047, 18.0406584631011, 12.8451985711037
), elev.locs = c(3024, 2694, 2932, 2706, 3589, 3054, 2293, 3449, 
3471, 3739, 3205, 3329, 2111, 2290, 3291)), row.names = c(1L, 
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 12L, 13L, 14L, 15L, 16L), class = "data.frame")

# Need to extract the xy grid and put in ascending order, as the fields package expects that. The top row or first record is NA, so removing the first row/record.
elev.raster.long <- raster::xFromCol(elev.raster)
elev.raster.lat <- raster::yFromRow(elev.raster) %>%
  sort()
elev.raster.elev <- as.matrix(elev.raster)
elev.raster.elev <- elev.raster.elev
# Transpose, so that rows and columns will match the long lat lists. Then, mirror the columns so that the latitude is ascending.
elev.raster.elev <- t(elev.raster.elev) %>%
  as.data.frame()
elev.raster.elev <-
  elev.raster.elev[, order(ncol(elev.raster.elev):1)] %>%
  as.matrix()
# Put long, lat, and elevation into 1 list. Then, rename to x, y, and z.
elev.raster.list <-
  list(elev.raster.long, elev.raster.lat, elev.raster.elev)
names(elev.raster.list) <- c("x", "y", "z")

#Create the grid list, which will be used in the prediction.
grid.list <-
  list(x = elev.raster.list$x, y = elev.raster.list$y)

obj <- fields::Tps(
  # Accepts points but expects them as matrix.
  x = as.matrix(temp.points[, c("long", "lat")]),
  # The dependent variable.
  Y = temp.points$temperature,
  # Elevation as an independent covariate.
  Z = temp.points$elev.locs,
  miles = TRUE
)

out.p <- predictSurface(obj,
                        grid.list = grid.list,
                        ZGrid = elev.raster.list,
                        extrap = TRUE)
out.raster <- raster(out.p)
crs(out.raster) <- CRS('+init=EPSG:4326')
out.raster <- crop(out.raster, sf::st_bbox(c(
  "xmin" = -109,
  "ymin" = 36.0,
  "xmax" = -105,
  "ymax" = 38.25
)))


# Now using the same data, but using interpolate.

tps2 <-
  Tps(as.matrix(temp.points[, c("long", "lat", "elev.locs")]), temp.points$temperature)
p2 <- interpolate(elev.raster, tps2, xyOnly = FALSE)
p2 <- crop(p2, sf::st_bbox(c(
  "xmin" = -109,
  "ymin" = 36.0,
  "xmax" = -105,
  "ymax" = 38.25
)))


# as a linear covariate
tps3 <-
  Tps(as.matrix(temp.points[, c("long", "lat")]),
      temp.points$temperature,
      Z = temp.points$elev.locs)
p3 <- interpolate(elev.raster, tps3, xyOnly = FALSE, fun = pfun)
p3 <- crop(p3, sf::st_bbox(c(
  "xmin" = -109,
  "ymin" = 36.0,
  "xmax" = -105,
  "ymax" = 38.25
)))


get_prism_annual("tmean", years = 2010, keepZip = FALSE, keep_pre81_months = FALSE)
my.layer<- raster("/Users/andrew/Downloads/prism_set_dl_dir/PRISM_tmean_stable_4kmM3_2010_bil/PRISM_tmean_stable_4kmM3_2010_bil.bil") %>% 
  crop(sf::st_bbox(c(
  "xmin" = -109,
  "ymin" = 36.0,
  "xmax" = -105,
  "ymax" = 38.25
)))






alt_grd_template_sf <- ok %>%
  as.data.frame() %>% 
  dplyr::rename(Z = 3) %>% 
    mutate(Z = 0)

crs_raster_format <- "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"

grd_template_raster <- alt_grd_template_sf %>% 
  raster::rasterFromXYZ( 
    crs = crs_raster_format)

site.preds.sf <- as.data.frame(p@coords) %>% 
    dplyr::mutate(value = site.preds$value)

site.preds.sf <- st_as_sf(site.preds.sf, coords = c("long", "lat"), crs = 4326) %>% 
  dplyr::select(value, geometry)

fit_NN <- gstat::gstat( # using package {gstat} 
  formula = value ~ 1,    # The column `NH4` is what we are interested in
  data = as(site.preds.sf, "Spatial"), # using {sf} and converting to {sp}, which is expected
  nmax = 10, nmin = 3 # Number of neighboring observations used for the fit
)

fit_IDW <- gstat::gstat( # The setup here is quite similar to NN
  formula = value ~ 1,
  data = as(sf_p3, "Spatial"),
  nmax = 10, nmin = 3,
  set = list(idp = 0.5) # inverse distance power
)

fit_TPS <- fields::Tps( # using {fields}
  x = as.matrix(site.preds[, c("long", "lat")]), # accepts points but expects them as matrix
  Y = site.preds$value,  # the dependent variable
  miles = FALSE     # EPSG 25833 is based in meters
)

fit_GAM <- mgcv::gam( # using {mgcv}
  value ~ s(long, lat, k = 13),      # here come our X/Y/Z data - straightforward enough
  data = site.preds      # specify in which object the data is stored
)

fit_TIN <- interp::interp( # using {interp}
  x = site.preds$long,           # the function actually accepts coordinate vectors
  y = site.preds$lat,
  z = site.preds$value,
  xo = alt_grd_template_sf$x,     # here we already define the target grid
  yo = alt_grd_template_sf$y,
  output = "points"
) %>% bind_cols()

site.preds.cart <- project(as.matrix(site.preds[, c("long", "lat")]), "+proj=utm +zone=51 ellps=WGS84")
coordinates(site.preds) <- ~ long + lat
proj4string(site.preds) <- CRS("+proj=longlat +datum=WGS84")
p <- sp::spTransform(site.preds, CRS("+proj=tmerc +lat_0=0 +lon_0=27 +k=1 +x_0=5500000 +y_0=0 +ellps=krass +units=m +no_defs"))
site.preds.cart2 <- site.preds %>% 
  dplyr::select(-long, -lat) %>% 
  cbind(site.preds.cart) %>% 
  st_as_sf(coords = c("long", "lat"))

p2 <- as.data.frame(p@coords) %>%
  dplyr::mutate(value = site.preds$value) %>% 
  st_as_sf(coords = c("long", "lat"))

p3 <- as.data.frame(p@coords) %>%
  dplyr::mutate(value = site.preds$value)

# This one did not work (smashed along x axis). Would have to figure out the projection issues.
# fit_KRIG <- automap::autoKrige(      # using {automap}
#   formula = value ~ 1,                 # The interface is similar to {gstat} but
#   input_data = as(site.preds.cart2, "Spatial") # {automap} makes a lot of assumptions for you
# ) %>% 
#   .$krige_output %>%  # the function returns a complex object with lot's of metainfo
#   as.data.frame() %>% # we keep only the data we are interested in
#   dplyr::select(X = x1, Y = x2, Z = var1.pred)

proj4string(grd_template_raster) <- CRS("+proj=tmerc +lat_0=0 +lon_0=27 +k=1 +x_0=5500000 +y_0=0 +ellps=krass +units=m +no_defs")
grd_template_raster <- raster::projectRaster(grd_template_raster, crs = "+proj=tmerc +lat_0=0 +lon_0=27 +k=1 +x_0=5500000 +y_0=0 +ellps=krass +units=m +no_defs")
site.preds.sf <- st_transform(site.preds.sf, crs = proj4string(grd_template_raster))


ok5 <- raster(idw_pow2)
crs(ok5) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"





x_range <- as.numeric(c(-112.81446, -105.52416))  # min/max longitude of the interpolation area
y_range <- as.numeric(c(32.65757, 38.089))  # min/max latitude of the interpolation area
# create an empty grid of values ranging from the xmin-xmax, ymin-ymax
grd <- expand.grid(x = seq(from = x_range[1],
                   to = x_range[2], 
                   by = 0.1),
                   y = seq(from = y_range[1], to = y_range[2], 
                       by = 0.1))  # expand points to grid
coordinates(grd) <- ~x + y
# turn into a spatial pixels object
gridded(grd) <- TRUE



tavg_idw <-
  purrr::map(
    site.preds.unlisted.anom.nested$site.predictions,
    .f = function(x) {
      coordinates(x) <- ~long + lat
      idw_pow2 <- gstat::idw(formula = anom ~ 1,
                locations = x,
                newdata = grd,
                idp = 1)
      idw_pow3 <- raster(idw_pow2)
      crs(idw_pow3) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
      return(idw_pow3)
    }
  )

tavg_idw_stack <- raster::stack(tavg_idw)
names(tavg_idw_stack) <- c("BC950","BC850","BC750", "BC650", "BC550","BC450", "BC350", "BC250", "BC150", "BC50", "AD50", "AD150", "AD250", "AD350", "AD450", "AD550", "AD650", "AD750", "AD850","AD950", "AD1050", "AD1150", "AD1250", "AD1350", "AD1450", "AD1550", "AD1650", "AD1750")

plot(tavg_idw_stack[[1:16]], col = colors)
plot(tavg_idw_stack[[17:28]], col = colors)


# Calculate the anomaly across time for entire area (to produce line graph).
anom_new <- purrr::map(tavg_idw, cellStats, mean) %>% 
  as.data.frame() %>% 
  tidyr::pivot_longer(cols = 1:28) %>% 
  #cols = BC950:AD1750
  dplyr::mutate(name = seq(-950, 1750, 100)) %>% 
  ggplot() +
  geom_line(aes(x = name, y = value))


```
